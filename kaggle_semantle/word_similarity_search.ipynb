{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4C5CMCrJ2vJT",
    "outputId": "ad22f717-d706-46ea-83c0-97ad8c13a502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Google word2vec dataset...\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Download complete.\n",
      "Downloading list of English words...\n",
      "Download complete.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Run this file first\n",
    "import os\n",
    "\n",
    "# Check if libraries are installed\n",
    "\n",
    "if not {\"word2vec.model\", \"word2vec.model.vectors.npy\"}.issubset(set(os.listdir())):\n",
    "    # Get the pretrained Google Word2Vec dataset\n",
    "    # This might take a couple minutes\n",
    "    print(\"Downloading Google word2vec dataset...\")\n",
    "\n",
    "    import gensim.downloader\n",
    "    wv = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    wv.save(\"./word2vec.model\")\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "\n",
    "if not \"english-words.txt\" in os.listdir():\n",
    "    print(\"Downloading list of English words...\")\n",
    "\n",
    "    import requests\n",
    "    words = requests.get(\n",
    "        \"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\").content.decode().strip().split(\"\\n\")\n",
    "\n",
    "    word2vec_words = set(wv.index_to_key)\n",
    "    words = [w.strip() for w in words if w.strip() in word2vec_words]\n",
    "\n",
    "    with open(\"./english-words.txt\", \"w\") as fout:\n",
    "        fout.write(\"\\n\".join(words))\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "koGBAlMd3M_7",
    "outputId": "5b491bf4-e874-4a5f-b1d3-4ef012ce53df"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query(input_data):\n",
    "    response = requests.post('http://semantle.advml.com/score', json={'data': input_data})\n",
    "    return response.json()\n",
    "\n",
    "query(\"asteroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQJ-HrW17wx5"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Make sure datasets are downloaded\n",
    "needed_files = [\"word2vec.model\",\n",
    "                \"word2vec.model.vectors.npy\", \"english-words.txt\"]\n",
    "existing_files = set(os.listdir())\n",
    "for file in existing_files:\n",
    "    if file not in existing_files:\n",
    "        print(\"Run 'setup.py' to download the datasets first.\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "wv = KeyedVectors.load(\"word2vec.model\", mmap=\"r\")\n",
    "with open(\"english-words.txt\") as fin:\n",
    "    english_words = fin.read().strip().split(\"\\n\")\n",
    "print(\"Datasets loaded.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qftGw-ZZiAEI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSh7AfhYiAVG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEs0VoWCiAeM"
   },
   "outputs": [],
   "source": [
    "# Problem One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGjPKB6I3M0u"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_possible_vectorized(guess, reported_sim, words_to_consider, tolerance=0.005):\n",
    "    \"\"\"\n",
    "    Returns the list of all words in [words_to_consider] <= 0.05 from [reported_sim] of [guess].\n",
    "    This version is vectorized and exploits the fast computation of matrix-vector products.\n",
    "\n",
    "    cos(A, B) = dot(A, B) / sqrt(A^2) * sqrt(B^2)\n",
    "\n",
    "    In this case, B is a matrix. Therefore, we get a vector of cosines, from which we look up the closest words.\n",
    "    \"\"\"\n",
    "    p_bar = tqdm(range(3))\n",
    "\n",
    "    # Need to map between indices of the matrix and words\n",
    "    id_to_word = list(words_to_consider) if not isinstance(words_to_consider, list) else words_to_consider\n",
    "\n",
    "    guess_v = wv[guess]  # features x 1\n",
    "    # TODO: This operation takes long because for-loop over words, is there a way to load word2vec directly as a matrix?\n",
    "    words_matrix = np.stack([wv[word] for word in words_to_consider])  # num_words x features\n",
    "\n",
    "    p_bar.update(1)\n",
    "\n",
    "    # num_words x features @ features x 1 = num_words x 1\n",
    "    numerator = words_matrix @ guess_v  # dot product of every datapoint with guess_v\n",
    "\n",
    "    # words_matrix @ words_matrix.T --> the squares of the matrix are on the diagonal\n",
    "    # we want to avoid computing all the non-diagonal elements somehow\n",
    "    # we can achieve this with einstein summation:\n",
    "    #   np.einsum('ij,jk') is the normal matrix product\n",
    "    #   np.einsum('ij,ji') gives us the sum over all diagonal elements of the matrix product\n",
    "    #   np.einsum('ij,ji->i') unforces the sum operation, so just returns the elements of the diagonal\n",
    "    # dim: num_words x features \\w features x num_words --> num_words x 1\n",
    "    norms = np.sqrt(np.einsum('ij,ji->i', words_matrix, words_matrix.T))\n",
    "\n",
    "    denominator = norms * np.linalg.norm(guess_v, 2)  # elem-multiply by norm of guess, denominator --> (num_words x 1)\n",
    "\n",
    "    # NOTE: We do not have to do 1 - cosine, because spatial.distance.cosine calculates cosine DISTANCE\n",
    "    # Whereas I here calculate cosine SIMILARITY directly\n",
    "    cosines = (numerator / denominator) * 100  # num_words x 1, cosines[i] = SEMANTLE cosine of guess with ith wv\n",
    "\n",
    "    p_bar.update(1)\n",
    "\n",
    "    # return all the indices where the difference is <= 0.05\n",
    "    candidates = np.where(np.abs(cosines - reported_sim) <= tolerance)[0]\n",
    "\n",
    "    p_bar.update(1)\n",
    "\n",
    "    return [id_to_word[i] for i in candidates]\n",
    "\n",
    "\n",
    "def similarity(word1, word2):\n",
    "    return (1 - spatial.distance.cosine(wv[word1], wv[word2])) * 100\n",
    "\n",
    "\n",
    "def find_possible(guess, reported_sim, words_to_consider):\n",
    "    \"\"\"\n",
    "    Return a set of possible words given a guess and its the similary to the secret word.\n",
    "    \"\"\"\n",
    "    ans = []\n",
    "    for w in tqdm(words_to_consider):\n",
    "        if abs(similarity(guess, w) - reported_sim) <= 0.005:\n",
    "            ans.append(w)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def make_list(words):\n",
    "    if len(words) == 0:\n",
    "        return \"\"\n",
    "    if len(words) == 1:\n",
    "        return f\"'{words[0]}'\"\n",
    "    if len(words) == 2:\n",
    "        return f\"'{words[0]}' and '{words[1]}'\"\n",
    "    return \"'\" + \"', '\".join(words[:-1]) + \"', and '\" + words[-1] + \"'\"\n",
    "\n",
    "\n",
    "# Start guessing process\n",
    "def do_run():\n",
    "    possible = set(english_words)\n",
    "\n",
    "    print(\"Enter guesses as '<guess>, <similarity>'\")\n",
    "    print(f\"There are {len(possible)} possible words remaining.\")\n",
    "    print()\n",
    "    guess = \"attention\"\n",
    "\n",
    "    count = 10\n",
    "    while len(possible) > 1 and count > 0:\n",
    "        count -= 1\n",
    "\n",
    "        print(f\"submitting {guess}\")\n",
    "        result = query(guess)\n",
    "        print(result)\n",
    "\n",
    "        if \"message\" not in result:\n",
    "          print(result)\n",
    "          break\n",
    "\n",
    "        score = result[\"message\"]\n",
    "        reported_sim = score * 10\n",
    "\n",
    "\n",
    "        possible = possible.intersection(\n",
    "            set(find_possible_vectorized(guess, reported_sim, possible, tolerance=1)))\n",
    "        sample_words = random.sample(sorted(possible), min(3, len(possible)))\n",
    "\n",
    "        if (len(sample_words) == 0):\n",
    "          print()\n",
    "          print(\"out of words\")\n",
    "          return\n",
    "\n",
    "        guess = sample_words[0]\n",
    "        print(f\"next guess: {guess}\")\n",
    "\n",
    "        print(f\"There are {len(possible)} possible words remaining: {make_list(sample_words)}.\")\n",
    "\n",
    "        if len(possible) == 0:\n",
    "            print(f\"There are no possible words remaining--something went wrong.\")\n",
    "            raise Exception()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw-6fxPF7u-y",
    "outputId": "fa91ff6c-36f5-4bd0-fc13-c5589867f88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter guesses as '<guess>, <similarity>'\n",
      "There are 97419 possible words remaining.\n",
      "\n",
      "submitting attention\n",
      "{'message': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim metrics\n",
      "[ 6.922587    0.92039305 10.869526   ...  7.0107183   1.6071666\n",
      " -9.745776  ]\n",
      "7.800000000000001\n",
      "\n",
      "next guess: shirring\n",
      "There are 9982 possible words remaining: 'shirring', 'repackage', and 'bumps'.\n",
      "submitting shirring\n",
      "{'message': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 30.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim metrics\n",
      "[21.462091  10.130891  26.03762   ... 12.817133   4.403933   5.1435685]\n",
      "7.4\n",
      "\n",
      "next guess: incitements\n",
      "There are 928 possible words remaining: 'incitements', 'diversification', and 'pigeonhole'.\n",
      "submitting incitements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 89.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim metrics\n",
      "[ 1.23200102e+01  1.92812004e+01  2.73667068e+01  4.43256426e+00\n",
      "  2.50281639e+01  1.99696999e+01  2.65306091e+01  1.31282167e+01\n",
      "  1.70877552e+00  1.85033054e+01  4.66263390e+00  5.69729662e+00\n",
      " -3.87126422e+00  2.37033882e+01 -3.02642679e+00  4.34650612e+00\n",
      "  2.30434799e+01  4.02459526e+00  2.29038448e+01  3.81360590e-01\n",
      "  1.86591587e+01  1.30262003e+01  6.84307051e+00  1.80633640e+01\n",
      "  1.06489639e+01  1.00231218e+01  8.72429562e+00  1.79203281e+01\n",
      "  1.47526922e+01 -1.48146820e+00  1.55805321e+01  8.71843624e+00\n",
      "  1.18232937e+01 -4.08709526e+00 -3.49312687e+00 -6.20905256e+00\n",
      "  5.59824419e+00  7.43867445e+00  1.21102314e+01  7.16193771e+00\n",
      "  4.58490133e+00  5.83055639e+00  2.65609150e+01  1.03038511e+01\n",
      "  7.95871162e+00  6.39781356e-01  5.74322510e+00  2.22112160e+01\n",
      " -2.33518147e+00  2.45565796e+00  1.39877481e+01  1.98693657e+01\n",
      "  1.58727837e+01  4.52231598e+00  6.97993135e+00  1.18322048e+01\n",
      "  2.48447590e+01  1.14831562e+01  1.62746010e+01 -7.07250178e-01\n",
      "  1.08681488e+01  2.88882089e+00 -1.69912171e+00 -2.65055180e+00\n",
      "  4.59662676e+00  1.56481028e+01  6.65381765e+00  2.99611893e+01\n",
      "  1.27406969e+01 -8.90149593e+00  1.29781542e+01  1.48635063e+01\n",
      " -2.81036168e-01  9.55547523e+00  6.20201063e+00  3.67818594e+00\n",
      "  5.54436350e+00  8.09566975e+00  8.29315281e+00  1.00421953e+01\n",
      "  2.34849644e+00 -2.65949845e+00  3.20653534e+00  6.18175077e+00\n",
      "  1.09307814e+01  1.61226234e+01  1.26265640e+01  1.14344044e+01\n",
      "  3.11340923e+01  2.13841419e+01 -4.81480980e+00  9.64494705e+00\n",
      "  4.38991022e+00 -8.77908692e-02  5.76177120e+00  8.49302006e+00\n",
      " -5.50904036e+00  7.92700291e+00  1.14509974e+01 -2.11086154e+00\n",
      "  3.76026702e+00  1.84825630e+01  7.55435133e+00 -3.18612528e+00\n",
      "  1.36214199e+01  1.35189037e+01  2.23022938e+01 -1.80879664e+00\n",
      "  2.38260984e+00  4.58810425e+00  2.44569054e+01  6.49263763e+00\n",
      "  2.10826778e+01  9.53683281e+00  9.77766037e+00 -8.29933548e+00\n",
      " -1.00115767e+01  1.06899929e+01  7.12611008e+00  3.21160507e+00\n",
      "  1.32340803e+01  1.55543566e+01 -1.90824378e+00 -2.02078104e+00\n",
      "  5.49879169e+00  1.53816595e+01  3.54747438e+00  7.13621235e+00\n",
      "  4.74586916e+00  3.35024118e+00 -1.29439914e+00  1.01243467e+01\n",
      "  9.17498207e+00  6.67206192e+00  1.18546286e+01  1.50099115e+01\n",
      "  1.25453587e+01 -2.50348544e+00  1.61348114e+01  1.62104301e+01\n",
      "  1.78092232e+01  7.45355177e+00  3.40418434e+00 -5.69668102e+00\n",
      " -1.87173629e+00 -2.07784176e+00 -2.42469206e-01 -6.22781849e+00\n",
      "  1.42991285e+01  6.91841841e-01  3.67985296e+00  1.70795841e+01\n",
      "  1.89663315e+01  7.98220062e+00  3.45059848e+00  1.36787567e+01\n",
      "  1.57818651e+00  1.10258455e+01 -1.36086535e+00  1.00158911e+01\n",
      " -3.33598733e+00  1.96198857e+00  4.86657715e+00  9.43105412e+00\n",
      "  2.67788525e+01  2.13801384e+01  5.84385633e+00 -3.61997938e+00\n",
      "  9.68608952e+00  3.51390958e+00  1.47055125e+00 -2.26724362e+00\n",
      "  6.88663197e+00 -6.27025223e+00  1.32737103e+01  7.36028099e+00\n",
      "  1.85211861e+00  7.03061533e+00  8.66813660e+00  5.99061966e+00\n",
      "  7.29977179e+00  1.42116289e+01  2.14854889e+01  1.95776558e+01\n",
      "  8.10331345e+00  3.19637394e+01  9.33560753e+00  2.67988658e+00\n",
      "  7.90450048e+00  8.17207813e+00 -6.63705158e+00  4.22579646e-01\n",
      "  1.89076633e+01  8.90450954e+00  2.73451233e+00  2.65885086e+01\n",
      "  5.37759066e+00  8.31631470e+00  1.04320936e+01 -1.54457167e-01\n",
      "  1.01859403e+00  7.07276106e+00  3.51916552e+00  3.65872312e+00\n",
      "  1.33922415e+01  2.42080164e+00  1.28903732e+01  1.29288893e+01\n",
      "  6.89632988e+00  2.57224121e+01 -2.56098771e+00  1.31091337e+01\n",
      "  1.12336864e+01  1.17428102e+01  9.16917229e+00  2.78344040e+01\n",
      "  5.30666733e+00  1.04834270e+01  5.89013290e+00  1.40095367e+01\n",
      "  2.12626839e+01  1.03291292e+01  1.46301374e+01  6.07331848e+00\n",
      "  2.54850006e+01  9.17845154e+00  1.62317314e+01  7.18408012e+00\n",
      "  1.86442769e+00  1.13943253e+01  1.41227274e+01  2.12717571e+01\n",
      "  7.62626171e+00  1.88659706e+01  2.41528339e+01 -4.76001114e-01\n",
      "  1.48670006e+01 -5.06035164e-02  1.61081299e-01  3.12588863e+01\n",
      " -7.40661240e+00  2.03387661e+01 -7.47387230e-01  1.09123211e+01\n",
      "  8.56879902e+00  1.21647882e+01  3.14399093e-01  1.36945400e+01\n",
      "  8.72608066e-01  5.90748358e+00  2.08575554e+01  1.19562149e+01\n",
      "  5.84512615e+00 -3.67899799e+00  1.74006844e+01  3.00296617e+00\n",
      "  7.84249020e+00 -1.88165581e+00  1.96095562e+01  1.85837784e+01\n",
      "  5.05037165e+00  1.05713692e+01  3.86273422e+01  2.13745213e+00\n",
      "  1.48927224e+00  1.48892660e+01  6.72604799e+00  5.57539225e+00\n",
      "  7.06190538e+00  9.99999924e+01  1.24781342e+01  9.47970486e+00\n",
      "  4.37165184e+01  1.01266537e+01  2.40836167e+00  4.47591352e+00\n",
      "  1.47608299e+01  1.93354168e+01  4.24766445e+00 -1.84304953e+00\n",
      " -2.79750204e+00 -2.06746507e+00  1.29135075e+01  1.56045570e+01\n",
      " -3.15287328e+00  2.24397602e+01  2.24538002e+01  4.22138119e+00\n",
      " -1.15026510e+00  6.24774075e+00 -1.50614142e+00  5.85325360e-01\n",
      "  1.47465115e+01  2.45967007e+00  2.17583036e+00  6.50302839e+00\n",
      "  1.27206430e+01  8.27752018e+00  2.05810833e+01 -4.02754354e+00\n",
      "  1.28262978e+01 -5.60167599e+00  7.13935852e+00  1.92615020e+00\n",
      " -6.24076128e+00  1.64696560e+01  2.27304792e+00  1.59155607e+01\n",
      "  9.40627193e+00  1.00925636e+00  2.28704548e+01  2.11319141e+01\n",
      "  8.56563568e+00  1.41680346e+01  7.25333691e-01  1.22594519e+01\n",
      "  5.60875845e+00  3.18385816e+00 -7.25157785e+00  1.50369730e+01\n",
      "  8.23495388e+00  1.91901646e+01 -4.09025621e+00  1.78044491e+01\n",
      "  2.75143299e+01  1.41274614e+01  3.38015342e+00  5.27356148e+00\n",
      "  8.82114315e+00  1.75392933e+01  9.83834934e+00 -4.57446516e-01\n",
      "  1.01025200e+01  2.60390644e+01  4.61379774e-02 -3.60049415e+00\n",
      " -5.45718098e+00  8.55181217e+00  3.27390885e+00  3.09755802e+00\n",
      " -4.87036943e+00  1.10335410e+00  4.00542355e+00 -1.69788189e-02\n",
      "  1.54296207e+01 -4.28411424e-01  7.42951727e+00 -1.58629394e+00\n",
      "  3.01002350e+01 -6.03545189e+00 -8.21954823e+00 -6.36662364e-01\n",
      "  8.33497941e-01  1.00767784e+01  1.25896854e+01  1.35866451e+01\n",
      "  2.29992151e+00 -2.45673370e+00  9.26172829e+00  4.44434977e+00\n",
      "  2.33042068e+01 -2.77092457e+00  8.30666542e+00  9.89681435e+00\n",
      "  4.77488756e+00 -1.04988728e+01  5.44502354e+00 -1.26784182e+00\n",
      "  1.26723027e+00  1.53628988e+01  1.44306021e+01  8.30810356e+00\n",
      "  1.22540569e+01  5.72622919e+00  1.24662030e+00  9.48114014e+00\n",
      " -5.73366451e+00  8.64846802e+00  1.15772772e+01  1.71981049e+00\n",
      "  2.49702759e+01  2.56846581e+01  1.63662899e+00  2.83630013e+00\n",
      "  2.88633585e+00  4.26869316e+01  1.12651062e+01  2.32023411e+01\n",
      "  1.81507416e+01 -3.45427990e-01  1.65785751e+01 -5.32262373e+00\n",
      "  1.30257349e+01  2.28977680e+01 -1.76817245e+01  1.36767683e+01\n",
      "  7.48177099e+00  9.63404083e+00  1.33161564e+01  2.10553360e+00\n",
      "  1.56795382e+00  1.85190029e+01  1.45276394e+01  1.49045181e+01\n",
      "  2.10305214e+01  7.48179245e+00  9.73306847e+00  4.51114368e+00\n",
      " -4.08819485e+00  1.05424318e+01 -1.50824683e-02  1.06005058e+01\n",
      "  1.04846544e+01  1.71234627e+01  1.85185170e+00 -3.27129245e+00\n",
      "  1.99791031e+01  1.91726284e+01  5.43658920e-02  9.98234558e+00\n",
      "  3.02765751e+00  9.71205044e+00  1.30404444e+01 -9.27815378e-01\n",
      "  7.27600002e+00  7.20842457e+00  1.36302176e+01  1.75619483e+00\n",
      "  2.04636002e+00  1.03081884e+01  1.33235664e+01 -6.55685306e-01\n",
      " -1.16016912e+00  8.83208561e+00  4.51671410e+00  2.23479195e+01\n",
      "  6.07586861e+00  1.59914255e+01 -3.18328142e-01  7.81624174e+00\n",
      "  1.04070635e+01  1.22109251e+01  1.34664373e+01  1.51896877e+01\n",
      "  1.80247879e+00  1.21970129e+01  2.87393856e+00  1.12016153e+01\n",
      "  2.97838521e+00  1.78043342e+00  2.05908985e+01  1.01374874e+01\n",
      "  9.09957027e+00  1.29557934e+01 -3.33769292e-01 -9.57416356e-01\n",
      "  2.13295627e+00  1.67291546e+01  2.57274742e+01  2.10598350e+00\n",
      "  3.48195877e+01  3.24821815e+01  4.05900717e+00  6.22462940e+00\n",
      "  1.27357807e+01  1.45455914e+01  2.65341425e+00  3.97907448e+00\n",
      "  1.95975018e+01 -3.77618372e-01  2.01348114e+01  2.06295948e+01\n",
      "  1.04035025e+01  1.75689316e+01  4.78371811e+00  1.44225273e+01\n",
      "  1.29744635e+01  1.52776289e+01  3.33425021e+00  1.13046455e+00\n",
      "  4.25223207e+00  6.66297007e+00  9.09371471e+00  3.51751852e+00\n",
      " -3.13682072e-02 -2.91293114e-01  8.22903156e+00  9.09594238e-01\n",
      "  1.99408996e+00  6.71900272e-01  5.81804848e+00 -2.32382536e+00\n",
      "  2.83138454e-01  2.88350906e+01  9.59255028e+00  4.86767197e+00\n",
      "  7.28518200e+00  2.21519566e+01  6.48321056e+00  1.47228632e+01\n",
      "  1.49768937e+00 -5.28810024e+00 -1.54525316e+00  3.15968246e+01\n",
      "  2.09724598e+01  3.13669205e+01  2.74210281e+01 -3.85873723e+00\n",
      " -9.09330559e+00  1.28681707e+01 -3.42973423e+00 -2.02441168e+00\n",
      "  7.70473480e+00 -4.94531780e-01  1.52208650e+00  1.05424910e+01\n",
      "  2.85157986e+01  7.25379276e+00  2.06301174e+01  8.04671097e+00\n",
      " -1.91591978e+00  3.11119056e+00  1.10041094e+01  6.46735668e+00\n",
      "  5.69667196e+00  6.42388153e+00  1.38949661e+01  9.01136208e+00\n",
      " -2.91215920e+00  1.20953739e+00 -5.34494925e+00  1.48026724e+01\n",
      " -8.53689492e-01  8.73482227e+00 -6.91176271e+00  4.14085293e+00\n",
      "  6.26083183e+00  6.02829266e+00  8.14422798e+00  1.15277901e+01\n",
      "  3.58490419e+00  8.14452362e+00  5.44567680e+00  1.11906004e+01\n",
      "  5.10022879e+00  1.09629221e+01  1.95302165e+00  1.45418568e+01\n",
      "  2.80343647e+01  3.50184917e+00 -1.32776952e+00  1.59219027e+01\n",
      "  1.55097055e+01  9.99338627e+00  2.81566143e+00  3.56499362e+00\n",
      "  4.51988077e+00  8.25297165e+00  1.17995834e+01 -1.93048096e+00\n",
      " -4.10400486e+00  4.69036102e+00  1.07072372e+01  4.87366390e+00\n",
      "  7.52570248e+00  1.55656042e+01  1.00809228e+00  2.63503990e+01\n",
      " -2.52042437e+00 -8.96574974e+00  1.12186995e+01  7.95060015e+00\n",
      " -1.37101424e+00  5.07007539e-01  1.49476271e+01 -1.58890116e+00\n",
      " -1.27273500e-01  7.78533983e+00  1.26621742e+01  4.33536530e+00\n",
      "  1.78640151e+00  1.95519772e+01  2.01418304e+01  1.48551359e+01\n",
      "  2.86102033e+00  1.33708391e+01  1.01344366e+01  2.05472717e+01\n",
      "  2.31509590e+01  2.40646057e+01  8.56342912e-03  1.90490570e+01\n",
      "  9.17455006e+00  2.00616074e+01  5.56568241e+00  6.18086290e+00\n",
      "  3.97608280e+00  2.69350505e+00  2.61515255e+01  6.78535318e+00\n",
      "  1.70906029e+01  2.19743309e+01  9.35890579e+00  1.21110859e+01\n",
      "  7.47347772e-01  1.67051945e+01  7.99147177e+00 -7.42392254e+00\n",
      " -2.75785637e+00 -5.99181700e+00  6.18923950e+00  4.95722342e+00\n",
      "  4.43289709e+00  1.12829323e+01  1.78317471e+01  2.24768734e+01\n",
      "  8.20555878e+00  1.00227470e+01  1.40856133e+01  2.05663376e+01\n",
      "  2.08529072e+01 -3.28327632e+00  1.24451389e+01  1.00354748e+01\n",
      "  1.73342247e+01  8.19862461e+00  2.85214367e+01  4.07611895e+00\n",
      "  5.93427420e+00 -5.04677105e+00  2.97073956e+01  3.67058921e+00\n",
      "  1.03456736e+01  1.64993916e+01  1.66713619e+01  7.61336994e+00\n",
      "  1.86098442e+01  1.56342478e+01  1.84863834e+01  1.16747456e+01\n",
      "  1.36543055e+01  1.66961308e+01  8.94132614e+00  3.35439777e+00\n",
      "  5.90421057e+00  8.44257355e+00  8.29835129e+00  9.68344688e+00\n",
      " -7.74944925e+00  2.02365189e+01  2.32473230e+00  4.59475851e+00\n",
      "  2.14985409e+01  1.30863323e+01  5.87758350e+00 -2.83022332e+00\n",
      "  5.62951326e+00  8.27807903e+00  1.47076235e+01  2.66934013e+00\n",
      "  1.10181112e+01  4.73367643e+00  1.36975708e+01  9.93046284e+00\n",
      " -3.78452730e+00 -3.52073503e+00  1.05679226e+01  3.14113283e+00\n",
      "  2.04706860e+01  4.45517015e+00  5.65534258e+00  1.36863995e+01\n",
      "  1.93455925e+01  1.65023212e+01  1.08068743e+01  3.40569353e+00\n",
      "  8.73298073e+00  1.94869156e+01  9.80757427e+00  5.03210878e+00\n",
      "  1.04448843e+01  1.14917364e+01  6.70152366e-01 -2.38831401e+00\n",
      "  8.81530666e+00  5.86656904e+00  2.24969521e+01  7.73258066e+00\n",
      "  6.87327147e+00  1.85652847e+01  1.33804169e+01  4.65677643e+00\n",
      "  1.64398003e+01  2.89951382e+01  1.61020527e+01  5.01148701e+00\n",
      "  4.96847486e+00  1.72249413e+01 -2.07677817e+00 -1.41129768e+00\n",
      "  1.95623150e+01  2.04935055e+01  2.73441672e+00  8.01075172e+00\n",
      "  1.42300587e+01  1.44047079e+01  1.59294720e+01  2.00392284e+01\n",
      "  2.16674137e+01  1.52787523e+01  7.21019220e+00 -1.41864645e+00\n",
      "  5.47445297e-01  3.79340005e+00  1.36384859e+01 -9.12448883e-01\n",
      "  2.70073681e+01  6.23138714e+00  9.91158009e+00  1.67124653e+01\n",
      "  1.51000843e+01  1.01952524e+01  1.18710542e+00  2.54142113e+01\n",
      "  2.40976505e+01  4.89095736e+00  1.31370220e+01  7.79293776e+00\n",
      "  2.87246857e+01  4.33090830e+00  3.79001760e+00  1.54782426e+00\n",
      "  4.42245531e+00 -8.24104786e-01  4.87647152e+00  1.35002289e+01\n",
      "  2.54687252e+01  1.71178360e+01  1.42033901e+01  1.09421797e+01\n",
      "  1.32018642e+01  1.94442711e+01 -6.59477329e+00  4.26216364e+00\n",
      "  2.58692856e+01 -4.14356828e-01 -2.66167831e+00  1.66148129e+01\n",
      " -2.53244901e+00  4.01447868e+00  1.09599848e+01  9.10515308e+00\n",
      "  1.33233414e+01  2.84826970e+00  5.17253208e+00  8.38485336e+00\n",
      "  9.95251083e+00  5.24163866e+00  1.76184273e+01  7.96077394e+00\n",
      "  1.08741963e+00  5.90307856e+00  1.00829735e+01  3.01819253e+00\n",
      "  2.38713703e+01 -2.78206515e+00  3.28009248e+00  2.56515770e+01\n",
      "  8.19251823e+00  5.57113218e+00  7.49996758e+00  1.16363792e+01\n",
      " -7.56941438e-01  1.49499435e+01  1.29535639e+00  2.74865742e+01\n",
      " -1.23532140e+00  1.76361299e+00  1.30454998e+01  7.92315817e+00\n",
      "  2.00850925e+01  5.31157351e+00  5.52202702e+00  1.04925585e+01\n",
      "  1.41818285e+01  7.09681511e+00  1.25134735e+01  5.88353920e+00\n",
      "  2.86284981e+01  2.57344818e+01  1.42514648e+01  9.67306614e+00\n",
      "  1.24196615e+01 -4.23387289e-01  8.84107018e+00  7.64084530e+00\n",
      "  8.06985617e-01 -2.96929789e+00  1.36567628e+00  6.32827854e+00\n",
      "  1.81082859e+01  2.23503170e+01  6.40521812e+00  1.05112529e+00\n",
      "  2.03028774e+01  4.35384941e+00  5.77141809e+00  2.42961426e+01\n",
      "  9.09019375e+00  1.05140734e+01  1.27848310e+01  8.59951115e+00\n",
      "  5.30648136e+00  6.97618437e+00  1.19955993e+00  4.07322979e+00\n",
      "  2.03421669e+01  6.55621195e+00  2.90046287e+00  4.90347099e+00\n",
      " -8.17057800e+00 -8.29066694e-01  1.08848133e+01 -4.26375103e+00\n",
      "  1.01400032e+01  2.79308796e+01 -3.65076393e-01  2.55108953e+00\n",
      "  1.70063937e+00 -2.72414923e+00  2.56144695e+01  4.09996319e+00\n",
      "  9.51944923e+00  1.67578278e+01 -1.17036409e+01  2.38204250e+01\n",
      "  6.05860186e+00 -1.42212242e-01  1.30952988e+01 -7.16752768e+00\n",
      "  2.32100844e+00 -2.67869759e+00  7.96886587e+00  4.57402086e+00\n",
      "  8.13387203e+00 -6.16767836e+00  2.25998440e+01  1.44089270e+01\n",
      "  1.02606153e+01 -5.76221657e+00  1.21455841e+01  7.84695053e+00\n",
      " -7.48815358e-01  2.58633976e+01  1.56933098e+01  2.38582382e+01\n",
      "  3.20473480e+01 -3.59312344e+00 -3.66305184e+00  2.22791348e+01\n",
      "  1.24756746e+01  2.18198547e+01  7.52808475e+00  1.15379372e+01\n",
      "  1.01733894e+01  7.51469421e+00  2.62032747e+00  2.73696136e+00\n",
      "  2.67577577e+00  2.83091965e+01  4.38024712e+00  1.10564823e+01\n",
      "  1.67748566e+01  5.73030424e+00  1.05300093e+01 -1.80961418e+00\n",
      "  7.74486589e+00  1.93961449e+01  8.31260014e+00 -1.60716549e-01\n",
      "  2.27761097e+01  1.53284683e+01  8.61503220e+00 -3.89734030e+00\n",
      "  1.53047962e+01  9.48489571e+00  9.17103386e+00  1.27271261e+01\n",
      " -5.44138050e+00  8.36038494e+00  4.95104408e+00 -1.25285971e+00\n",
      "  4.64404869e+00  1.24056587e+01 -6.94964707e-01  5.68354034e+00\n",
      "  3.12711549e+00  6.84120297e-01  1.26885176e+01  1.26415300e+00\n",
      "  5.19887865e-01  2.74830933e+01  1.17386780e+01  1.16970043e+01\n",
      "  1.76886292e+01 -9.73987293e+00  9.62992764e+00  8.36256981e+00\n",
      "  1.01867113e+01  9.26679730e-01  2.29587173e+01  5.65635395e+00\n",
      "  1.86855087e+01  7.23070574e+00  1.55636091e+01  8.00413609e+00\n",
      "  6.18400514e-01  1.06195669e+01  4.27334356e+00  1.73845596e+01\n",
      "  1.19011612e+01  3.17285061e+01  8.08188057e+00 -4.64509821e+00\n",
      "  1.14924049e+01  9.64916706e+00  1.77538986e+01  4.72333431e+00]\n",
      "7.5\n",
      "\n",
      "next guess: dabbling\n",
      "There are 89 possible words remaining: 'dabbling', 'coastlines', and 'cooperating'.\n",
      "submitting dabbling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 587.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim metrics\n",
      "[-3.72751307e+00  7.71386194e+00 -2.88834900e-01  1.53985138e+01\n",
      "  2.78452244e+01  3.71353381e-04  1.13884258e+01  8.38618946e+00\n",
      "  2.40017605e+00  1.98257256e+01 -3.41025066e+00  5.55519390e+00\n",
      "  6.29943705e+00  4.52690506e+00 -3.86224604e+00  1.00991983e+01\n",
      "  1.42184000e+01  3.93707490e+00  8.83966255e+00  3.96269608e+00\n",
      "  2.61942444e+01 -5.50797367e+00  1.36067028e+01  8.21339703e+00\n",
      "  5.44327307e+00  8.31383705e+00  5.49881554e+00  5.31138611e+00\n",
      "  1.11462140e+00 -1.52658720e+01  1.38860111e+01  1.00000000e+02\n",
      "  1.78479443e+01  2.54118977e+01  1.05571890e+01  6.20073366e+00\n",
      "  1.36844950e+01  8.72223186e+00 -4.35724115e+00 -3.97454238e+00\n",
      "  8.18801880e+00  1.65977848e+00  1.39932871e+01  2.30138645e+01\n",
      "  2.46375847e+01  1.48886175e+01  1.89366837e+01  1.36455479e+01\n",
      " -5.04696488e-01  6.00392246e+00  8.94685173e+00  1.06019764e+01\n",
      "  3.74289703e+00  1.32858191e+01  1.89334564e+01  1.21732626e+01\n",
      "  5.91321087e+00  1.44910364e+01  4.80929375e+00  1.50979674e+00\n",
      " -1.70415568e+00  1.86032867e+01  7.06109333e+00  2.07089007e-01\n",
      "  7.16292953e+00 -2.98393822e+00  4.87552822e-01  1.05874014e+01\n",
      " -2.30527687e+00  1.06693280e+00  7.03684998e+00  1.14334555e+01\n",
      " -3.87542784e-01  1.75133610e+01  9.26537228e+00  8.36117089e-01\n",
      "  3.67287874e+00  5.86156321e+00 -8.27278233e+00  4.49169606e-01\n",
      "  1.43104386e+00 -4.25161183e-01 -1.78122103e-01 -4.00461912e+00\n",
      "  4.11072874e+00  1.34919910e+01  9.03374290e+00  1.52755260e+01\n",
      "  1.42633810e+01]\n",
      "7.7\n",
      "\n",
      "next guess: fretful\n",
      "There are 8 possible words remaining: 'fretful', 'invites', and 'as'.\n",
      "submitting fretful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1364.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim metrics\n",
      "[  4.762543    1.2883575   9.283867  100.         16.95606     1.1646332\n",
      "   7.717012   16.009117 ]\n",
      "7.5\n",
      "\n",
      "next guess: invites\n",
      "There are 1 possible words remaining: 'invites'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1FQJr8N77tM"
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FK_Znqw07ui_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muOLPchQ7uAc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kP5mlJiF5YhT",
    "outputId": "63daa12b-b586-449c-d205-6d12e654804f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter guesses as '<guess>, <similarity>'\n",
      "There are 97419 possible words remaining.\n",
      "\n",
      "submitting attention\n",
      "{'message': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done guessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start guessing process\n",
    "def do_run():\n",
    "    possible = set(english_words)\n",
    "\n",
    "    print(\"Enter guesses as '<guess>, <similarity>'\")\n",
    "    print(f\"There are {len(possible)} possible words remaining.\")\n",
    "    print()\n",
    "\n",
    "    guess = \"attention\"\n",
    "\n",
    "    count = 10\n",
    "    while len(possible) > 1 and count > 0:\n",
    "        count -= 1\n",
    "\n",
    "        print(f\"submitting {guess}\")\n",
    "        result = query(guess)\n",
    "        print(result)\n",
    "\n",
    "        if \"message\" not in result:\n",
    "          print(result)\n",
    "          break\n",
    "\n",
    "        score = result[\"message\"]\n",
    "        reported_sim = score * 100\n",
    "\n",
    "\n",
    "        possible = possible.intersection(\n",
    "            set(find_possible_vectorized(guess, reported_sim, possible)))\n",
    "        sample_words = random.sample(sorted(possible), min(3, len(possible)))\n",
    "\n",
    "        if (len(sample_words) == 0):\n",
    "          print(\"Done guessing\")\n",
    "          return\n",
    "\n",
    "\n",
    "        guess = sample_words[0]\n",
    "        print(f\"next guess: {guess}\")\n",
    "\n",
    "        print(f\"There are {len(possible)} possible words remaining: {make_list(sample_words)}.\")\n",
    "\n",
    "        if len(possible) == 0:\n",
    "            print(f\"There are no possible words remaining--something went wrong.\")\n",
    "            raise Exception()\n",
    "\n",
    "\n",
    "do_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3LIWi1v2NJg",
    "outputId": "509ceebe-6b67-4610-c490-6347a19549bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.0945737361908"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(\"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c-GgJV-2NOk",
    "outputId": "9486ade1-cf0b-4528-f481-1c7ff9f4cec8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.98883068561554"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(\"apple\", \"bible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLC-n-SN63Nn",
    "outputId": "c43f39c9-f273-4f55-8163-0acc1789b66c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.741386592388153"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(\"apple\", \"john\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOuD9kev2NT6",
    "outputId": "74dcd683-6f4f-4955-8768-f73e4f7ffe3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(\"apple\", \"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7HJyhtp2NYm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4f-VndW2Nc-"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Simulated query function\n",
    "def query(word):\n",
    "    # Placeholder for the real API call\n",
    "    return 1 if word == \"banana\" else 0.5  # Assume a score of 0.5 for demonstration\n",
    "\n",
    "# Function to get weighted score\n",
    "def weighted_score(word, target_word, api_score):\n",
    "    similarity = nlp(word).similarity(nlp(target_word))\n",
    "    return similarity * api_score\n",
    "\n",
    "# Initial random word\n",
    "target_word = \"apple\"\n",
    "\n",
    "english_words = words.words()\n",
    "np.random.shuffle(english_words)  # Shuffle for randomness\n",
    "\n",
    "for _ in range(100):  # Limit the number of iterations to avoid infinite loop\n",
    "    api_score = query(target_word)\n",
    "    if api_score == 1:\n",
    "        print(f\"Found the word: {target_word}\")\n",
    "        break\n",
    "\n",
    "    scores = [weighted_score(word, target_word, api_score) for word in english_words]\n",
    "    next_index = np.argmax(scores)\n",
    "    target_word = english_words[next_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnnZU3ViANrz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udQJHbHjAOTL",
    "outputId": "738e48b0-d5f4-47b6-c0fd-c045f58fe270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for 'attention': {'message': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97419/97419 [00:07<00:00, 13151.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No suitable next guess found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_next_word(guess, reported_sim, words_to_consider, tolerance=0.005):\n",
    "    candidates_scores = []\n",
    "\n",
    "    for word in tqdm(words_to_consider):\n",
    "        cosine_sim = similarity(guess, word)\n",
    "        # Considering words that have a cosine similarity close to the reported similarity\n",
    "        if abs(cosine_sim - reported_sim) <= tolerance:\n",
    "            candidates_scores.append((word, cosine_sim))\n",
    "\n",
    "    # Sorting candidates based on their cosine similarity, and picking the one with highest similarity\n",
    "    candidates_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates_scores[0][0] if candidates_scores else None\n",
    "\n",
    "def do_run():\n",
    "    possible = set(english_words)\n",
    "    guess = \"attention\"\n",
    "    count = 10\n",
    "\n",
    "    while len(possible) > 1 and count > 0:\n",
    "        count -= 1\n",
    "\n",
    "        result = query(guess)\n",
    "        print(f\"Result for '{guess}': {result}\")\n",
    "\n",
    "        api_score = result.get(\"message\", 0) * 100  # Adjust based on actual API response\n",
    "\n",
    "        next_guess = find_next_word(guess, api_score, possible, tolerance=)\n",
    "        if next_guess:\n",
    "            guess = next_guess\n",
    "            print(f\"Next guess: {guess}\")\n",
    "        else:\n",
    "            print(\"No suitable next guess found.\")\n",
    "            break\n",
    "\n",
    "# Assuming query is a function that takes a word as input and returns a dictionary with a \"message\" key holding the score.\n",
    "do_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfomMTq_AOZT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwfMgBYeLpKh",
    "outputId": "d94c18d1-d1a2-449d-9ef8-2016b98e6b06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from nltk.corpus import words\n",
    "import nltk\n",
    "\n",
    "# Downloading the word2vec word embeddings\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")  # you can choose another model if you prefer\n",
    "\n",
    "# Downloading the words corpus\n",
    "nltk.download('words')\n",
    "english_words = words.words()\n",
    "\n",
    "# Now you can use `word2vec_model` to get the embeddings and `english_words` to get the English words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zc9naX_hLpnJ",
    "outputId": "ad55f819-1a5b-432f-895f-c3e270bf090d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.78}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def query(input_data):\n",
    "    response = requests.post('http://semantle.advml.com/score', json={'data': input_data})\n",
    "    return response.json()\n",
    "\n",
    "query(\"attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPs0bbjQLpsL"
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_high_score_words(word2vec_model, english_words, target_score=1.0):\n",
    "    high_score_words = []\n",
    "\n",
    "    for word in english_words:\n",
    "        # Query the word itself\n",
    "        response = query(word)\n",
    "        if response.get(\"message\") == target_score:\n",
    "            high_score_words.append(word)\n",
    "\n",
    "        # Find similar words and query them\n",
    "        if word in word2vec_model:\n",
    "            similar_words = word2vec_model.most_similar(word, topn=5)\n",
    "            for similar_word, _ in similar_words:\n",
    "                response = query(similar_word)\n",
    "                if response.get(\"message\") == target_score:\n",
    "                    high_score_words.append(similar_word)\n",
    "\n",
    "    return high_score_words\n",
    "\n",
    "# Assuming word2vec_model and english_words are already loaded\n",
    "high_score_words = find_high_score_words(word2vec_model, english_words)\n",
    "\n",
    "print(f\"Words with score {1.0}: {high_score_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3V3wiDEcLpx2",
    "outputId": "63d3d76b-6231-4e1e-a085-9e09e7df8aee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302295327186584),\n",
       " ('pears', 0.613396167755127)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"apple\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UK8uwBbMo8W",
    "outputId": "d192ab3c-f48b-4085-82c6-6e3c33399481"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06445312, -0.16015625, -0.01208496,  0.13476562, -0.22949219,\n",
       "        0.16210938,  0.3046875 , -0.1796875 , -0.12109375,  0.25390625,\n",
       "       -0.01428223, -0.06396484, -0.08056641, -0.05688477, -0.19628906,\n",
       "        0.2890625 , -0.05151367,  0.14257812, -0.10498047, -0.04736328,\n",
       "       -0.34765625,  0.35742188,  0.265625  ,  0.00188446, -0.01586914,\n",
       "        0.00195312, -0.35546875,  0.22167969,  0.05761719,  0.15917969,\n",
       "        0.08691406, -0.0267334 , -0.04785156,  0.23925781, -0.05981445,\n",
       "        0.0378418 ,  0.17382812, -0.41796875,  0.2890625 ,  0.32617188,\n",
       "        0.02429199, -0.01647949, -0.06494141, -0.08886719,  0.07666016,\n",
       "       -0.15136719,  0.05249023, -0.04199219, -0.05419922,  0.00108337,\n",
       "       -0.20117188,  0.12304688,  0.09228516,  0.10449219, -0.00408936,\n",
       "       -0.04199219,  0.01409912, -0.02111816, -0.13476562, -0.24316406,\n",
       "        0.16015625, -0.06689453, -0.08984375, -0.07177734, -0.00595093,\n",
       "       -0.00482178, -0.00089264, -0.30664062, -0.0625    ,  0.07958984,\n",
       "       -0.00909424, -0.04492188,  0.09960938, -0.33398438, -0.3984375 ,\n",
       "        0.05541992, -0.06689453, -0.04467773,  0.11767578, -0.13964844,\n",
       "       -0.26367188,  0.17480469, -0.17382812, -0.40625   , -0.06738281,\n",
       "       -0.07617188,  0.09423828,  0.20996094, -0.16308594, -0.08691406,\n",
       "       -0.0534668 , -0.10351562, -0.07617188, -0.11083984, -0.03515625,\n",
       "       -0.14941406,  0.0378418 ,  0.38671875,  0.14160156, -0.2890625 ,\n",
       "       -0.16894531, -0.140625  , -0.04174805,  0.22753906,  0.24023438,\n",
       "       -0.01599121, -0.06787109,  0.21875   , -0.42382812, -0.5625    ,\n",
       "       -0.49414062, -0.3359375 ,  0.13378906,  0.01141357,  0.13671875,\n",
       "        0.0324707 ,  0.06835938, -0.27539062, -0.15917969,  0.00121307,\n",
       "        0.01208496, -0.0039978 ,  0.00442505, -0.04541016,  0.08642578,\n",
       "        0.09960938, -0.04296875, -0.11328125,  0.13867188,  0.41796875,\n",
       "       -0.28320312, -0.07373047, -0.11425781,  0.08691406, -0.02148438,\n",
       "        0.328125  , -0.07373047, -0.01348877,  0.17773438, -0.02624512,\n",
       "        0.13378906, -0.11132812, -0.12792969, -0.12792969,  0.18945312,\n",
       "       -0.13867188,  0.29882812, -0.07714844, -0.37695312, -0.10351562,\n",
       "        0.16992188, -0.10742188, -0.29882812,  0.00866699, -0.27734375,\n",
       "       -0.20996094, -0.1796875 , -0.19628906, -0.22167969,  0.08886719,\n",
       "       -0.27734375, -0.13964844,  0.15917969,  0.03637695,  0.03320312,\n",
       "       -0.08105469,  0.25390625, -0.08691406, -0.21289062, -0.18945312,\n",
       "       -0.22363281,  0.06542969, -0.16601562,  0.08837891, -0.359375  ,\n",
       "       -0.09863281,  0.35546875, -0.00741577,  0.19042969,  0.16992188,\n",
       "       -0.06005859, -0.20605469,  0.08105469,  0.12988281, -0.01135254,\n",
       "        0.33203125, -0.08691406,  0.27539062, -0.03271484,  0.12011719,\n",
       "       -0.0625    ,  0.1953125 , -0.10986328, -0.11767578,  0.20996094,\n",
       "        0.19921875,  0.02954102, -0.16015625,  0.00276184, -0.01367188,\n",
       "        0.03442383, -0.19335938,  0.00352478, -0.06542969, -0.05566406,\n",
       "        0.09423828,  0.29296875,  0.04052734, -0.09326172, -0.10107422,\n",
       "       -0.27539062,  0.04394531, -0.07275391,  0.13867188,  0.02380371,\n",
       "        0.13085938,  0.00236511, -0.2265625 ,  0.34765625,  0.13574219,\n",
       "        0.05224609,  0.18164062,  0.0402832 ,  0.23730469, -0.16992188,\n",
       "        0.10058594,  0.03833008,  0.10839844, -0.05615234, -0.00946045,\n",
       "        0.14550781, -0.30078125, -0.32226562,  0.18847656, -0.40234375,\n",
       "       -0.3125    , -0.08007812, -0.26757812,  0.16699219,  0.07324219,\n",
       "        0.06347656,  0.06591797,  0.17285156, -0.17773438,  0.00276184,\n",
       "       -0.05761719, -0.2265625 , -0.19628906,  0.09667969,  0.13769531,\n",
       "       -0.49414062, -0.27929688,  0.12304688, -0.30078125,  0.01293945,\n",
       "       -0.1875    , -0.20898438, -0.1796875 , -0.16015625, -0.03295898,\n",
       "        0.00976562,  0.25390625, -0.25195312,  0.00210571,  0.04296875,\n",
       "        0.01184082, -0.20605469,  0.24804688, -0.203125  , -0.17773438,\n",
       "        0.07275391,  0.04541016,  0.21679688, -0.2109375 ,  0.14550781,\n",
       "       -0.16210938,  0.20410156, -0.19628906, -0.35742188,  0.35742188,\n",
       "       -0.11962891,  0.35742188,  0.10351562,  0.07080078, -0.24707031,\n",
       "       -0.10449219, -0.19238281,  0.1484375 ,  0.00057983,  0.296875  ,\n",
       "       -0.12695312, -0.03979492,  0.13183594, -0.16601562,  0.125     ,\n",
       "        0.05126953, -0.14941406,  0.13671875, -0.02075195,  0.34375   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rW8y07V6MoiL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5wtx-IGM3_L",
    "outputId": "2df5ecf8-b734-4bc8-8793-2de7be8c2d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Bumbledom 0.79\n",
      "query: yawmeter 0.76\n",
      "query: roomy 0.77\n",
      "query: rampler 0.79\n",
      "query: merchant 0.78\n",
      "query: attention 0.78\n",
      "Step 1: New word is 'roomy'\n",
      "query: redemptrice 0.76\n",
      "query: unfurnished 0.74\n",
      "query: tympanal 0.77\n",
      "query: duke 0.78\n",
      "query: urethropenile 0.78\n",
      "Step 2: New word is 'unfurnished'\n",
      "query: billethead 0.77\n",
      "query: thelyplasty 0.77\n",
      "query: Valsaceae 0.71\n",
      "query: seraphtide 0.78\n",
      "query: hogan 0.78\n",
      "Step 3: New word is 'hogan'\n",
      "query: parumbilical 0.78\n",
      "query: hexose 0.76\n",
      "query: demology 0.76\n",
      "query: tiklin 0.77\n",
      "query: cerago 0.78\n",
      "Step 4: New word is 'hexose'\n",
      "query: foliosity 0.75\n",
      "query: magnochromite 0.78\n",
      "query: chinching 0.76\n",
      "query: defacement 0.73\n",
      "query: retenant 0.75\n",
      "Step 5: New word is 'defacement'\n",
      "Optimized word: 'defacement'\n",
      "Queried words and their scores: {'Bumbledom': 0.79, 'yawmeter': 0.76, 'roomy': 0.77, 'rampler': 0.79, 'merchant': 0.78, 'attention': 0.78, 'redemptrice': 0.76, 'unfurnished': 0.74, 'tympanal': 0.77, 'duke': 0.78, 'urethropenile': 0.78, 'billethead': 0.77, 'thelyplasty': 0.77, 'Valsaceae': 0.71, 'seraphtide': 0.78, 'hogan': 0.78, 'parumbilical': 0.78, 'hexose': 0.76, 'demology': 0.76, 'tiklin': 0.77, 'cerago': 0.78, 'foliosity': 0.75, 'magnochromite': 0.78, 'chinching': 0.76, 'defacement': 0.73, 'retenant': 0.75}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "\n",
    "def query(input_data, queried_words):\n",
    "    if input_data in queried_words:\n",
    "        return queried_words[input_data]\n",
    "\n",
    "    response = requests.post('http://semantle.advml.com/score', json={'data': input_data})\n",
    "    score = response.json()[\"message\"]\n",
    "    print(f\"query: {input_data} {score}\")\n",
    "    queried_words[input_data] = score\n",
    "    return score\n",
    "\n",
    "def calculate_gradient(start_word_vec, word_vectors, scores, epsilon):\n",
    "    gradients = [(score * word_vec) for score, word_vec in zip(scores, word_vectors)]\n",
    "    avg_gradient = np.mean(gradients, axis=0)\n",
    "    updated_vector = start_word_vec + epsilon * avg_gradient\n",
    "    return updated_vector\n",
    "\n",
    "def optimize_word(word2vec_model, english_words, start_word, n_steps=5, n_sample=5, epsilon=0.1):\n",
    "    current_word = start_word\n",
    "    queried_words = {}\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        sampled_words = []\n",
    "        while len(sampled_words) < n_sample:\n",
    "            word = random.choice(english_words)\n",
    "            if word not in queried_words:\n",
    "                sampled_words.append(word)\n",
    "\n",
    "        word_vectors = [word2vec_model[word] for word in sampled_words if word in word2vec_model]\n",
    "        scores = [query(word, queried_words) for word in sampled_words]\n",
    "\n",
    "        if current_word in word2vec_model:\n",
    "            start_word_vec = word2vec_model[current_word]\n",
    "            scores.append(query(current_word, queried_words))\n",
    "\n",
    "            updated_vector = calculate_gradient(start_word_vec, word_vectors, scores, epsilon)\n",
    "            similar_words = word2vec_model.similar_by_vector(updated_vector, topn=1)\n",
    "\n",
    "            if similar_words:\n",
    "                current_word = similar_words[0][0]\n",
    "                print(f\"Step {step+1}: New word is '{current_word}'\")\n",
    "            else:\n",
    "                print(f\"Step {step+1}: No similar words found.\")\n",
    "        else:\n",
    "            print(f\"Step {step+1}: The word '{current_word}' is not in the model vocabulary.\")\n",
    "            raise Exception()\n",
    "\n",
    "    return current_word, queried_words\n",
    "\n",
    "# Assuming word2vec_model and english_words are already loaded\n",
    "optimized_word, queried_words_dict = optimize_word(\n",
    "    word2vec_model, english_words, start_word=\"attention\", n_steps=5, n_sample=5, epsilon=1000_000\n",
    ")\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "print(f\"Queried words and their scores: {queried_words_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kTKTdZ7M4ZA",
    "outputId": "56995851-9cb5-47fc-ddfd-cc63c8730c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key with the maximum value is 'b' with a value of 50\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_iLCpdWTbZc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNX9D79YTbOr",
    "outputId": "94d9a861-1334-422c-fee9-b83126ed9338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: stoner 0.81\n",
      "query: sediment 0.8\n",
      "The word with the maximum score is 'stoner' with a score of 0.81\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class QueryManager:\n",
    "    def __init__(self):\n",
    "        self.queried_words = {}\n",
    "\n",
    "    def query(self, input_data):\n",
    "        if input_data in self.queried_words:\n",
    "            return self.queried_words[input_data]\n",
    "\n",
    "        response = requests.post('http://semantle.advml.com/score', json={'data': input_data})\n",
    "        score = response.json()[\"message\"]\n",
    "        print(f\"query: {input_data} {score}\")\n",
    "        self.queried_words[input_data] = score\n",
    "        return score\n",
    "\n",
    "    def get_max_score_word(self):\n",
    "        max_key = max(self.queried_words, key=lambda k: self.queried_words[k])\n",
    "        return max_key, self.queried_words[max_key]\n",
    "\n",
    "# Example usage:\n",
    "manager = QueryManager()\n",
    "\n",
    "# Perform some queries\n",
    "manager.query('stoner')\n",
    "manager.query('sediment')\n",
    "\n",
    "# Get the word with the maximum score\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wA-cpH2UrUF",
    "outputId": "effc799c-6fd7-4a67-8f03-21224ad101ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stoner': 0.81,\n",
       " 'sediment': 0.8,\n",
       " 'precipice': 0.78,\n",
       " 'lignose': 0.78,\n",
       " 'nudibranchian': 0.77,\n",
       " 'myelosyphilis': 0.74,\n",
       " 'prodigiosity': 0.74,\n",
       " 'acetylenediurein': 0.73,\n",
       " 'bucky': 0.78,\n",
       " 'leucocytopenia': 0.74,\n",
       " 'Ortheris': 0.81,\n",
       " 'interseminal': 0.77,\n",
       " 'urinomancy': 0.76,\n",
       " 'uninjurious': 0.74,\n",
       " 'oothecal': 0.78,\n",
       " 'Myxogastrales': 0.75,\n",
       " 'synchronously': 0.74,\n",
       " 'nitrosochloride': 0.73,\n",
       " 'ruttiness': 0.78,\n",
       " 'lupulus': 0.79,\n",
       " 'unbred': 0.77,\n",
       " 'polymath': 0.77,\n",
       " 'indiminishable': 0.77,\n",
       " 'polenta': 0.78,\n",
       " 'shraddha': 0.76,\n",
       " 'civil': 0.78,\n",
       " 'lavishment': 0.74}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.queried_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul2aatyITays"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "719MrCkEM4eQ",
    "outputId": "eb4cb093-f98c-4750-b17c-5960c99f9d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: commensality 0.73\n",
      "query: Pleurosteon 0.77\n",
      "query: acockbill 0.79\n",
      "query: enchylema 0.77\n",
      "query: Bryanism 0.75\n",
      "query: meanwhile 0.77\n",
      "query: tamas 0.77\n",
      "query: trimethylmethane 0.76\n",
      "query: goodman 0.77\n",
      "query: Actinozoa 0.78\n",
      "query: reirrigation 0.74\n",
      "query: possessorship 0.74\n",
      "query: drifting 0.78\n",
      "query: tritium 0.8\n",
      "query: manufactural 0.77\n",
      "query: nephrolysin 0.75\n",
      "query: antagonistic 0.77\n",
      "query: bombarder 0.8\n",
      "query: experience 0.77\n",
      "query: hisingerite 0.8\n",
      "query: transfusive 0.75\n",
      "query: pairedness 0.75\n",
      "query: meekheartedness 0.73\n",
      "query: fencible 0.75\n",
      "query: dorsimesal 0.77\n",
      "Optimized word: 'Ortheris'\n",
      "The word with the maximum score is 'stoner' with a score of 0.81\n"
     ]
    }
   ],
   "source": [
    "def get_max_value_key(input_dict):\n",
    "    # Find the key with the maximum value\n",
    "    max_key = max(input_dict, key=lambda k: input_dict[k])\n",
    "    return max_key, input_dict[max_key]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradient(current_word_vec, new_word_vec, score_difference):\n",
    "    gradient = new_word_vec - current_word_vec\n",
    "    normalized_gradient = gradient / np.linalg.norm(gradient)\n",
    "    return normalized_gradient * score_difference\n",
    "\n",
    "\n",
    "def optimize_word(word2vec_model, english_words, start_word, n_steps=5, n_sample=5, epsilon=0.1):\n",
    "    current_word = start_word\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        current_score = manager.query(current_word)\n",
    "\n",
    "        for _ in range(n_sample):\n",
    "            new_word = random.choice(english_words)\n",
    "            while new_word in manager.queried_words:\n",
    "                new_word = random.choice(english_words)\n",
    "\n",
    "            new_score = manager.query(new_word)\n",
    "\n",
    "            if new_score > current_score and new_word in word2vec_model:\n",
    "                gradient = calculate_gradient(\n",
    "                    word2vec_model[current_word],\n",
    "                    word2vec_model[new_word],\n",
    "                    new_score - current_score\n",
    "                )\n",
    "\n",
    "                for attempt in range(5):\n",
    "                    step_size = epsilon * (10 ** attempt)\n",
    "                    new_vector = word2vec_model[current_word] + step_size * gradient\n",
    "                    similar_word = word2vec_model.similar_by_vector(new_vector, topn=1)[0][0]\n",
    "                    similar_word_score = manager.query(similar_word)\n",
    "\n",
    "                    if similar_word_score > current_score:\n",
    "                        current_word = similar_word\n",
    "                        current_score = similar_word_score\n",
    "                        print(f\"Step {step+1}: New word is '{current_word}' with score {current_score}\")\n",
    "                    else:\n",
    "                        print(f\"Step {step+1}, Attempt {attempt+1}: No improvement, trying a bigger step.\")\n",
    "\n",
    "    return current_word\n",
    "\n",
    "\n",
    "optimized_word = optimize_word(\n",
    "    word2vec_model, english_words, start_word=\"Ortheris\", n_steps=5, n_sample=5, epsilon=1\n",
    ")\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P85bxLjM4kB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olUZ2e-VM4pN",
    "outputId": "6de69822-acae-4094-879e-b7852a0111b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: stoner 0.81\n",
      "query: textrine 0.77\n",
      "query: hyostyly 0.77\n",
      "query: figbird 0.78\n",
      "query: Elaeagnus 0.74\n",
      "query: idiomelon 0.79\n",
      "query: motleyness 0.78\n",
      "query: argillomagnesian 0.76\n",
      "query: chaute 0.77\n",
      "query: buckwashing 0.72\n",
      "query: Naticidae 0.73\n",
      "query: panterer 0.79\n",
      "query: iridodesis 0.77\n",
      "query: divertingly 0.76\n",
      "Step 3, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 5: No improvement, trying a bigger step.\n",
      "query: tapermaker 0.76\n",
      "query: tidological 0.77\n",
      "query: anemometric 0.76\n",
      "query: pyramidologist 0.79\n",
      "query: hemocytolysis 0.73\n",
      "query: scorpionic 0.78\n",
      "query: bridebed 0.76\n",
      "query: renickel 0.77\n",
      "query: whisperer 0.77\n",
      "Step 5, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 5: No improvement, trying a bigger step.\n",
      "query: pyridinium 0.75\n",
      "query: grits 0.77\n",
      "Step 5, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 5: No improvement, trying a bigger step.\n",
      "query: cuminic 0.75\n",
      "Optimized word: 'stoner'\n",
      "The word with the maximum score is 'stoner' with a score of 0.81\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import requests\n",
    "# import random\n",
    "\n",
    "# class QueryManager:\n",
    "#     def __init__(self):\n",
    "#         self.queried_words = {}\n",
    "\n",
    "#     def query(self, input_data):\n",
    "#         if input_data in self.queried_words:\n",
    "#             return self.queried_words[input_data]\n",
    "\n",
    "#         response = requests.post('http://semantle.advml.com/score', json={'data': input_data})\n",
    "#         score = response.json()[\"message\"]\n",
    "#         print(f\"query: {input_data} {score}\")\n",
    "#         self.queried_words[input_data] = score\n",
    "#         return score\n",
    "\n",
    "# def calculate_gradient(current_word_vec, new_word_vec, score_difference):\n",
    "#     gradient = new_word_vec - current_word_vec\n",
    "#     normalized_gradient = gradient / np.linalg.norm(gradient)\n",
    "#     return normalized_gradient * score_difference\n",
    "\n",
    "def optimize_word(word2vec_model, english_words, start_word, n_steps=5, n_sample=5, epsilon=0.1):\n",
    "    manager = QueryManager()\n",
    "    current_word = start_word\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        current_score = manager.query(current_word)\n",
    "\n",
    "        for _ in range(n_sample):\n",
    "            new_word = random.choice(english_words)\n",
    "            while new_word in manager.queried_words:\n",
    "                new_word = random.choice(english_words)\n",
    "\n",
    "            new_score = manager.query(new_word)\n",
    "\n",
    "            if new_score > current_score and new_word in word2vec_model:\n",
    "                gradient = calculate_gradient(\n",
    "                    word2vec_model[current_word],\n",
    "                    word2vec_model[new_word],\n",
    "                    new_score - current_score\n",
    "                )\n",
    "            elif new_score < current_score and new_word in word2vec_model:\n",
    "                gradient = -calculate_gradient(\n",
    "                    word2vec_model[current_word],\n",
    "                    word2vec_model[new_word],\n",
    "                    current_score - new_score\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for attempt in range(10):\n",
    "                step_size = epsilon * (10 ** attempt)\n",
    "                new_vector = word2vec_model[current_word] + step_size * gradient\n",
    "                similar_word = word2vec_model.similar_by_vector(new_vector, topn=1)[0][0]\n",
    "                similar_word_score = manager.query(similar_word)\n",
    "\n",
    "                if similar_word_score > current_score:\n",
    "                    current_word = similar_word\n",
    "                    current_score = similar_word_score\n",
    "                    print(f\"Step {step+1}: New word is '{current_word}' with score {current_score}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Step {step+1}, Attempt {attempt+1}: No improvement, trying a bigger step.\")\n",
    "\n",
    "    return current_word\n",
    "\n",
    "optimized_word = optimize_word(\n",
    "    word2vec_model, english_words, start_word=\"stoner\", n_steps=5, n_sample=5, epsilon=0.1\n",
    ")\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meGhl6H1M4uX",
    "outputId": "b07e975e-2611-4ea8-812f-ff9dbe940195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Madi 0.76\n",
      "query: nonsectarian 0.75\n",
      "query: dastardly 0.76\n",
      "query: unedifying 0.75\n",
      "query: middy 0.76\n",
      "Step 1, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 10: No improvement, trying a bigger step.\n",
      "query: regret 0.77\n",
      "query: superficial 0.78\n",
      "query: surliness 0.74\n",
      "query: stude 0.79\n",
      "query: unsigned 0.77\n",
      "Step 2, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 10: No improvement, trying a bigger step.\n",
      "query: vestry 0.77\n",
      "query: countermand 0.76\n",
      "query: twiddly 0.76\n",
      "query: bricklaying 0.75\n",
      "query: revisitation 0.75\n",
      "Step 3, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 10: No improvement, trying a bigger step.\n",
      "query: intentioned 0.77\n",
      "query: outsmart 0.78\n",
      "query: magnificence 0.75\n",
      "query: disagreed 0.75\n",
      "query: magistrature 0.73\n",
      "Step 4, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 10: No improvement, trying a bigger step.\n",
      "query: Barsac 0.74\n",
      "query: muntjac 0.76\n",
      "query: frothing 0.75\n",
      "query: implore 0.75\n",
      "query: afikomen 0.77\n",
      "Step 5, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 10: No improvement, trying a bigger step.\n",
      "Optimized word: 'stoner'\n",
      "The word with the maximum score is 'stoner' with a score of 0.81\n"
     ]
    }
   ],
   "source": [
    "def optimize_word(word2vec_model, english_words, start_word, n_steps=5, n_sample=5, epsilon=0.1):\n",
    "    current_word = start_word\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        current_score = manager.query(current_word)\n",
    "\n",
    "        best_word, best_score = None, -float('inf')\n",
    "        worst_word, worst_score = None, float('inf')\n",
    "\n",
    "        for _ in range(n_sample):\n",
    "            new_word = random.choice(english_words)\n",
    "            while new_word in manager.queried_words or new_word not in word2vec_model:\n",
    "                new_word = random.choice(english_words)\n",
    "\n",
    "            new_score = manager.query(new_word)\n",
    "            if new_score > best_score:\n",
    "                best_word, best_score = new_word, new_score\n",
    "            if new_score < worst_score:\n",
    "                worst_word, worst_score = new_word, new_score\n",
    "\n",
    "        if best_score > current_score:\n",
    "            target_word, target_score = best_word, best_score\n",
    "        else:\n",
    "            target_word, target_score = worst_word, worst_score\n",
    "\n",
    "        gradient = calculate_gradient(\n",
    "            word2vec_model[current_word],\n",
    "            word2vec_model[target_word],\n",
    "            target_score - current_score\n",
    "        )\n",
    "\n",
    "        for attempt in range(10):\n",
    "            step_size = epsilon * (10 ** attempt)\n",
    "            new_vector = word2vec_model[current_word] + step_size * gradient\n",
    "            similar_word = word2vec_model.similar_by_vector(new_vector, topn=1)[0][0]\n",
    "            similar_word_score = manager.query(similar_word)\n",
    "\n",
    "            if similar_word_score > current_score:\n",
    "                current_word = similar_word\n",
    "                current_score = similar_word_score\n",
    "                print(f\"Step {step+1}: New word is '{current_word}' with score {current_score}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Step {step+1}, Attempt {attempt+1}: No improvement, trying a bigger step.\")\n",
    "\n",
    "    return current_word\n",
    "\n",
    "\n",
    "optimized_word = optimize_word(\n",
    "    word2vec_model, english_words, start_word=\"stoner\", n_steps=5, n_sample=5, epsilon=1\n",
    ")\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QE3bgPAM40O",
    "outputId": "0fd25e7a-728d-4c82-8861-640e095142c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: banana 0.81\n",
      "query: faun 0.8\n",
      "query: constrictor 0.77\n",
      "query: curator 0.77\n",
      "query: pried 0.76\n",
      "query: surplice 0.76\n",
      "Step 1, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 10: No improvement, trying a bigger step.\n",
      "query: iconostasis 0.78\n",
      "query: Llandeilo 0.74\n",
      "query: cuff 0.75\n",
      "query: coonskin 0.75\n",
      "query: grievous 0.76\n",
      "Step 2, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 10: No improvement, trying a bigger step.\n",
      "query: outsider 0.8\n",
      "query: Belone 0.77\n",
      "query: Dashnak 0.76\n",
      "query: musicologist 0.76\n",
      "query: adjustable 0.75\n",
      "Step 3, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 10: No improvement, trying a bigger step.\n",
      "query: moonless 0.82\n",
      "query: antiparasitic 0.78\n",
      "query: oclock 0.79\n",
      "query: thrush 0.78\n",
      "query: Montepulciano 0.76\n",
      "Step 4, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 4: New word is 'moonless' with score 0.82\n",
      "query: sloka 0.78\n",
      "query: Sept 0.78\n",
      "query: Andries 0.76\n",
      "query: margined 0.76\n",
      "query: healthfully 0.75\n",
      "Step 5, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 10: No improvement, trying a bigger step.\n",
      "Optimized word: 'moonless'\n",
      "The word with the maximum score is 'moonless' with a score of 0.82\n"
     ]
    }
   ],
   "source": [
    "'obstacle'\n",
    "'banana'\n",
    "\n",
    "optimized_word = optimize_word(\n",
    "    word2vec_model, english_words, start_word=\"banana\", n_steps=5, n_sample=5, epsilon=0.1\n",
    ")\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4x1vgT1Lp33",
    "outputId": "46ac9c9a-fcc1-4667-c1a3-8a63fae87413"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stoner', 0.81)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.get_max_score_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTCdFEkYWNgi",
    "outputId": "2dc33499-aa51-4a97-e979-610a6756e495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Jenson 0.78\n",
      "Step 1, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 1, Attempt 10: No improvement, trying a bigger step.\n",
      "query: sinking 0.78\n",
      "Step 2, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 2, Attempt 10: No improvement, trying a bigger step.\n",
      "query: indirectness 0.75\n",
      "Step 3, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 3, Attempt 10: No improvement, trying a bigger step.\n",
      "query: stagnancy 0.79\n",
      "Step 4, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 4, Attempt 10: No improvement, trying a bigger step.\n",
      "query: Bucky 0.78\n",
      "Step 5, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 5, Attempt 10: No improvement, trying a bigger step.\n",
      "query: bundler 0.75\n",
      "Step 6, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 2: No improvement, trying a bigger step.\n",
      "query: terra_forming 0.79\n",
      "Step 6, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 6, Attempt 10: No improvement, trying a bigger step.\n",
      "query: rented 0.78\n",
      "Step 7, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 7, Attempt 10: No improvement, trying a bigger step.\n",
      "query: out 0.78\n",
      "Step 8, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 8, Attempt 10: No improvement, trying a bigger step.\n",
      "query: aseptically 0.75\n",
      "Step 9, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 9, Attempt 10: No improvement, trying a bigger step.\n",
      "query: floatation 0.78\n",
      "Step 10, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 2: No improvement, trying a bigger step.\n",
      "query: Flannery_O'Conner 0.76\n",
      "Step 10, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 10, Attempt 10: No improvement, trying a bigger step.\n",
      "query: mermaid 0.8\n",
      "Step 11, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 2: No improvement, trying a bigger step.\n",
      "query: arXiv_####.#### 0.77\n",
      "Step 11, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 11, Attempt 10: No improvement, trying a bigger step.\n",
      "query: osteodystrophy 0.77\n",
      "Step 12, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 12, Attempt 10: No improvement, trying a bigger step.\n",
      "query: ranging 0.77\n",
      "Step 13, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 13, Attempt 10: No improvement, trying a bigger step.\n",
      "query: enema 0.79\n",
      "Step 14, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 3: No improvement, trying a bigger step.\n",
      "query: tropospheric_temperature 0.76\n",
      "Step 14, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 14, Attempt 10: No improvement, trying a bigger step.\n",
      "query: resile 0.73\n",
      "Step 15, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 15, Attempt 10: No improvement, trying a bigger step.\n",
      "query: handedness 0.73\n",
      "Step 16, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 3: No improvement, trying a bigger step.\n",
      "query: Devonian_shales 0.76\n",
      "Step 16, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 16, Attempt 10: No improvement, trying a bigger step.\n",
      "query: parliamentary 0.74\n",
      "Step 17, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 17, Attempt 10: No improvement, trying a bigger step.\n",
      "query: Makari 0.77\n",
      "Step 18, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 18, Attempt 10: No improvement, trying a bigger step.\n",
      "query: Miriam 0.76\n",
      "Step 19, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 3: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 19, Attempt 10: No improvement, trying a bigger step.\n",
      "query: null 0.78\n",
      "Step 20, Attempt 1: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 2: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 3: No improvement, trying a bigger step.\n",
      "query: astrobiologists 0.84\n",
      "Step 20, Attempt 4: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 5: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 6: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 7: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 8: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 9: No improvement, trying a bigger step.\n",
      "Step 20, Attempt 10: No improvement, trying a bigger step.\n",
      "Optimized word: 'chondritic'\n",
      "The word with the maximum score is 'chondritic' with a score of 0.85\n"
     ]
    }
   ],
   "source": [
    "'obstacle'\n",
    "'banana'\n",
    "\n",
    "optimized_word = optimize_word(\n",
    "    word2vec_model, english_words, start_word=\"chondritic\", n_steps=20, n_sample=1, epsilon=10\n",
    ")\n",
    "\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kI-QdYPXZXpZ",
    "outputId": "881bcadb-bf30-4ad2-e973-178c7601a6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: floresiensis 0.78\n",
      "query: olivine_crystals 0.78\n",
      "query: plagioclase 0.8\n",
      "query: isotope_ratios 0.76\n",
      "query: asteroidal 0.92\n",
      "query: spheroidal 0.83\n",
      "query: stromatolite 0.84\n",
      "query: Fig._1b 0.76\n",
      "query: granitoids 0.8\n",
      "query: planetesimal 0.88\n",
      "query: stony_meteorite 0.86\n",
      "query: phyllosilicates 0.77\n",
      "query: lunar_meteorites 0.87\n",
      "query: nebular 0.85\n",
      "query: mineralogically 0.78\n",
      "query: circumstellar_disk 0.85\n",
      "query: Cretaceous_Paleogene_boundary 0.78\n",
      "query: post_perovskite 0.77\n",
      "query: graviton 0.82\n",
      "query: olivines 0.78\n",
      "query: comet_nuclei 0.82\n",
      "query: pyroxene 0.79\n",
      "query: anorthosite 0.81\n",
      "query: moonlet 0.85\n",
      "query: electrons_orbiting 0.8\n",
      "query: lunar_highlands 0.8\n",
      "query: planktonic_foraminifera 0.8\n",
      "query: aragonite 0.81\n",
      "query: hematite_mineral 0.79\n",
      "query: silicate_mineral 0.78\n",
      "query: phlogopite 0.78\n",
      "query: circumstellar 0.85\n",
      "query: uraninite 0.8\n",
      "query: magma_ocean 0.8\n",
      "query: allanite 0.78\n",
      "Step 1: New word is 'asteroidal' with score 0.92\n",
      "query: protoplanet 0.84\n",
      "query: icy_moons 0.8\n",
      "query: extrasolar 0.84\n",
      "query: lunar_crust 0.83\n",
      "query: Gliese_###c 0.82\n",
      "query: binary_asteroid 0.91\n",
      "query: elongated_orbits 0.82\n",
      "query: protoplanetary_disk 0.84\n",
      "query: meteoritic_material 0.84\n",
      "query: cometary_material 0.83\n",
      "query: Kuiper_belt_objects 0.8\n",
      "query: gaseous_planet 0.84\n",
      "query: Epsilon_Eridani 0.81\n",
      "query: Upsilon_Andromedae 0.81\n",
      "query: type_Ia_supernova 0.81\n",
      "query: planetary_nebulae 0.85\n",
      "query: Kuiper_Belt_Objects 0.81\n",
      "query: globulars 0.8\n",
      "query: basaltic_rocks 0.81\n",
      "query: protoplanetary 0.84\n",
      "query: Corot_7b 0.77\n",
      "query: VY_Canis_Majoris 0.78\n",
      "query: HL_Tau 0.76\n",
      "query: retrograde_orbit 0.83\n",
      "query: Arches_Cluster 0.76\n",
      "query: lensing_galaxy 0.8\n",
      "query: infall 0.76\n",
      "query: Fomalhaut_b 0.77\n",
      "query: silicate_dust 0.8\n",
      "query: solar_nebula 0.86\n",
      "query: interstellar_dust_grains 0.83\n",
      "query: circumstellar_disks 0.83\n",
      "query: spatially_resolved 0.74\n",
      "query: KBOs 0.75\n",
      "query: crystalline_silicates 0.77\n",
      "query: radioactive_nuclei 0.79\n",
      "query: Beta_Pictoris 0.8\n",
      "query: Gravitational_waves 0.81\n",
      "query: ejecta 0.84\n",
      "query: mineral_olivine 0.79\n",
      "query: filamentary_structure 0.73\n",
      "query: Kuiper_belt 0.82\n",
      "query: X_ray_binaries 0.77\n",
      "query: meteor_impacts 0.82\n",
      "Step 2: No improvement.\n",
      "Step 3: No improvement.\n",
      "Step 4: No improvement.\n",
      "Step 5: No improvement.\n",
      "Step 6: No improvement.\n",
      "Step 7: No improvement.\n",
      "Step 8: No improvement.\n",
      "Step 9: No improvement.\n",
      "Step 10: No improvement.\n",
      "Step 11: No improvement.\n",
      "Step 12: No improvement.\n",
      "Step 13: No improvement.\n",
      "Step 14: No improvement.\n",
      "Step 15: No improvement.\n",
      "Step 16: No improvement.\n",
      "Step 17: No improvement.\n",
      "Step 18: No improvement.\n",
      "Step 19: No improvement.\n",
      "Step 20: No improvement.\n",
      "Step 21: No improvement.\n",
      "Step 22: No improvement.\n",
      "Step 23: No improvement.\n",
      "Step 24: No improvement.\n",
      "Step 25: No improvement.\n",
      "Step 26: No improvement.\n",
      "Step 27: No improvement.\n",
      "Step 28: No improvement.\n",
      "Step 29: No improvement.\n",
      "Step 30: No improvement.\n",
      "Step 31: No improvement.\n",
      "Step 32: No improvement.\n",
      "Step 33: No improvement.\n",
      "Step 34: No improvement.\n",
      "Step 35: No improvement.\n",
      "Step 36: No improvement.\n",
      "Step 37: No improvement.\n",
      "Step 38: No improvement.\n",
      "Step 39: No improvement.\n",
      "Step 40: No improvement.\n",
      "Step 41: No improvement.\n",
      "Step 42: No improvement.\n",
      "Step 43: No improvement.\n",
      "Step 44: No improvement.\n",
      "Step 45: No improvement.\n",
      "Step 46: No improvement.\n",
      "Step 47: No improvement.\n",
      "Step 48: No improvement.\n",
      "Step 49: No improvement.\n",
      "Step 50: No improvement.\n",
      "Optimized word: 'asteroidal'\n",
      "The word with the maximum score is 'asteroidal' with a score of 0.92\n"
     ]
    }
   ],
   "source": [
    "# Function to optimize word based on closest words and their scores\n",
    "def optimize_word_closest(word2vec_model, start_word, n_steps=5, n_words=10):\n",
    "    current_word = start_word\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Getting the 10 closest words\n",
    "        closest_words = [word for word, _ in word2vec_model.most_similar(current_word, topn=n_words)]\n",
    "\n",
    "        # Querying scores of the closest words\n",
    "        scores = {word: manager.query(word) for word in closest_words}\n",
    "\n",
    "        # Getting the word with the highest score\n",
    "        best_word, best_score = max(scores.items(), key=lambda x: x[1])\n",
    "\n",
    "        if best_score > manager.query(current_word):\n",
    "            current_word = best_word\n",
    "            print(f\"Step {step+1}: New word is '{current_word}' with score {best_score}\")\n",
    "        else:\n",
    "            print(f\"Step {step+1}: No improvement.\")\n",
    "\n",
    "    return current_word\n",
    "\n",
    "# Example of usage\n",
    "optimized_word = optimize_word_closest(word2vec_model, start_word=\"asteroidal\", n_steps=50, n_words=50)\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOF8tadKZXc-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjO9XULWasCD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VxN2lE71ar28",
    "outputId": "0468bf0f-00a6-4ae9-aec3-35a731887c5f"
   },
   "outputs": [],
   "source": [
    "# Function to optimize word based on closest words and their scores\n",
    "def optimize_word_closest(word2vec_model, start_word, n_steps=5, n_words=10):\n",
    "    current_word = start_word\n",
    "    visited_words = set([start_word])\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Getting the 10 closest words excluding visited words\n",
    "        closest_words = [word for word, _ in word2vec_model.most_similar(current_word, topn=n_words)\n",
    "                         if word not in visited_words]\n",
    "\n",
    "        if not closest_words:\n",
    "            print(f\"No unvisited closest words found for '{current_word}'. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "        # Querying scores of the closest words\n",
    "        scores = {word: manager.query(word) for word in closest_words}\n",
    "\n",
    "        # Getting the word with the highest score\n",
    "        best_word, best_score = max(scores.items(), key=lambda x: x[1])\n",
    "\n",
    "        current_word = best_word\n",
    "        visited_words.add(current_word)\n",
    "        print(f\"Step {step+1}: New word is '{current_word}' with score {best_score}\")\n",
    "\n",
    "    return current_word\n",
    "\n",
    "\n",
    "\n",
    "optimized_word = optimize_word_closest(word2vec_model, start_word=\"asteroidal\", n_steps=50, n_words=50)\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR5VWValcA8w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "GVupx_KpcAf-",
    "outputId": "e3dbfe8d-8fc6-4ac9-8044-95f08bbc4848"
   },
   "outputs": [],
   "source": [
    "# Function to optimize word based on closest words and their scores\n",
    "def optimize_word_closest(word2vec_model, start_word, n_steps=5, n_words=10):\n",
    "    current_word = start_word\n",
    "    visited_words = set([start_word])\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Getting the 10 closest words excluding visited words\n",
    "        closest_words = [word for word, _ in word2vec_model.most_similar(current_word, topn=n_words)\n",
    "                         if word not in visited_words]\n",
    "\n",
    "        if not closest_words:\n",
    "            print(f\"No unvisited closest words found for '{current_word}'. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "        # Querying scores of the closest words\n",
    "        scores = {}\n",
    "        for word in closest_words:\n",
    "          try:\n",
    "            scores[word] = manager.query(word)\n",
    "          except KeyError as e:\n",
    "            print(word)\n",
    "            print(e)\n",
    "            raise e\n",
    "\n",
    "        # Getting the word with the highest score\n",
    "        best_word, best_score = max(scores.items(), key=lambda x: x[1])\n",
    "\n",
    "        current_word = best_word\n",
    "        visited_words.add(current_word)\n",
    "        print(f\"Step {step+1}: New word is '{current_word}' with score {best_score}\")\n",
    "\n",
    "    return current_word\n",
    "\n",
    "\n",
    "\n",
    "optimized_word = optimize_word_closest(word2vec_model, start_word=\"asteroids_comets\", n_steps=50, n_words=50)\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c57gxZPBarXX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b86Q_jDmZXRe",
    "outputId": "3bc8f867-28e3-4c7f-9110-cb9ad7a6dcfa"
   },
   "outputs": [],
   "source": [
    "optimized_word = optimize_word_closest(word2vec_model, start_word=\"asteroidal\", n_steps=5, n_words=200)\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "CKFWr9hXZWsW",
    "outputId": "b1993d4c-764d-4392-c3c0-df3e8d93697c"
   },
   "outputs": [],
   "source": [
    "query(\"asteroids\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWjwrjYZWM8b"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Your dictionary\n",
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO65WWk5WMav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-p3N138X_PN"
   },
   "source": [
    "# Second challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co3Xdbi_hkGX",
    "outputId": "682c5f10-f704-4f1f-d462-501d333d9d0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.75}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "input_data = \"cat jungle hat space water\"\n",
    "\n",
    "def query(input_data):\n",
    "    response = requests.post('http://semantle2.advml.com/score', json={'data': input_data})\n",
    "    return response.json()\n",
    "\n",
    "query(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YiUpj-NhkRa"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class QueryManager:\n",
    "    def __init__(self):\n",
    "        self.queried_words = {}\n",
    "\n",
    "    def query(self, input_data):\n",
    "        if input_data in self.queried_words:\n",
    "            return self.queried_words[input_data]\n",
    "\n",
    "        response = requests.post('http://semantle.advml.com/score', json={'data': input_data})\n",
    "        score = response.json()[\"message\"]\n",
    "        print(f\"query: {input_data} {score}\")\n",
    "        self.queried_words[input_data] = score\n",
    "        return score\n",
    "\n",
    "    def get_max_score_word(self):\n",
    "        max_key = max(self.queried_words, key=lambda k: self.queried_words[k])\n",
    "        return max_key, self.queried_words[max_key]\n",
    "\n",
    "# Example usage:\n",
    "manager = QueryManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4dwSibHhkiN"
   },
   "outputs": [],
   "source": [
    "# Function to optimize word based on closest words and their scores\n",
    "def optimize_word_closest(word2vec_model, start_word, n_steps=5, n_words=10):\n",
    "    current_word = start_word\n",
    "    visited_words = set([start_word])\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Getting the 10 closest words excluding visited words\n",
    "        closest_words = [word for word, _ in word2vec_model.most_similar(current_word, topn=n_words)\n",
    "                         if word not in visited_words]\n",
    "\n",
    "        if not closest_words:\n",
    "            print(f\"No unvisited closest words found for '{current_word}'. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "        # Querying scores of the closest words\n",
    "        scores = {}\n",
    "        for word in closest_words:\n",
    "          try:\n",
    "            scores[word] = manager.query(word)\n",
    "          except KeyError as e:\n",
    "            print(word)\n",
    "            print(e)\n",
    "            raise e\n",
    "\n",
    "        # Getting the word with the highest score\n",
    "        best_word, best_score = max(scores.items(), key=lambda x: x[1])\n",
    "\n",
    "        current_word = best_word\n",
    "        visited_words.add(current_word)\n",
    "        print(f\"Step {step+1}: New word is '{current_word}' with score {best_score}\")\n",
    "\n",
    "    return current_word\n",
    "\n",
    "\n",
    "\n",
    "optimized_word = optimize_word_closest(word2vec_model, start_word=\"asteroids_comets\", n_steps=50, n_words=50)\n",
    "print(f\"Optimized word: '{optimized_word}'\")\n",
    "\n",
    "max_word, max_score = manager.get_max_score_word()\n",
    "print(f\"The word with the maximum score is '{max_word}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P2rVBBDhknA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1C9F7cwhkrv"
   },
   "outputs": [],
   "source": [
    "# Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQGW-Kb1g8KW",
    "outputId": "a960d66a-b9ad-407c-eef3-112e99f6ebdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dg6ittdwg8qV",
    "outputId": "e2b6d681-3dae-47e6-a17f-501f20e0fe94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from nltk.corpus import words\n",
    "import nltk\n",
    "\n",
    "# Downloading the word2vec word embeddings\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")  # you can choose another model if you prefer\n",
    "\n",
    "# Downloading the words corpus\n",
    "nltk.download('words')\n",
    "english_words = words.words()\n",
    "\n",
    "# Now you can use `word2vec_model` to get the embeddings and `english_words` to get the English words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C-ivBHcg8uY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWjOpWVig8zp",
    "outputId": "65e3c701-ee87-4b30-edef-78e898f31fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: dog forest hat space wind 0.74\n",
      "query: dog jogging table earth wind 0.77\n",
      "query: dogs snowboarding_mountain_biking dining_room mankind wind 0.83\n",
      "query: dogs snowboarding_mountain_biking dining_room mankind gale 0.83\n",
      "The combination with the maximum score is 'dogs snowboarding_mountain_biking dining_room mankind wind' with a score of 0.83\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "class QueryManager:\n",
    "    def __init__(self):\n",
    "        self.queried_words = {}\n",
    "\n",
    "    def query(self, words_list):\n",
    "        words_key = ' '.join(words_list)  # Convert list to string to use as a key\n",
    "        if words_key in self.queried_words:\n",
    "            return self.queried_words[words_key]\n",
    "\n",
    "        response = requests.post('http://semantle2.advml.com/score', json={'data': words_key})\n",
    "        response_data = response.json()\n",
    "\n",
    "        if 'flag' in response_data:\n",
    "            print(response.text)\n",
    "\n",
    "        score = response_data[\"message\"]\n",
    "        print(f\"query: {words_key} {score}\")\n",
    "        self.queried_words[words_key] = score\n",
    "        return score\n",
    "\n",
    "    def get_max_score_word(self):\n",
    "        max_key = max(self.queried_words, key=lambda k: self.queried_words[k])\n",
    "        return max_key, self.queried_words[max_key]\n",
    "\n",
    "# Example usage:\n",
    "manager = QueryManager()\n",
    "\n",
    "# Reading from json and assigning it to manager's queried_words\n",
    "# with open('data.json', 'r', encoding='utf-8') as file:\n",
    "#     manager.queried_words = json.load(file)\n",
    "\n",
    "# Now the manager's queried_words has been populated with the data from the JSON file\n",
    "\n",
    "\n",
    "manager.query([\"dog\", \"forest\", \"hat\", \"space\", \"wind\"])\n",
    "manager.query([\"dog\", \"jogging\", \"table\", \"earth\", \"wind\"])\n",
    "manager.query(['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'wind'])\n",
    "manager.query(['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'gale'])\n",
    "manager.get_max_score_word()\n",
    "\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyomfqcVg832"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyagI60ag87i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_word_closest(word2vec_model, start_words, n_steps=5, n_words=20, temperature=0.5):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        for i, word in enumerate(current_words):\n",
    "            # Get n_words most similar words to the current one\n",
    "            all_similar_words = word2vec_model.most_similar(word, topn=n_words * 2)  # Increase the pool for diversity\n",
    "            similar_words = [w for w, _ in all_similar_words if w not in current_words]\n",
    "\n",
    "            # Gather scores for similar words to enable intelligent sampling\n",
    "            word_scores = []\n",
    "            for similar_word in similar_words:\n",
    "                combined_words = current_words[:]\n",
    "                combined_words[i] = similar_word\n",
    "                combined_words_str = ' '.join(combined_words)\n",
    "                if combined_words_str in manager.queried_words:\n",
    "                    word_scores.append(manager.queried_words[combined_words_str])\n",
    "                else:\n",
    "                    # This part involves querying the API for the score, you'd handle it as needed\n",
    "                    score = manager.query(combined_words)  # Assume this function exists and works as intended\n",
    "                    word_scores.append(score)\n",
    "\n",
    "            # If no new scores are found, skip to the next word\n",
    "            if not word_scores:\n",
    "                continue\n",
    "\n",
    "            # Select a word using intelligent sampling\n",
    "            selected_word = intelligent_sampling(similar_words, word_scores, temperature=temperature)\n",
    "            new_combination = current_words[:]\n",
    "            new_combination[i] = selected_word\n",
    "            score = manager.query(new_combination)\n",
    "\n",
    "            # Update the best score and combination found so far\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_combination = new_combination[:]\n",
    "                print(f\"Step {step+1}, Word {i+1}: New combination is '{new_combination}' with score {score}\")\n",
    "\n",
    "        current_words = best_combination\n",
    "        if not best_combination:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "# Assuming the existence of the required objects and methods:\n",
    "# - word2vec_model: A word2vec model object with a most_similar method\n",
    "# - manager: An instance of QueryManager with an appropriate query method\n",
    "\n",
    "sw = ['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'gale']\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw, n_steps=10, n_words=50\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nz0u-9__hX4m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmG5ohRxhYWS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def genetic_optimization(word2vec_model, start_words, n_steps=5, population_size=10, mutation_rate=0.1):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    # Initialize population with the start_words being one of them\n",
    "    population = [start_words]\n",
    "    for _ in range(population_size - 1):\n",
    "        population.append(random.sample(start_words, len(start_words)))\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Evaluate all combinations\n",
    "        scored_population = []\n",
    "        for combination in population:\n",
    "            score = manager.query(combination)\n",
    "            scored_population.append((score, combination))\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_combination = combination\n",
    "\n",
    "        # Sort combinations by their score\n",
    "        scored_population.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Selection: Pick the top scoring combinations\n",
    "        survivors = scored_population[:population_size // 2]\n",
    "\n",
    "        # Crossover: Create new combinations by merging survivors\n",
    "        new_population = []\n",
    "        for i in range(len(survivors) // 2):\n",
    "            for j in range(i + 1, len(survivors)):\n",
    "                combo1 = survivors[i][1]\n",
    "                combo2 = survivors[j][1]\n",
    "                # Simple one-point crossover\n",
    "                point = random.randint(1, len(combo1) - 2)\n",
    "                new_combination = combo1[:point] + combo2[point:]\n",
    "                new_population.append(new_combination)\n",
    "\n",
    "        # Mutation: Randomly change one word in the combination\n",
    "        for combination in new_population:\n",
    "            if random.random() < mutation_rate:\n",
    "                word_to_replace = random.choice(combination)\n",
    "                replacements = word2vec_model.most_similar(word_to_replace, topn=1)\n",
    "                if replacements:\n",
    "                    replacement_word = replacements[0][0]\n",
    "                    combination[combination.index(word_to_replace)] = replacement_word\n",
    "\n",
    "        # Add best combinations and new ones to the population for the next step\n",
    "        population = [combo for _, combo in survivors] + new_population\n",
    "\n",
    "        print(f\"Step {step+1}: Best combination so far is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "# Assuming the existence of the required objects and methods:\n",
    "# - word2vec_model: A word2vec model object with a most_similar method\n",
    "# - manager: An instance of QueryManager with an appropriate query method\n",
    "\n",
    "optimized_combination = genetic_optimization(\n",
    "    word2vec_model, start_words=[\"word1\", \"word2\", \"word3\", \"word4\", \"word5\"],\n",
    "    n_steps=50, population_size=20, mutation_rate=0.2\n",
    ")\n",
    "\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaByzOUYhYbr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UAh_8vDXhYha",
    "outputId": "97f67588-9969-4fa9-9ab1-f620e717d824"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_word_closest(word2vec_model, start_words, n_steps=5, n_words=20, temperature=0.5):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "    visited_combinations = set()\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        improvement_found = False\n",
    "        for i, word in enumerate(current_words):\n",
    "            # Get n_words most similar words to the current one\n",
    "            all_similar_words = word2vec_model.most_similar(word, topn=n_words * 2)  # Increase the pool for diversity\n",
    "            similar_words = [w for w, _ in all_similar_words if w not in current_words]\n",
    "\n",
    "            # Gather scores for similar words to enable intelligent sampling\n",
    "            word_scores = []\n",
    "            for similar_word in similar_words:\n",
    "                combined_words = current_words[:]\n",
    "                combined_words[i] = similar_word\n",
    "                combined_words_str = ' '.join(combined_words)\n",
    "                if combined_words_str in manager.queried_words:\n",
    "                    word_scores.append(manager.queried_words[combined_words_str])\n",
    "                else:\n",
    "                    # Query the API for the score\n",
    "                    score = manager.query(combined_words)\n",
    "                    word_scores.append(score)\n",
    "\n",
    "            # If no new scores are found, skip to the next word\n",
    "            if not word_scores:\n",
    "                continue\n",
    "\n",
    "            # Select a word using intelligent sampling\n",
    "            selected_word = intelligent_sampling(similar_words, word_scores, temperature=temperature)\n",
    "            new_combination = current_words[:]\n",
    "            new_combination[i] = selected_word\n",
    "            new_combination_str = ' '.join(new_combination)\n",
    "            if new_combination_str not in visited_combinations:\n",
    "                visited_combinations.add(new_combination_str)\n",
    "                score = manager.query(new_combination)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_combination = new_combination[:]\n",
    "                    improvement_found = True\n",
    "                    print(f\"Step {step+1}, Word {i+1}: New combination is '{new_combination}' with score {score}\")\n",
    "\n",
    "        if not improvement_found:\n",
    "            # Sample randomly from the highest ranking not already seen values\n",
    "            new_scores_and_combos = [(score, combo) for score, combo in zip(word_scores, similar_words)\n",
    "                                     if ' '.join(combo) not in visited_combinations]\n",
    "            if new_scores_and_combos:\n",
    "                new_scores_and_combos.sort(reverse=True)  # Highest scores first\n",
    "                _, new_word = new_scores_and_combos[0]  # Select the highest score not seen\n",
    "                current_words[i] = new_word\n",
    "                improvement_found = True\n",
    "\n",
    "        if improvement_found:\n",
    "            current_words = best_combination\n",
    "        else:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "# Assuming the existence of the required objects and methods\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'gale'],\n",
    "    n_steps=10, n_words=50\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sh5CKDTj15H"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Your dictionary\n",
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DATnoSOBhYnW",
    "outputId": "423d3af3-a3db-449d-95c0-4b25b66f97e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'dogs snowboarding_mountain_biking dining_room mankind Repair_Remote_Monitoring' with a score of 0.84\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUuJAeorhYxl",
    "outputId": "aa41600a-4f1d-4480-be20-d8dc362593ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: snowboarder mankind ski human_beings diningroom 0.85\n",
      "The combination with the maximum score is 'snowboarder mankind ski human_beings diningroom' with a score of 0.85\n"
     ]
    }
   ],
   "source": [
    "sw = ['snowboarder', 'mankind', 'ski', 'human_beings', 'diningroom']\n",
    "\n",
    "manager.query(sw)\n",
    "\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYSsmzmqmwYT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AmPQF9Rmwwm",
    "outputId": "e6cc86d1-1f71-4fd1-ad84-347d0945ade0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: freestyle_skiers mankind ski civilized_society diningroom 0.81\n",
      "Step 1: New best combination is '['freestyle_skiers', 'mankind', 'ski', 'civilized_society', 'diningroom']' with score 0.81\n",
      "query: skiiers mankind ski Humans diningroom 0.82\n",
      "Step 1: New best combination is '['skiiers', 'mankind', 'ski', 'Humans', 'diningroom']' with score 0.82\n",
      "query: snowmobiler mankind ski mortals diningroom 0.82\n",
      "query: freeskiing mankind ski sentient_beings diningroom 0.81\n",
      "query: Snowboarder mankind ski sentient_beings diningroom 0.82\n",
      "query: downhiller mankind ski Untermenschen diningroom 0.81\n",
      "query: snowboard mankind ski fallible_humans diningroom 0.82\n",
      "query: snowboarders mankind ski homo_sapiens diningroom 0.82\n",
      "query: Tricia_Byrnes mankind ski humanness diningroom 0.82\n",
      "query: snowboarders mankind ski sentient_creatures diningroom 0.82\n",
      "query: skiers mankind ski humankind diningroom 0.82\n",
      "query: freeskiing mankind ski beings diningroom 0.82\n",
      "query: Jossi_Wells mankind ski subhumans diningroom 0.8\n",
      "query: sledder mankind ski sentient_creature diningroom 0.81\n",
      "query: Jossi_Wells mankind ski inferior_beings diningroom 0.8\n",
      "query: freeskiing mankind ski civilized_societies diningroom 0.8\n",
      "query: snowshoer mankind ski rational_beings diningroom 0.82\n",
      "query: motocross_rider mankind ski subhumans diningroom 0.83\n",
      "Step 1: New best combination is '['motocross_rider', 'mankind', 'ski', 'subhumans', 'diningroom']' with score 0.83\n",
      "query: skiiers mankind ski jinns diningroom 0.8\n",
      "query: skiing mankind Waterville_Valley human_beings diningroom 0.82\n",
      "query: freestyle_skiers mankind telemark human_beings diningroom 0.83\n",
      "query: BMX_rider mankind ski_racers human_beings diningroom 0.83\n",
      "query: snowmobiler mankind snowshoe human_beings diningroom 0.83\n",
      "query: moguls_skier mankind backcountry_ski human_beings diningroom 0.82\n",
      "query: Crispin_Lipscomb mankind ski_racers human_beings diningroom 0.81\n",
      "query: skateboarder mankind Nordic_ski human_beings diningroom 0.83\n",
      "query: Snowboarding mankind snowshoe human_beings diningroom 0.83\n",
      "query: Tara_Dakides mankind snowboarders human_beings diningroom 0.82\n",
      "query: backcountry_skier mankind Chamonix human_beings diningroom 0.83\n",
      "query: telemark_skier mankind Ski_Resort human_beings diningroom 0.83\n",
      "query: freestyle_skier mankind ski_resorts human_beings diningroom 0.83\n",
      "query: Snowboarding mankind snowboard human_beings diningroom 0.83\n",
      "query: slopestyle mankind snowboarders human_beings diningroom 0.82\n",
      "query: skiier mankind downhill_skiing human_beings diningroom 0.83\n",
      "query: downhiller mankind telemark human_beings diningroom 0.83\n",
      "query: alpine_skier mankind Bridger_Bowl human_beings diningroom 0.83\n",
      "query: snowboarding mankind Ski human_beings diningroom 0.84\n",
      "Step 1: New best combination is '['snowboarding', 'mankind', 'Ski', 'human_beings', 'diningroom']' with score 0.84\n",
      "query: telemark_skier mankind Smugglers_Notch human_beings diningroom 0.82\n",
      "query: snowboarder mankind ski fleshly_desires flagstone_patio 0.82\n",
      "query: snowboarder mankind ski fallible_human_beings wrought_iron_chandelier 0.81\n",
      "query: snowboarder mankind ski subhumans inglenook 0.81\n",
      "query: snowboarder mankind ski posthumans maple_cabinetry 0.81\n",
      "query: snowboarder mankind ski posthumans frosted_glass_panels 0.81\n",
      "query: snowboarder mankind ski fellowmen demilune 0.82\n",
      "query: snowboarder mankind ski anthropomorphise screened_lanai 0.82\n",
      "query: snowboarder mankind ski sentient deep_soaking_tub 0.82\n",
      "query: snowboarder mankind ski inferior_beings deep_soaking_tub 0.82\n",
      "query: snowboarder mankind ski beings beige_ceramic_tile 0.82\n",
      "query: snowboarder mankind ski inferior_beings beadboard_wainscoting 0.81\n",
      "query: snowboarder mankind ski nonhuman_animals cabinets_granite_counters 0.83\n",
      "query: snowboarder mankind ski monotheists inglenook 0.81\n",
      "query: snowboarder mankind ski inferior_beings butler_pantry 0.82\n",
      "query: snowboarder mankind ski mortals exposed_brickwork 0.81\n",
      "query: snowboarder mankind ski God_dess beamed_ceiling 0.82\n",
      "query: snowboarder mankind ski sentient_creature banquette_seating 0.82\n",
      "query: snowboarder gnosis ski human_beings ceramic_tile_flooring 0.8\n",
      "query: snowboarder Divine_Creator ski human_beings jetted_tub 0.81\n",
      "query: snowboarder beings ski human_beings beige_ceramic_tile 0.82\n",
      "query: snowboarder gnosis ski human_beings fieldstone_fireplace 0.81\n",
      "query: snowboarder earthly ski human_beings screened_lanai 0.82\n",
      "query: snowboarder sentient_beings ski human_beings cabinets_granite_counters 0.82\n",
      "query: snowboarder mutantkind ski human_beings Vict_oak 0.79\n",
      "query: snowboarder finitude ski human_beings limestone_fireplace 0.81\n",
      "query: snowboarder Allah_Subhanahu_wa_ta'ala ski human_beings Vict_oak 0.8\n",
      "query: snowboarder finitude ski human_beings soaring_ceilings 0.81\n",
      "query: snowboarder civilization ski human_beings soaker_tub 0.81\n",
      "query: snowboarder planet ski human_beings demilune 0.81\n",
      "query: snowboarder vicegerent ski human_beings inglenook 0.82\n",
      "query: snowboarder beings ski human_beings demilune 0.81\n",
      "query: snowboarder Ps._##:# ski human_beings limestone_fireplace 0.81\n",
      "query: snowboarder finitude ski human_beings tiled_bathroom 0.82\n",
      "query: snowboarder monotheists ski human_beings beamed_ceiling 0.8\n",
      "query: snowboarder mankinds ski human_beings ensuite_bathroom 0.83\n",
      "query: snowboarder monotheists ski human_beings soaker_tub 0.81\n",
      "query: Freeskiing mankind ski human_beings Palladian_windows 0.81\n",
      "query: BMX_rider mankind ski human_beings ensuite_bath 0.81\n",
      "query: Crispin_Lipscomb mankind ski human_beings beveled_mirrors 0.81\n",
      "query: skiiers mankind ski human_beings wraparound_balcony 0.82\n",
      "query: mogul_skier mankind ski human_beings fieldstone_fireplace 0.82\n",
      "query: snowboarders mankind ski human_beings jetted_tub 0.82\n",
      "query: skiier mankind ski human_beings soaker_tub 0.83\n",
      "query: superpipe mankind ski human_beings cabinets_granite_counters 0.82\n",
      "query: freeskiers mankind ski human_beings wainscoted_walls 0.81\n",
      "query: backcountry_skier mankind ski human_beings Palladian_window 0.82\n",
      "query: telemark_skier mankind ski human_beings beige_ceramic_tile 0.82\n",
      "query: ski_racer mankind ski human_beings travertine_tile 0.81\n",
      "query: snowboarding mankind ski human_beings maple_cabinetry 0.82\n",
      "query: backcountry_skier mankind ski human_beings tiled_bathroom 0.82\n",
      "query: motocross_rider mankind ski human_beings jetted_tub 0.82\n",
      "query: snowshoer mankind ski human_beings cabinets_granite_counters 0.82\n",
      "query: Peter_Olenick mankind ski human_beings polished_floorboards 0.81\n",
      "query: skiiers mankind ski human_beings antique_mahogany 0.81\n",
      "query: snowboarder mankind downhill_skiing human_beings frosted_glass_panels 0.82\n",
      "query: snowboarder mankind Ski_Resort human_beings ensuite_bath 0.82\n",
      "query: snowboarder mankind Smugglers_Notch human_beings Palladian_window 0.81\n",
      "query: snowboarder mankind Snowsports human_beings tiled_shower 0.83\n",
      "query: snowboarder mankind telemark_gear human_beings Palladian_window 0.84\n",
      "query: snowboarder mankind Smugglers_Notch human_beings butler_pantry 0.81\n",
      "query: snowboarder mankind Skiing human_beings inglenook_fireplace 0.83\n",
      "query: snowboarder mankind ski_snowboard human_beings beveled_mirror 0.83\n",
      "query: snowboarder mankind Grands_Montets human_beings tiled_foyer 0.82\n",
      "query: snowboarder mankind telemark_gear human_beings antique_mahogany 0.83\n",
      "query: snowboarder mankind Bridger_Bowl human_beings fanlight 0.82\n",
      "query: snowboarder mankind Okemo_Mountain human_beings wood_wainscoting 0.82\n",
      "query: snowboarder mankind telemark_skis human_beings tiled_bathroom 0.84\n",
      "query: snowboarder mankind telemarking human_beings beige_ceramic_tile 0.83\n",
      "query: snowboarder mankind nordic_ski human_beings coffered_ceiling 0.81\n",
      "query: snowboarder mankind Smugglers_Notch human_beings travertine_tile 0.81\n",
      "query: snowboarder mankind skier human_beings coved_ceiling 0.83\n",
      "query: snowboarder mankind backcountry_skiing human_beings arched_ceilings 0.81\n",
      "query: snowboarder mankind Chamonix human_beings breakfasting_kitchen 0.83\n",
      "query: snowboarder mankind alpine_skiing human_beings frosted_glass_panels 0.82\n",
      "query: snowboarder peoples backcountry_ski human_beings diningroom 0.83\n",
      "query: snowboarder Genesis_#:#-# ski_resorts human_beings diningroom 0.81\n",
      "query: snowboarder perfectibility telemarking human_beings diningroom 0.83\n",
      "query: snowboarder Gd_Himself snowboard human_beings diningroom 0.82\n",
      "query: snowboarder peoples freestyle_skiers human_beings diningroom 0.83\n",
      "query: snowboarder Matt._##:## telemark_skiers human_beings diningroom 0.82\n",
      "query: snowboarder eschaton telemark human_beings diningroom 0.83\n",
      "query: snowboarder fallenness alpine_skiing human_beings diningroom 0.83\n",
      "query: snowboarder earthly snowboard human_beings diningroom 0.83\n",
      "query: snowboarder civilizations Sölden human_beings diningroom 0.8\n",
      "query: snowboarder humanity nordic human_beings diningroom 0.82\n",
      "query: snowboarder human telemarking human_beings diningroom 0.84\n",
      "query: snowboarder Allah_subhanahu_wa_ta'ala ski_racers human_beings diningroom 0.81\n",
      "query: snowboarder God alpine_ski human_beings diningroom 0.82\n",
      "query: snowboarder G_d ski_resort human_beings diningroom 0.83\n",
      "query: snowboarder cosmos nordic_ski human_beings diningroom 0.82\n",
      "query: snowboarder cosmos snowboard human_beings diningroom 0.82\n",
      "query: snowboarder perfectibility skier human_beings diningroom 0.83\n",
      "query: snowboarder fallenness ski_racers human_beings diningroom 0.83\n",
      "query: snowboarder fallenness backcountry_ski human_beings diningroom 0.83\n",
      "query: snowboarder mankind freestyle_skiers mortal_beings diningroom 0.83\n",
      "query: snowboarder mankind nordic humans diningroom 0.82\n",
      "query: snowboarder mankind nordic_ski fallenness diningroom 0.82\n",
      "query: snowboarder mankind telemark sentient diningroom 0.83\n",
      "query: snowboarder mankind skier subhumans diningroom 0.83\n",
      "query: snowboarder mankind Smugglers_Notch fallible_humans diningroom 0.81\n",
      "query: snowboarder mankind alpine_ski sentient_beings diningroom 0.83\n",
      "query: snowboarder mankind chairlift speciesism diningroom 0.8\n",
      "query: snowboarder mankind Ski creatures diningroom 0.83\n",
      "query: snowboarder mankind ski_racers Divine_Creator diningroom 0.81\n",
      "query: snowboarder mankind nordic border_Millona diningroom 0.81\n",
      "query: snowboarder mankind Chamonix Divine_Creator diningroom 0.81\n",
      "query: snowboarder mankind Fernie_Alpine_Resort speciesism diningroom 0.8\n",
      "query: snowboarder mankind snowshoe fleshly_desires diningroom 0.82\n",
      "query: snowboarder mankind Sölden jinns diningroom 0.79\n",
      "query: snowboarder mankind snowboarders Human_beings diningroom 0.83\n",
      "query: snowboarder mankind freestyle_skiers monotheists diningroom 0.82\n",
      "query: snowboarder mankind nordic nonhuman_animals diningroom 0.81\n",
      "query: backcountry_skier God_Almighty ski human_beings diningroom 0.81\n",
      "query: freestyle_skier Divine_Creator ski human_beings diningroom 0.81\n",
      "query: Crispin_Lipscomb humankind ski human_beings diningroom 0.81\n",
      "query: ski_racer Muslim_ummah ski human_beings diningroom 0.8\n",
      "query: climber blessedness ski human_beings diningroom 0.81\n",
      "query: snowmobiler cosmos ski human_beings diningroom 0.82\n",
      "query: Peter_Olenick eschaton ski human_beings diningroom 0.8\n",
      "query: sledder God_Himself ski human_beings diningroom 0.8\n",
      "query: alpine_skier Rom._#:# ski human_beings diningroom 0.81\n",
      "query: Nordic_skier humanity ski human_beings diningroom 0.82\n",
      "query: halfpipe endureth ski human_beings diningroom 0.81\n",
      "query: motocross_rider god ski human_beings diningroom 0.82\n",
      "query: freeskiing endureth ski human_beings diningroom 0.81\n",
      "query: freeskiing mutantkind ski human_beings diningroom 0.8\n",
      "query: Snowboarding monotheists ski human_beings diningroom 0.81\n",
      "query: freeskiing technological_singularity ski human_beings diningroom 0.82\n",
      "query: backcountry_skier planetwide ski human_beings diningroom 0.83\n",
      "query: freeskiers earthly_sojourn ski human_beings diningroom 0.81\n",
      "query: freestyle_skiers Allah_subhanahu_wa_ta'ala ski human_beings diningroom 0.79\n",
      "query: Jossi_Wells Genesis_#:#-# ski human_beings diningroom 0.8\n",
      "query: snowboarder technological_singularity ski speciesism diningroom 0.8\n",
      "query: snowboarder blessedness ski anthropomorphise diningroom 0.79\n",
      "query: snowboarder Allah_SWT ski fallenness diningroom 0.8\n",
      "query: snowboarder monotheists ski Divine_Creator diningroom 0.79\n",
      "query: snowboarder human ski inferior_beings diningroom 0.82\n",
      "query: snowboarder humans ski fleshly_desires diningroom 0.82\n",
      "query: snowboarder perfectibility ski inferior_beings diningroom 0.8\n",
      "query: snowboarder blessedness ski mortal_beings diningroom 0.8\n",
      "query: snowboarder eschaton ski anthropomorphise diningroom 0.79\n",
      "query: snowboarder Muslim_ummah ski mortal_beings diningroom 0.8\n",
      "query: snowboarder sentient_beings ski imperfect_beings diningroom 0.81\n",
      "query: snowboarder perfectibility ski sentient_beings diningroom 0.81\n",
      "query: snowboarder dwell_therein ski humankind diningroom 0.82\n",
      "query: snowboarder God ski sentient_intelligent diningroom 0.8\n",
      "query: snowboarder civilizations ski humankind diningroom 0.81\n",
      "query: snowboarder vicegerent ski fallenness diningroom 0.81\n",
      "query: snowboarder finiteness ski Human_beings diningroom 0.82\n",
      "query: snowboarder Rom._#:# ski lesser_beings diningroom 0.81\n",
      "query: freeride mankind Ski humans diningroom 0.8\n",
      "query: freestyle_skiing mankind Ski brute_beasts diningroom 0.8\n",
      "query: mountainboarding mankind Ski fallible_humans diningroom 0.81\n",
      "query: freeride_skiing mankind Ski civilized_societies diningroom 0.79\n",
      "query: skateboarding_snowboarding mankind Ski inherent_goodness diningroom 0.81\n",
      "query: telemarkers mankind Ski Human_beings diningroom 0.84\n",
      "query: ski_mountaineering mankind Ski humans diningroom 0.82\n",
      "query: ski_mountaineering mankind Ski Untermenschen diningroom 0.81\n",
      "query: Dew_Tour mankind Ski mankinds diningroom 0.82\n",
      "query: Superpark mankind Ski humans diningroom 0.81\n",
      "query: ski mankind Ski humans diningroom 0.82\n",
      "query: freestyle_skiing mankind Ski immortal_souls diningroom 0.81\n",
      "query: freeskiers mankind Ski rational_beings diningroom 0.81\n",
      "query: snowboard mankind Ski border_Millona diningroom 0.81\n",
      "query: Snowboard mankind Ski imperfect_beings diningroom 0.82\n",
      "query: Shane_McConkey mankind Ski speciesism diningroom 0.8\n",
      "query: telemark_skiing mankind Ski nonliving diningroom 0.83\n",
      "query: skiing_snowboarding mankind Ski fleshly_desires diningroom 0.82\n",
      "query: boardercross mankind Ski monotheists diningroom 0.79\n",
      "query: mountain_biking mankind Ski fallenness diningroom 0.81\n",
      "query: skateboarding mankind Ski human_beings Palladian_windows 0.81\n",
      "query: Snowboard mankind Ski human_beings beveled_mirrors 0.82\n",
      "query: alpine_skiing mankind Ski human_beings tiled_shower 0.81\n",
      "query: freeskiing mankind Ski human_beings antique_mahogany 0.8\n",
      "query: snowboards mankind Ski human_beings oak_staircase 0.81\n",
      "query: skateboarding mankind Ski human_beings polished_floorboards 0.8\n",
      "query: mountain_biking mankind Ski human_beings inglenook 0.8\n",
      "query: superpipe mankind Ski human_beings arched_ceilings 0.8\n",
      "query: freestyle_skiing mankind Ski human_beings fieldstone_fireplace 0.81\n",
      "query: freeride_skiing mankind Ski human_beings fieldstone_fireplace 0.8\n",
      "query: Freeskiing mankind Ski human_beings butler_pantry 0.81\n",
      "query: skateboarding mankind Ski human_beings wraparound_balcony 0.81\n",
      "query: Freeskiing mankind Ski human_beings Palladian_windows 0.81\n",
      "query: snowboards mankind Ski human_beings Vict_oak 0.81\n",
      "query: snowboards mankind Ski human_beings tiled_shower 0.83\n",
      "query: boardercross mankind Ski human_beings ceramic_tile_flooring 0.8\n",
      "query: telemark_skiing mankind Ski human_beings wraparound_balcony 0.82\n",
      "query: Shane_McConkey mankind Ski human_beings beadboard_wainscoting 0.8\n",
      "query: freeriding mankind Ski human_beings ensuite_bath 0.79\n",
      "query: Telemark_skiing mankind Ski human_beings ensuite_bath 0.82\n",
      "query: snowboarder mankind Buckman_Ski human_beings diningroom 0.83\n",
      "query: skier mankind Nordic_Ski human_beings diningroom 0.83\n",
      "query: Nordic_skiing mankind Skiing human_beings diningroom 0.82\n",
      "query: alpine_skiing mankind Skier human_beings diningroom 0.83\n",
      "query: Dew_Tour mankind Mountain_Bike human_beings diningroom 0.82\n",
      "query: freestyle_skiing mankind ski human_beings diningroom 0.83\n",
      "query: Freeskiing mankind Snowboard human_beings diningroom 0.82\n",
      "query: skateboarding mankind SnowSports human_beings diningroom 0.83\n",
      "query: skier mankind XC_Ski human_beings diningroom 0.83\n",
      "query: downhill_skiing mankind Ski_Instructors human_beings diningroom 0.82\n",
      "query: Dew_Tour mankind Snowboard_Expo human_beings diningroom 0.83\n",
      "query: freeride mankind Adaptive_Ski human_beings diningroom 0.8\n",
      "query: skateboarding mankind Bogus_Basin_Ski human_beings diningroom 0.82\n",
      "query: skiing_snowboarding mankind Freestyle_Ski human_beings diningroom 0.82\n",
      "query: Skiing mankind Snowsports human_beings diningroom 0.82\n",
      "query: telemark_skier mankind Ski_Team human_beings diningroom 0.83\n",
      "query: snowboard mankind Alpine_Ski human_beings diningroom 0.83\n",
      "query: Downhill_skiing mankind Nordic_ski human_beings diningroom 0.82\n",
      "query: mountain_biking mankind Dartmouth_Skiway human_beings diningroom 0.81\n",
      "query: snowboarded mankind Nordic_Ski human_beings diningroom 0.82\n",
      "query: snowboarding human Ski human_beings travertine_tile 0.82\n",
      "query: snowboarding perfectibility Ski human_beings oak_staircase 0.79\n",
      "query: snowboarding planet Ski human_beings antique_mahogany 0.81\n",
      "query: snowboarding earthly_sojourn Ski human_beings wrought_iron_chandelier 0.8\n",
      "query: snowboarding mankinds Ski human_beings inglenook 0.82\n",
      "query: snowboarding peoples Ski human_beings tiled_shower 0.83\n",
      "query: snowboarding finiteness Ski human_beings wrought_iron_chandelier 0.8\n",
      "query: snowboarding Allah_SWT Ski human_beings barrel_vaulted_ceiling 0.79\n",
      "query: snowboarding humans Ski human_beings demilune 0.81\n",
      "query: snowboarding technological_singularity Ski human_beings coffered_ceiling 0.8\n",
      "query: snowboarding Allah_Subhanahu_wa_ta'ala Ski human_beings galleried_landing 0.78\n",
      "query: snowboarding planetwide Ski human_beings Vict_oak 0.8\n",
      "query: snowboarding Triune_God Ski human_beings Vict_oak 0.79\n",
      "query: snowboarding Muslim_ummah Ski human_beings soaring_ceilings 0.79\n",
      "query: snowboarding Divine_Creator Ski human_beings antique_mahogany 0.8\n",
      "query: snowboarding God_Almighty Ski human_beings Palladian_window 0.8\n",
      "query: snowboarding vicegerent Ski human_beings beveled_mirrors 0.82\n",
      "query: snowboarding Genesis_#:#-# Ski human_beings wraparound_balcony 0.8\n",
      "query: snowboarding Allah_subhanahu_wa_ta'ala Ski human_beings coffered_ceiling 0.78\n",
      "query: snowboarding peoples Ski human_beings antique_mahogany 0.81\n",
      "query: downhill_skiing sentient_beings Ski human_beings diningroom 0.8\n",
      "query: skier mutantkind Ski human_beings diningroom 0.81\n",
      "query: freeskiers sentient_beings Ski human_beings diningroom 0.8\n",
      "query: Skiing Gd_Himself Ski human_beings diningroom 0.8\n",
      "query: freeride humankind Ski human_beings diningroom 0.8\n",
      "query: mountain_biking sentient_intelligent Ski human_beings diningroom 0.8\n",
      "query: Skiing God_Himself Ski human_beings diningroom 0.8\n",
      "query: freeskier eschaton Ski human_beings diningroom 0.81\n",
      "query: skiing_snowboarding humans Ski human_beings diningroom 0.82\n",
      "query: freeride_skiing peoples Ski human_beings diningroom 0.81\n",
      "query: skier Divine_Creator Ski human_beings diningroom 0.81\n",
      "query: slopestyle beings Ski human_beings diningroom 0.8\n",
      "query: alpine_skiing Allah_SWT Ski human_beings diningroom 0.8\n",
      "query: telemark_skiers Matt._##:## Ski human_beings diningroom 0.81\n",
      "query: backcountry_skiing planet Ski human_beings diningroom 0.82\n",
      "query: telemarkers God_Himself Ski human_beings diningroom 0.81\n",
      "query: skiers Matt._##:## Ski human_beings diningroom 0.81\n",
      "query: freeskier Allah_subhanahu_wa_ta'ala Ski human_beings diningroom 0.78\n",
      "query: snowboard Rom._#:# Ski human_beings diningroom 0.81\n",
      "query: snowboarders blessedness Ski human_beings diningroom 0.81\n",
      "query: snowboarding mankind Ski humanity fanlight 0.82\n",
      "query: snowboarding mankind Ski civilized_society galleried_landing 0.8\n",
      "query: snowboarding mankind Ski humans tiled_foyer 0.82\n",
      "query: snowboarding mankind Ski speciesism flagstone_patio 0.8\n",
      "query: snowboarding mankind Ski inherent_goodness coved_ceiling 0.81\n",
      "query: snowboarding mankind Ski sentient_creature tiled_shower 0.82\n",
      "query: snowboarding mankind Ski brute_beasts coffered_ceiling 0.8\n",
      "query: snowboarding mankind Ski nonhuman_animals flagstone_patio 0.81\n",
      "query: snowboarding mankind Ski border_Millona banquette_seating 0.81\n",
      "query: snowboarding mankind Ski nonhuman_animals limestone_fireplace 0.81\n",
      "query: snowboarding mankind Ski intelligent_beings Palladian_window 0.82\n",
      "query: snowboarding mankind Ski intelligent_beings beveled_mirrors 0.81\n",
      "query: snowboarding mankind Ski mortals ensuite_bath 0.81\n",
      "query: snowboarding mankind Ski speciesism tiled_foyer 0.81\n",
      "query: snowboarding mankind Ski mankinds coffered_ceiling 0.82\n",
      "query: snowboarding mankind Ski nonliving china_cupboard 0.82\n",
      "query: snowboarding mankind Ski peoples beamed_ceiling 0.82\n",
      "query: snowboarding mankind Ski nonhumans soaker_tub 0.81\n",
      "query: snowboarding mankind Ski mankinds breakfasting_kitchen 0.83\n",
      "query: snowboarding beings snowsport human_beings diningroom 0.82\n",
      "query: snowboarding Allah_Subhanahu_wa_ta'ala Steamboat_Ski_Resort human_beings diningroom 0.79\n",
      "query: snowboarding Matt._##:## Snowboard_Expo human_beings diningroom 0.81\n",
      "query: snowboarding gnosis Heavenly_Ski human_beings diningroom 0.81\n",
      "query: snowboarding eschaton Snowsport human_beings diningroom 0.82\n",
      "query: snowboarding Allah_Subhanahu_Tahla Ski_Snowboard human_beings diningroom 0.8\n",
      "query: snowboarding sentient_intelligent telemark human_beings diningroom 0.83\n",
      "query: snowboarding technological_singularity Dartmouth_Skiway human_beings diningroom 0.82\n",
      "query: snowboarding peoples Snowsport human_beings diningroom 0.83\n",
      "query: snowboarding God_Himself Ski_Area human_beings diningroom 0.81\n",
      "query: snowboarding technological_singularity snowsports human_beings diningroom 0.83\n",
      "query: snowboarding monotheists skiers human_beings diningroom 0.82\n",
      "query: snowboarding Rom._#:# Freeskier human_beings diningroom 0.82\n",
      "query: snowboarding beings snowboard human_beings diningroom 0.83\n",
      "query: snowboarding Muslim_ummah SnowSports human_beings diningroom 0.8\n",
      "query: snowboarding blessedness Dartmouth_Skiway human_beings diningroom 0.8\n",
      "query: snowboarding planet skis human_beings diningroom 0.83\n",
      "query: snowboarding mankind Kip_Pitou human_beings ceramic_tiled 0.82\n",
      "query: snowboarding mankind Snowboard_Expo human_beings limestone_fireplace 0.82\n",
      "query: snowboarding mankind Snowsports human_beings screened_lanai 0.82\n",
      "query: snowboarding mankind Kip_Pitou human_beings beamed_ceiling 0.82\n",
      "query: snowboarding mankind Skiiers human_beings wrought_iron_chandelier 0.82\n",
      "query: snowboarding mankind skiing human_beings barrel_vaulted_ceiling 0.82\n",
      "query: snowboarding mankind Heavenly_Ski human_beings tiled_foyer 0.82\n",
      "query: snowboarding mankind snowsport human_beings tiled_shower 0.83\n",
      "query: snowboarding mankind snowboard human_beings breakfasting_kitchen 0.83\n",
      "query: snowboarding mankind Downhill_skiing human_beings travertine_tile 0.82\n",
      "query: snowboarding mankind Nordic_Ski human_beings demilune 0.81\n",
      "query: snowboarding mankind Snowsports human_beings inglenook 0.81\n",
      "query: snowboarding mankind Buckman_Ski human_beings wraparound_balcony 0.82\n",
      "query: snowboarding mankind Buckman_Ski human_beings Palladian_windows 0.82\n",
      "query: snowboarding mankind SnowSports human_beings flagstone_patio 0.82\n",
      "query: snowboarding mankind Skiing human_beings beadboard_wainscoting 0.82\n",
      "query: snowboarding mankind Adaptive_Ski human_beings inglenook_fireplace 0.81\n",
      "query: snowboarding mankind Snowsport human_beings demilune 0.81\n",
      "query: snowboarding mankind Freestyle_Ski human_beings butler_pantry 0.82\n",
      "query: snowboarding mankind ski.com human_beings beveled_mirrors 0.83\n",
      "query: snowboarding God Ski mankinds diningroom 0.82\n",
      "query: snowboarding dwell_therein Ski fallenness diningroom 0.8\n",
      "query: snowboarding peoples Ski Human_beings diningroom 0.83\n",
      "query: snowboarding endureth Ski fallible_human_beings diningroom 0.81\n",
      "query: snowboarding Genesis_#:#-# Ski creatures diningroom 0.78\n",
      "query: snowboarding Allah_subhanahu_wa_ta'ala Ski immortal_souls diningroom 0.77\n",
      "query: snowboarding earthly Ski civilized_societies diningroom 0.8\n",
      "query: snowboarding Matt._##:## Ski fallible_humans diningroom 0.81\n",
      "query: snowboarding God_Almighty Ski human diningroom 0.81\n",
      "query: snowboarding civilizations Ski mankinds diningroom 0.81\n",
      "query: snowboarding Allah_SWT Ski creatures diningroom 0.8\n",
      "query: snowboarding monotheists Ski lesser_beings diningroom 0.79\n",
      "query: snowboarding finitude Ski anthropomorphise diningroom 0.79\n",
      "query: snowboarding technological_singularity Ski anthropomorphise diningroom 0.8\n",
      "query: snowboarding Triune_God Ski mortal_beings diningroom 0.8\n",
      "query: snowboarding humankind Ski homo_sapiens diningroom 0.81\n",
      "query: snowboarding civilizations Ski lesser_beings diningroom 0.79\n",
      "query: snowboarding civilizations Ski speciesism diningroom 0.78\n",
      "query: snowboarding mankinds Ski creatures diningroom 0.83\n",
      "query: snowboarding mankind skiing God_dess diningroom 0.82\n",
      "query: snowboarding mankind Freeski creatures diningroom 0.81\n",
      "query: snowboarding mankind Freestyle_Ski fellowmen diningroom 0.82\n",
      "query: snowboarding mankind Buckman_Ski fallible_human_beings diningroom 0.82\n",
      "query: snowboarding mankind Snowboarding sentient diningroom 0.82\n",
      "query: snowboarding mankind Snowboarding civilized_societies diningroom 0.81\n",
      "query: snowboarding mankind Heavenly_Ski Untermenschen diningroom 0.81\n",
      "query: snowboarding mankind Ski_Mountaineering intelligent_beings diningroom 0.82\n",
      "query: snowboarding mankind ski.com beings diningroom 0.82\n",
      "query: snowboarding mankind SnowSports inferior_beings diningroom 0.82\n",
      "query: snowboarding mankind skiing civilized_societies diningroom 0.82\n",
      "query: snowboarding mankind Freestyle_Ski speciesism diningroom 0.81\n",
      "query: snowboarding mankind Mountain_Bike fleshly_desires diningroom 0.82\n",
      "query: snowboarding mankind ski.com humankind diningroom 0.82\n",
      "query: snowboarding mankind snowsports inherently_sinful diningroom 0.8\n",
      "query: snowboarding mankind Freeskier intelligent_beings diningroom 0.82\n",
      "query: snowboarding mankind Freeskier humans diningroom 0.83\n",
      "query: snowboarding mankind Nordic_ski fallible_humans diningroom 0.82\n",
      "No improvement found in step 2. Ending optimization.\n",
      "Optimized combination: '['snowboarding', 'mankind', 'Ski', 'human_beings', 'diningroom']'\n",
      "The combination with the maximum score is 'snowboarder mankind ski human_beings diningroom' with a score of 0.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_two_words_sampling(word2vec_model, start_words, n_steps=5, n_words=20, temperature=0.5, combinations_to_sample=10):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "    visited_combinations = set()\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        improvement_found = False\n",
    "        index_pairs = [(i, j) for i in range(len(current_words)) for j in range(i+1, len(current_words))]\n",
    "        random.shuffle(index_pairs)\n",
    "\n",
    "        for i, j in index_pairs:\n",
    "            word1, word2 = current_words[i], current_words[j]\n",
    "            similar_words1 = [word for word, _ in word2vec_model.most_similar(word1, topn=n_words)]\n",
    "            similar_words2 = [word for word, _ in word2vec_model.most_similar(word2, topn=n_words)]\n",
    "\n",
    "            # Generate all possible combinations from the two lists of similar words\n",
    "            all_combinations = list(itertools.product(similar_words1, similar_words2))\n",
    "            random.shuffle(all_combinations)  # Shuffle to ensure randomness\n",
    "\n",
    "            # Sample a subset of combinations to try\n",
    "            sampled_combinations = all_combinations[:combinations_to_sample] if combinations_to_sample < len(all_combinations) else all_combinations\n",
    "\n",
    "            for sim_word1, sim_word2 in sampled_combinations:\n",
    "                if sim_word1 == sim_word2 or sim_word1 in current_words or sim_word2 in current_words:\n",
    "                    continue\n",
    "\n",
    "                new_combination = current_words[:]\n",
    "                new_combination[i], new_combination[j] = sim_word1, sim_word2\n",
    "                new_combination_str = ' '.join(new_combination)\n",
    "\n",
    "                if new_combination_str not in visited_combinations:\n",
    "                    visited_combinations.add(new_combination_str)\n",
    "                    score = manager.query(new_combination)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_combination = new_combination\n",
    "                        improvement_found = True\n",
    "                        print(f\"Step {step+1}: New best combination is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "        if not improvement_found:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "        else:\n",
    "            current_words = best_combination\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "# Assuming the existence of the required objects and methods\n",
    "sw = ['snowboarder', 'mankind', 'ski', 'human_beings', 'diningroom']\n",
    "\n",
    "optimized_combination = optimize_two_words_sampling(\n",
    "    word2vec_model,\n",
    "    start_words=sw,\n",
    "    n_steps=10, n_words=50, combinations_to_sample=20\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "neOf6ladnXo1",
    "outputId": "9d402083-4750-4302-d60f-124d27e96794"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_all_words_sampling(word2vec_model, start_words, n_steps=5, n_words=20, temperature=0.5):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "    visited_combinations = set()\n",
    "\n",
    "    improvement_found = 0\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        similar_words_lists = [word2vec_model.most_similar(word, topn=n_words) for word in current_words]\n",
    "\n",
    "        all_combinations = list(itertools.product(*similar_words_lists))\n",
    "        random.shuffle(all_combinations)\n",
    "\n",
    "        for new_combination in all_combinations:\n",
    "            new_combination = [word for word, _ in new_combination]  # Only take the word, not its similarity score\n",
    "            if any(word in current_words for word in new_combination):  # Ensure no words from the current list are used\n",
    "                continue\n",
    "\n",
    "            new_combination_str = ' '.join(new_combination)\n",
    "            if new_combination_str not in visited_combinations:\n",
    "                visited_combinations.add(new_combination_str)\n",
    "                score = manager.query(new_combination)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_combination = new_combination[:]\n",
    "                    improvement_found += 1\n",
    "                    print(f\"Step {step+1}: New best combination is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "        if improvement_found > 4:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "        else:\n",
    "            current_words = best_combination\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "\n",
    "sw = ['snowboarder', 'mankind', 'ski', 'human_beings', 'diningroom']\n",
    "# Assuming the existence of the required objects and methods\n",
    "optimized_combination = optimize_all_words_sampling(\n",
    "    word2vec_model, start_words=sw,\n",
    "    n_steps=10, n_words=20\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYHvCgLonYDh",
    "outputId": "b04743e0-5391-45bf-ab77-a81f342007ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'snowboarder mankind ski human_beings diningroom' with a score of 0.85\n"
     ]
    }
   ],
   "source": [
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghwVasl6nYHp"
   },
   "outputs": [],
   "source": [
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rsQPQXVXnYOm",
    "outputId": "0a72d5f9-a99f-41a2-b2e7-9f46b4423af3"
   },
   "outputs": [],
   "source": [
    "sw =  [\"sandwich\", \"banana\", \"snowing\", \"awareness\", \"housepets\"]\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw,\n",
    "    n_steps=20, n_words=50\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-dtu0w6uxOS",
    "outputId": "4ba061a7-fec0-4400-fc0d-48c50bfe25cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'snowboarder mankind ski human_beings diningroom' with a score of 0.85\n"
     ]
    }
   ],
   "source": [
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4MHR5hxuxny",
    "outputId": "a153eb3b-c5a8-455d-c85f-27feafeafd15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: snow snow snow snow snow 0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"snow\", \"snow\", \"snow\", \"snow\", \"snow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxyYjrkcwoyW",
    "outputId": "5b6449fd-933f-4561-e6c0-fd86fc05e7f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: ski ski snow ski snow 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"ski\", \"ski\", \"snow\", \"ski\", \"snow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoIXtvAEwtQN",
    "outputId": "62dcdaec-f3c9-4386-c500-19324fed8026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: ski ski ski ski ski 0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"ski\", \"ski\", \"ski\", \"ski\", \"ski\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4IMNI1Awscb",
    "outputId": "09282120-def4-41d4-ee2a-89dbcbb2b4eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: human human human human ski 0.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"human\", \"human\", \"human\", \"human\", \"ski\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwzzdLIEw1hN",
    "outputId": "971d88b0-74de-4e79-e6be-e4c7b05b3fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: human human human human human 0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"human\", \"human\", \"human\", \"human\", \"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uy5fcSAxw4tl",
    "outputId": "2c200c67-1125-4596-ab8e-b26ef26e316b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"human\", \"person\", \"snowman\", \"woman\", \"snow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzTvm6T-xklD",
    "outputId": "1dc9ed8c-f983-4f88-f882-49c104f53c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: snowing falling snowman woman snowaaabadasdfas 0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"snowing\", \"falling\", \"snowman\", \"woman\", \"snowaaabadasdfas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CG4wmOJVxk8R",
    "outputId": "b62b79b5-328f-42ba-a974-02aa26c348af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: snowing skldahfaipuwohf snowman woman snowaaabadasdfas 0.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"snowing\", \"skldahfaipuwohf\", \"snowman\", \"woman\", \"snowaaabadasdfas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBp4cUHix2jh",
    "outputId": "791b3265-d20f-4427-e38b-dbbfcaff1a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: asdkl;fjaslkj skldahfaipuwohf aeawf a snowaaabadasdfas 0.77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"asdkl;fjaslkj\", \"skldahfaipuwohf\", \"aeawf\", \"a\", \"snowaaabadasdfas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lc1yzTlZx2Xx",
    "outputId": "8036fa3d-68c5-43f8-b92f-f91086456518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: ab cd ef ghijkas 0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query([\"ab\", \"cd\", \"ef\", \"gh\" \"ijkas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zq0BQzAeyBTo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fe-IkJLyAea",
    "outputId": "1cf0c506-d11b-4af5-ef48-1788478ebfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'snowboarder mankind ski human_beings diningroom' with a score of 0.85\n"
     ]
    }
   ],
   "source": [
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0myKpecpx15w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Iu8q2sNvw4ib",
    "outputId": "a40297d8-60d1-4b95-bef7-2ab05a6c594d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_word_closest(word2vec_model, start_words, n_steps=5, n_words=20, n_words_random=10, temperature=0.5):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "\n",
    "    all_words = english_words\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        for i, word in enumerate(current_words):\n",
    "            # Get n_words most similar words to the current one\n",
    "            all_similar_words = word2vec_model.most_similar(word, topn=n_words * 2)\n",
    "            similar_words = [w for w, _ in all_similar_words if w not in current_words]\n",
    "\n",
    "            # Get n_words_random random words from the corpus\n",
    "            random_words = random.sample(all_words, n_words_random)\n",
    "            random_words = [w for w in random_words if w not in current_words]\n",
    "\n",
    "            # Combine similar and random words\n",
    "            candidate_words = similar_words + random_words\n",
    "\n",
    "            # Gather scores for candidate words to enable intelligent sampling\n",
    "            word_scores = []\n",
    "            for candidate_word in candidate_words:\n",
    "                combined_words = current_words[:]\n",
    "                combined_words[i] = candidate_word\n",
    "                combined_words_str = ' '.join(combined_words)\n",
    "                if combined_words_str in manager.queried_words:\n",
    "                    word_scores.append(manager.queried_words[combined_words_str])\n",
    "                else:\n",
    "                    score = manager.query(combined_words)\n",
    "                    word_scores.append(score)\n",
    "\n",
    "            # If no new scores are found, skip to the next word\n",
    "            if not word_scores:\n",
    "                continue\n",
    "\n",
    "            # Select a word using intelligent sampling\n",
    "            selected_word = intelligent_sampling(candidate_words, word_scores, temperature=temperature)\n",
    "            new_combination = current_words[:]\n",
    "            new_combination[i] = selected_word\n",
    "            score = manager.query(new_combination)\n",
    "\n",
    "            # Update the best score and combination found so far\n",
    "            if score >= best_score:\n",
    "                best_score = score\n",
    "                best_combination = new_combination[:]\n",
    "                print(f\"Step {step+1}, Word {i+1}: New combination is '{new_combination}' with score {score}\")\n",
    "\n",
    "        current_words = best_combination\n",
    "        if not best_combination:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "# Assuming the existence of the required objects and methods:\n",
    "# - word2vec_model: A word2vec model object with a most_similar method and a vocab attribute\n",
    "# - manager: An instance of QueryManager with an appropriate query method\n",
    "\n",
    "sw = ['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'gale']\n",
    "sw = [\"human\", \"person\", \"snowman\", \"woman\", \"snow\"]\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw, n_steps=10, n_words=50, n_words_random=50\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggy5H-Gnw4W1",
    "outputId": "ec546691-50a3-4681-80c2-27662dc7b957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMzGjw7C1Hcr",
    "outputId": "7ed6ce9f-8700-470a-ad54-b417e93eb823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: snowboarder mankind telemarkers human_beings diningroom 0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = 'snowboarder mankind telemarkers human_beings diningroom'.split()\n",
    "manager.query(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6BNhQww0u9S",
    "outputId": "d3313977-3269-4c16-f545-647a8a6c163b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'snowboarder mankind telemarkers human_beings diningroom' with a score of 0.86\n"
     ]
    }
   ],
   "source": [
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pl7GBrs0_Ak"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Your dictionary\n",
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5RUwwQr0u0W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtgYQ3DU0ucf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMmWHb1Fw39S"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "freq_dist = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "common_words = [word for word, count in freq_dist.most_common(20_000)]  # get the top 5000 common words\n",
    "\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLZIBfZy03er"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_word_closest(word2vec_model, start_words, n_steps=5, n_words=20, n_words_random=10, temperature=0.5):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "\n",
    "    all_words = common_words\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        for i, word in enumerate(current_words):\n",
    "            # Get n_words most similar words to the current one\n",
    "            all_similar_words = word2vec_model.most_similar(word, topn=n_words * 2)\n",
    "            similar_words = [w for w, _ in all_similar_words if w not in current_words]\n",
    "\n",
    "            # Get n_words_random random words from the corpus\n",
    "            random_words = random.sample(all_words, n_words_random)\n",
    "            random_words = [w for w in random_words if w not in current_words]\n",
    "\n",
    "            # Combine similar and random words\n",
    "            candidate_words = similar_words + random_words\n",
    "\n",
    "            # Gather scores for candidate words to enable intelligent sampling\n",
    "            word_scores = []\n",
    "            for candidate_word in candidate_words:\n",
    "                combined_words = current_words[:]\n",
    "                combined_words[i] = candidate_word\n",
    "                combined_words_str = ' '.join(combined_words)\n",
    "                if combined_words_str in manager.queried_words:\n",
    "                    word_scores.append(manager.queried_words[combined_words_str])\n",
    "                else:\n",
    "                    try:\n",
    "                      score = manager.query(combined_words)\n",
    "                    except JSONDecodeError as e:\n",
    "                      print(\"An error occurred while trying to decode the JSON response.\")\n",
    "                      print(combined_words)\n",
    "                      raise e\n",
    "                    word_scores.append(score)\n",
    "\n",
    "            # If no new scores are found, skip to the next word\n",
    "            if not word_scores:\n",
    "                continue\n",
    "\n",
    "            # Select a word using intelligent sampling\n",
    "            selected_word = intelligent_sampling(candidate_words, word_scores, temperature=temperature)\n",
    "            new_combination = current_words[:]\n",
    "            new_combination[i] = selected_word\n",
    "            score = manager.query(new_combination)\n",
    "\n",
    "            # Update the best score and combination found so far\n",
    "            if score >= best_score:\n",
    "                best_score = score\n",
    "                best_combination = new_combination[:]\n",
    "                print(f\"Step {step+1}, Word {i+1}: New combination is '{new_combination}' with score {score}\")\n",
    "\n",
    "        current_words = best_combination\n",
    "        if not best_combination:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "# Assuming the existence of the required objects and methods:\n",
    "# - word2vec_model: A word2vec model object with a most_similar method and a vocab attribute\n",
    "# - manager: An instance of QueryManager with an appropriate query method\n",
    "\n",
    "sw = ['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'gale']\n",
    "sw = [\"human\", \"person\", \"snowman\", \"woman\", \"snow\"]\n",
    "sw = 'snowboarder mankind telemarkers human_beings diningroom'.split()\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw, n_steps=10, n_words=10, n_words_random=20\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-i2WXCX03SV",
    "outputId": "e43e7ec0-cca2-4770-93d4-006f99a597f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'snowboarder mankind telemarkers human_beings diningroom' with a score of 0.86\n"
     ]
    }
   ],
   "source": [
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qekiyHhE03GY",
    "outputId": "c4edef6c-d244-40ee-b910-9b353a7f3d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination 'snowboarder mankind telemarkers human_beings diningroom' has a score of 0.86\n",
      "The combination 'avid_snowboarder mankind telemarkers human_beings diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers Human_beings diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers humans diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers humanity diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers human diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers Humans diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers humanness diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers humanbeing diningroom' has a score of 0.86\n",
      "The combination 'snowboarder mankind telemarkers human_beings stainless_appliances' has a score of 0.86\n",
      "The combination 'cameras mankind telemarkers humanity Palladian_window' has a score of 0.86\n"
     ]
    }
   ],
   "source": [
    "def get_all_max_score_words(manager):\n",
    "    # Get all the scores from the manager's queried words\n",
    "    scores = manager.queried_words.values()\n",
    "\n",
    "    # Find the maximum score\n",
    "    max_score = max(scores)\n",
    "\n",
    "    # Find all keys (word combinations) that have the maximum score\n",
    "    max_combinations = [key for key, value in manager.queried_words.items() if value == max_score]\n",
    "\n",
    "    return max_combinations, max_score\n",
    "\n",
    "\n",
    "# Usage:\n",
    "max_combinations, max_score = get_all_max_score_words(manager)\n",
    "for combination in max_combinations:\n",
    "    print(f\"The combination '{combination}' has a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smWQAlPb0207"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ww21Zs4Z6mUf",
    "outputId": "c323984b-6d6b-41e7-ec12-58f8dd0a5aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: snowboarder mankind telemarkers sun-tan tile_backsplashes 0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query(\"snowboarder mankind telemarkers sun-tan tile_backsplashes\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2KGCnhL7OwT"
   },
   "outputs": [],
   "source": [
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zwx6nNm59Q6j",
    "outputId": "ce93f369-df0b-42c9-9df2-efb05775350e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'snowboarder mankind telemarkers human_beings diningroom' with a score of 0.86\n"
     ]
    }
   ],
   "source": [
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6pJrYvAa02fA",
    "outputId": "68ba4d64-54d3-4485-e01c-6f1b41b77d26"
   },
   "outputs": [],
   "source": [
    "sw = 'snowboarder mankind telemarkers human diningroom'.split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw, n_steps=50, n_words=100, n_words_random=100\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NY_AKoZw0u7",
    "outputId": "4cc222af-fdde-4689-bc8b-92e4bb447e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: snowboarder mankind telemarkers human glazed_atrium 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query(['snowboarder', 'mankind', 'telemarkers', 'human', 'glazed_atrium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLAZatcLuxsh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnq4soe_-yJN",
    "outputId": "45957b3f-c8c6-43d3-efd8-8be12101073a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49815\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "freq_dist = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "common_words = [word for word, count in freq_dist.most_common(50_000)]  # get the top 5000 common words\n",
    "\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjeXQ_yiuxxl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_all_words_sampling(word2vec_model, start_words, n_steps=5, n_words=20, n_words_random=20, temperature=0.5):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "    visited_combinations = set()\n",
    "\n",
    "    all_words = common_words\n",
    "\n",
    "    improvement_found = 0\n",
    "    for step in range(n_steps):\n",
    "\n",
    "\n",
    "        similar_words_lists = []\n",
    "        for word in current_words:\n",
    "            list_1 = word2vec_model.most_similar(word, topn=n_words)\n",
    "            list_2 = random_words = random.sample(all_words, n_words_random)\n",
    "            similar_words_lists.append(list_1 + list_2)\n",
    "\n",
    "        all_combinations = list(itertools.product(*similar_words_lists))\n",
    "        random.shuffle(all_combinations)\n",
    "\n",
    "        for new_combination in all_combinations:\n",
    "            new_combination = [word for word, _ in new_combination]  # Only take the word, not its similarity score\n",
    "            if any(word in current_words for word in new_combination):  # Ensure no words from the current list are used\n",
    "                continue\n",
    "\n",
    "            new_combination_str = ' '.join(new_combination)\n",
    "            if new_combination_str not in visited_combinations:\n",
    "                visited_combinations.add(new_combination_str)\n",
    "                try:\n",
    "                    score = manager.query(new_combination)\n",
    "                except JSONDecodeError as e:\n",
    "                    print(\"An error occurred while trying to decode the JSON response.\")\n",
    "                    print(combined_words)\n",
    "                    raise e\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_combination = new_combination[:]\n",
    "                    improvement_found += 1\n",
    "                    print(f\"Step {step+1}: New best combination is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "        if improvement_found > 10:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "        else:\n",
    "            current_words = best_combination\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "\n",
    "sw = ['snowboarder', 'mankind', 'ski', 'human_beings', 'diningroom']\n",
    "sw = 'snowboarder mankind telemarkers humanity diningroom'.split()\n",
    "\n",
    "\n",
    "# Assuming the existence of the required objects and methods\n",
    "optimized_combination = optimize_all_words_sampling(\n",
    "    word2vec_model, start_words=sw,\n",
    "    n_steps=100,\n",
    "    n_words_random=40,\n",
    "    n_words=10\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HGBA5EAux1z"
   },
   "outputs": [],
   "source": [
    "data_dict = manager.queried_words\n",
    "\n",
    "# Writing to json\n",
    "with open('data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_dict, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XE9Dsl20903y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DnmRJmACI8E",
    "outputId": "79392d6b-1f6f-4323-cb51-9e8f3583a35c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.75}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "input_data = \"cat jungle hat space water\"\n",
    "\n",
    "def query(input_data):\n",
    "    response = requests.post('http://semantle2.advml.com/score', json={'data': input_data})\n",
    "    return response.json()\n",
    "\n",
    "query(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mXzLcnB91A_"
   },
   "outputs": [],
   "source": [
    "query('snowboarder mankind telemarker humanity room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GBFGXzK91Hz",
    "outputId": "ae78db1d-f33f-42b9-af6b-0ed7627d3597"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.87}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person man telemarker humanity room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRUfOpAy91PM",
    "outputId": "e94f3907-fe41-48a1-ec0e-22db5b6bf1e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.89}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman telemarker humanity room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8xNGlhD91le",
    "outputId": "b8e0482d-7a2b-4225-b1d6-139da432c009"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.88}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman couple humanity room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjyHUdarnYT8",
    "outputId": "9dfb2595-bcae-4940-9d67-e840c359a5fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.9}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman telemarker mankind room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZ_ZVsbbCje1",
    "outputId": "f4d49be9-e9d1-42f6-b20d-adbcc28d62c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.89}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman telemarker mankind all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltx8PpZqCjko",
    "outputId": "4b3ab2b6-1e72-418c-9cf2-10ceab8fa43e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.89}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman telemarker mankind house')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vk0y0o20CjrR",
    "outputId": "83b59ecc-f690-4b88-d2fd-f82cd6b87033"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.9}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman telemarker mankind room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jwl7g5IcGIXb",
    "outputId": "cb37e226-628f-4f1a-aec3-0af26a5fc078"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.9}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman person man person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQb-3uNmGSTZ",
    "outputId": "f5c23da1-8f66-4448-8385-dd673e9e96d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.9}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person man person woman person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPxE620CGSGc",
    "outputId": "f8412f5b-3039-4e58-bace-77063f915d67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.9}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman human man people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a25vaFsWGR5T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1ggS49bGRaH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_moCMHvBDJxw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNuAupy3DJ2P",
    "outputId": "fcba8efd-6e0a-4412-cac7-b9f005900900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_6-DGjRDJ7y",
    "outputId": "f93d63dc-1adf-45fe-85c0-c8188b614527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from nltk.corpus import words\n",
    "import nltk\n",
    "\n",
    "# Downloading the word2vec word embeddings\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")  # you can choose another model if you prefer\n",
    "\n",
    "# Downloading the words corpus\n",
    "nltk.download('words')\n",
    "english_words = words.words()\n",
    "\n",
    "# Now you can use `word2vec_model` to get the embeddings and `english_words` to get the English words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CklZZb6gDuJW",
    "outputId": "a8fcb162-c060-4b29-b8dd-c507d3f578da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49815\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "freq_dist = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "common_words = [word for word, count in freq_dist.most_common(100_000)]  # get the top 5000 common words\n",
    "\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZqCSWr0DKBR",
    "outputId": "576e7783-a4d6-48c2-91e6-f0803d575fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: dog forest hat space wind 0.74\n",
      "query: dog jogging table earth wind 0.76\n",
      "query: dogs snowboarding_mountain_biking dining_room mankind wind 0.83\n",
      "query: dogs snowboarding_mountain_biking dining_room mankind gale 0.83\n",
      "query: person woman telemarker mankind room 0.9\n",
      "query: person woman telemarker mankind all 0.89\n",
      "query: man woman man camera camera 0.96\n",
      "The combination with the maximum score is 'man woman man camera camera' with a score of 0.96\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "class QueryManager:\n",
    "    def __init__(self):\n",
    "        self.queried_words = {}\n",
    "\n",
    "    def query(self, words_list):\n",
    "        words_key = ' '.join(words_list)  # Convert list to string to use as a key\n",
    "        if words_key in self.queried_words:\n",
    "            return self.queried_words[words_key]\n",
    "\n",
    "        try:\n",
    "          response = requests.post('http://semantle2.advml.com/score', json={'data': words_key})\n",
    "          response_data = response.json()\n",
    "        except JSONDecodeError as e:\n",
    "          print(\"An error occurred while trying to decode the JSON response.\")\n",
    "          print(words_list)\n",
    "          print(response)\n",
    "          print(response.text)\n",
    "          raise e\n",
    "\n",
    "        if 'flag' in response_data:\n",
    "            print(response.text)\n",
    "\n",
    "        score = response_data[\"message\"]\n",
    "        print(f\"query: {words_key} {score}\")\n",
    "        self.queried_words[words_key] = score\n",
    "        return score\n",
    "\n",
    "    def get_max_score_word(self):\n",
    "        max_key = max(self.queried_words, key=lambda k: self.queried_words[k])\n",
    "        return max_key, self.queried_words[max_key]\n",
    "\n",
    "# Example usage:\n",
    "manager = QueryManager()\n",
    "\n",
    "# Reading from json and assigning it to manager's queried_words\n",
    "# with open('data.json', 'r', encoding='utf-8') as file:\n",
    "#     manager.queried_words = json.load(file)\n",
    "\n",
    "# Now the manager's queried_words has been populated with the data from the JSON file\n",
    "\n",
    "\n",
    "manager.query([\"dog\", \"forest\", \"hat\", \"space\", \"wind\"])\n",
    "manager.query([\"dog\", \"jogging\", \"table\", \"earth\", \"wind\"])\n",
    "manager.query(['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'wind'])\n",
    "manager.query(['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'gale'])\n",
    "manager.query('person woman telemarker mankind room'.split())\n",
    "manager.query('person woman telemarker mankind all'.split())\n",
    "manager.query('man woman man camera camera'.split())\n",
    "manager.query('man woman man camera camera'.split())\n",
    "\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkTj69DlDKGM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "\n",
    "\n",
    "def softmax(scores, temperature=1.0):\n",
    "    exp_scores = np.exp(np.array(scores) / temperature)\n",
    "    return exp_scores / sum(exp_scores)\n",
    "\n",
    "def intelligent_sampling(similar_words, word_scores, temperature=1.0):\n",
    "    probabilities = softmax(word_scores, temperature)\n",
    "    word_index = np.random.choice(range(len(similar_words)), p=probabilities)\n",
    "    return similar_words[word_index]\n",
    "\n",
    "def optimize_word_closest(word2vec_model, start_words, n_steps=5, n_words=20, n_words_random=10, temperature=0.5):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = -float('inf')\n",
    "    best_combination = []\n",
    "\n",
    "    all_words = common_words\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        for i, word in enumerate(current_words):\n",
    "            # Get n_words most similar words to the current one\n",
    "            all_similar_words = word2vec_model.most_similar(word, topn=n_words * 2)\n",
    "            similar_words = [w for w, _ in all_similar_words if w not in current_words]\n",
    "\n",
    "            # Get n_words_random random words from the corpus\n",
    "            random_words = random.sample(all_words, n_words_random)\n",
    "            random_words = [w for w in random_words if w not in current_words]\n",
    "\n",
    "            # Combine similar and random words\n",
    "            candidate_words = similar_words + random_words\n",
    "\n",
    "            # Gather scores for candidate words to enable intelligent sampling\n",
    "            word_scores = []\n",
    "            for candidate_word in candidate_words:\n",
    "                combined_words = current_words[:]\n",
    "                combined_words[i] = candidate_word\n",
    "                combined_words_str = ' '.join(combined_words)\n",
    "                if combined_words_str in manager.queried_words:\n",
    "                    word_scores.append(manager.queried_words[combined_words_str])\n",
    "                else:\n",
    "                    try:\n",
    "                      score = manager.query(combined_words)\n",
    "                    except JSONDecodeError as e:\n",
    "                      print(\"An error occurred while trying to decode the JSON response.\")\n",
    "                      print(combined_words)\n",
    "                      raise e\n",
    "                    word_scores.append(score)\n",
    "\n",
    "            # If no new scores are found, skip to the next word\n",
    "            if not word_scores:\n",
    "                continue\n",
    "\n",
    "            # Select a word using intelligent sampling\n",
    "            selected_word = intelligent_sampling(candidate_words, word_scores, temperature=temperature)\n",
    "            new_combination = current_words[:]\n",
    "            new_combination[i] = selected_word\n",
    "            score = manager.query(new_combination)\n",
    "\n",
    "            # Update the best score and combination found so far\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_combination = new_combination[:]\n",
    "                print(f\"Step {step+1}, Word {i+1}: New combination is '{new_combination}' with score {score}\")\n",
    "\n",
    "        current_words = best_combination\n",
    "        if not best_combination:\n",
    "            print(f\"No improvement found in step {step+1}. Ending optimization.\")\n",
    "            break\n",
    "\n",
    "    return best_combination\n",
    "\n",
    "# Assuming the existence of the required objects and methods:\n",
    "# - word2vec_model: A word2vec model object with a most_similar method and a vocab attribute\n",
    "# - manager: An instance of QueryManager with an appropriate query method\n",
    "\n",
    "# sw = ['dogs', 'snowboarding_mountain_biking', 'dining_room', 'mankind', 'gale']\n",
    "# sw = [\"human\", \"person\", \"snowman\", \"woman\", \"snow\"]\n",
    "# sw = 'snowboarder mankind telemarkers human_beings diningroom'.split()\n",
    "sw = 'person woman telemarker mankind room'.split()\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw, n_steps=100, n_words=100, n_words_random=100\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5emx_eBDDKLB",
    "outputId": "8cf73d26-8b04-40bc-f81f-9bc8ece26286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 0.9}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('person woman human man people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCGq0n9ZHAVY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BN2-G7eHAEo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5IohifXtDKPs",
    "outputId": "b8898d0a-d5c0-4604-c383-2375f5fe7ffa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "def optimize_word_closest(word2vec_model, start_words, n_steps=5, n_words=20, n_words_random=10):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = manager.query(start_words)\n",
    "    print(f\"starting score: {best_score}\")\n",
    "    best_combination = start_words[:]\n",
    "\n",
    "    all_words = common_words\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        for i, word in enumerate(current_words):\n",
    "            # Get n_words most similar words to the current one\n",
    "            all_similar_words = word2vec_model.most_similar(word, topn=n_words * 2)\n",
    "            similar_words = [w for w, _ in all_similar_words if w not in current_words]\n",
    "\n",
    "            # Get n_words_random random words from the corpus\n",
    "            random_words = random.sample(all_words, n_words_random)\n",
    "            random_words = [w for w in random_words if w not in current_words]\n",
    "\n",
    "            # Combine similar and random words\n",
    "            candidate_words = similar_words + random_words\n",
    "\n",
    "            # Gather scores for candidate words\n",
    "            word_scores = []\n",
    "            for candidate_word in candidate_words:\n",
    "                combined_words = current_words[:]\n",
    "                combined_words[i] = candidate_word\n",
    "                # combined_words_str = ' '.join(combined_words)\n",
    "                try:\n",
    "                    score = manager.query(combined_words)\n",
    "                    word_scores.append(score)\n",
    "                except JSONDecodeError as e:\n",
    "                    print(\"An error occurred while trying to decode the JSON response.\")\n",
    "                    print(combined_words)\n",
    "                    raise e\n",
    "\n",
    "            # Find the word with the highest score among candidates\n",
    "            if word_scores:\n",
    "                max_index = word_scores.index(max(word_scores))\n",
    "                if word_scores[max_index] > best_score:\n",
    "                    print(\"!!!!!!! IMPROVED SCORE !!!!!!!!!!!!!\")\n",
    "                    best_score = word_scores[max_index]\n",
    "                    best_combination = current_words[:]\n",
    "                    best_combination[i] = candidate_words[max_index]\n",
    "                    print(f\"Step {step+1}, Word {i+1}: New combination is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "\n",
    "        if current_words == best_combination:\n",
    "          max_index = word_scores.index(max(word_scores))\n",
    "          best_score = word_scores[max_index]\n",
    "          best_combination = current_words[:]\n",
    "          best_combination[i] = candidate_words[max_index]\n",
    "          print(f\"Step {step+1}, Word {i+1}: No Better Word Found but let us update combination is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "        current_words = best_combination\n",
    "\n",
    "\n",
    "    return current_words\n",
    "\n",
    "# Assuming the existence of the required objects and methods:\n",
    "# - word2vec_model: A word2vec model object with a most_similar method and a vocab attribute\n",
    "# - manager: An instance of QueryManager with an appropriate query method\n",
    "\n",
    "sw = 'person woman telemarker mankind room'.split()\n",
    "\n",
    "# sw = 'person woman human man people'.split()\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw, n_steps=100, n_words=100, n_words_random=100\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fF2y1ZPJv0j",
    "outputId": "e1b57081-40ca-48d4-9b83-9f4197d4b024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting score: 0.96\n",
      "query: whosoever woman man camera camera 0.9\n",
      "query: student woman man camera camera 0.93\n",
      "query: enquirer woman man camera camera 0.91\n",
      "query: she woman man camera camera 0.95\n",
      "query: camper woman man camera camera 0.91\n",
      "query: shooter woman man camera camera 0.91\n",
      "query: citizens woman man camera camera 0.92\n",
      "query: complainant woman man camera camera 0.9\n",
      "query: or woman man camera camera 0.93\n",
      "query: player woman man camera camera 0.95\n",
      "query: driver woman man camera camera 0.91\n",
      "query: type woman man camera camera 0.92\n",
      "query: everybody woman man camera camera 0.92\n",
      "query: hunter woman man camera camera 0.89\n",
      "query: shopper woman man camera camera 0.91\n",
      "query: truthfully woman man camera camera 0.91\n",
      "query: caring woman man camera camera 0.92\n",
      "query: seeker woman man camera camera 0.9\n",
      "query: day woman man camera camera 0.92\n",
      "query: how woman man camera camera 0.92\n",
      "query: assailant woman man camera camera 0.9\n",
      "query: villager woman man camera camera 0.91\n",
      "query: member woman man camera camera 0.94\n",
      "query: beggar woman man camera camera 0.9\n",
      "query: survivor woman man camera camera 0.92\n",
      "query: he woman man camera camera 0.94\n",
      "query: whosever woman man camera camera 0.89\n",
      "query: wil woman man camera camera 0.91\n",
      "query: person divorcee man camera camera 0.91\n",
      "query: person nun man camera camera 0.91\n",
      "query: person waitress man camera camera 0.89\n",
      "query: person maid man camera camera 0.89\n",
      "query: person passerby man camera camera 0.9\n",
      "query: person female man camera camera 0.95\n",
      "query: person husband man camera camera 0.92\n",
      "query: person aunt man camera camera 0.89\n",
      "query: person herself man camera camera 0.92\n",
      "query: person someone man camera camera 0.94\n",
      "query: person assailant man camera camera 0.88\n",
      "query: person housekeeper man camera camera 0.9\n",
      "query: person men man camera camera 0.93\n",
      "query: person child man camera camera 0.92\n",
      "query: person newlywed man camera camera 0.9\n",
      "query: person blonde man camera camera 0.9\n",
      "query: person granddaughter man camera camera 0.9\n",
      "query: person policeman man camera camera 0.9\n",
      "query: person male man camera camera 0.93\n",
      "query: person teen man camera camera 0.9\n",
      "query: person kidnapper man camera camera 0.9\n",
      "query: person lass man camera camera 0.92\n",
      "query: person soldier man camera camera 0.92\n",
      "query: person actress man camera camera 0.93\n",
      "query: person robber man camera camera 0.87\n",
      "query: person mana man camera camera 0.91\n",
      "query: person bride man camera camera 0.91\n",
      "query: person wife man camera camera 0.93\n",
      "query: person widow man camera camera 0.91\n",
      "query: person nurse man camera camera 0.91\n",
      "query: person caller man camera camera 0.91\n",
      "query: person resident man camera camera 0.91\n",
      "query: person elderly man camera camera 0.9\n",
      "query: person suspect man camera camera 0.88\n",
      "query: person pensioner man camera camera 0.89\n",
      "query: person raped man camera camera 0.87\n",
      "query: person clergyman man camera camera 0.89\n",
      "query: person chick man camera camera 0.92\n",
      "query: person kitten man camera camera 0.89\n",
      "query: person matron man camera camera 0.9\n",
      "query: person baby man camera camera 0.9\n",
      "query: person acquaintance man camera camera 0.89\n",
      "query: person thief man camera camera 0.89\n",
      "query: person gentleman man camera camera 0.93\n",
      "query: person redhead man camera camera 0.89\n",
      "query: person exboyfriend man camera camera 0.9\n",
      "query: person hers man camera camera 0.92\n",
      "query: person ladies man camera camera 0.93\n",
      "query: person pregnant man camera camera 0.91\n",
      "query: person cabdriver man camera camera 0.87\n",
      "query: person fiance man camera camera 0.91\n",
      "query: person curdling man camera camera 0.89\n",
      "query: person woman someone camera camera 0.95\n",
      "query: person woman assailant camera camera 0.9\n",
      "query: person woman chap camera camera 0.94\n",
      "query: person woman fella camera camera 0.93\n",
      "query: person woman motorist camera camera 0.91\n",
      "query: person woman burglar camera camera 0.9\n",
      "query: person woman passerby camera camera 0.92\n",
      "query: person woman lad camera camera 0.93\n",
      "query: person woman soldier camera camera 0.92\n",
      "query: person woman youngster camera camera 0.93\n",
      "query: person woman bandit camera camera 0.91\n",
      "query: person woman bloke camera camera 0.94\n",
      "query: person woman kidnapper camera camera 0.91\n",
      "query: person woman him camera camera 0.94\n",
      "query: person woman businessman camera camera 0.94\n",
      "query: person woman schoolboy camera camera 0.91\n",
      "query: person woman grandfather camera camera 0.92\n",
      "query: person woman feller camera camera 0.93\n",
      "query: person woman lady camera camera 0.94\n",
      "query: person woman cabdriver camera camera 0.89\n",
      "query: person woman gunman camera camera 0.92\n",
      "query: person woman woman camera camera 0.95\n",
      "query: person woman acquaintance camera camera 0.92\n",
      "query: person woman clergyman camera camera 0.91\n",
      "query: person woman handyman camera camera 0.91\n",
      "query: person woman robbed camera camera 0.89\n",
      "query: person woman kid camera camera 0.93\n",
      "query: person woman cop camera camera 0.92\n",
      "query: person woman workman camera camera 0.93\n",
      "query: person woman son camera camera 0.94\n",
      "query: person woman grandmother camera camera 0.92\n",
      "query: person woman vagrant camera camera 0.91\n",
      "query: person woman old camera camera 0.93\n",
      "query: person woman thug camera camera 0.92\n",
      "query: person woman schoolgirl camera camera 0.91\n",
      "query: person woman he camera camera 0.94\n",
      "query: person woman bystander camera camera 0.92\n",
      "query: person woman father camera camera 0.94\n",
      "query: person woman accomplice camera camera 0.93\n",
      "query: person woman attacker camera camera 0.9\n",
      "query: person woman uncle camera camera 0.93\n",
      "query: person woman stepson camera camera 0.92\n",
      "query: person woman sailor camera camera 0.91\n",
      "query: person woman nephew camera camera 0.91\n",
      "query: person woman preacher camera camera 0.92\n",
      "query: person woman postman camera camera 0.91\n",
      "query: person woman caller camera camera 0.92\n",
      "query: person woman robbery camera camera 0.9\n",
      "query: person woman fireman camera camera 0.92\n",
      "query: person woman intruder camera camera 0.91\n",
      "query: person woman mother camera camera 0.94\n",
      "query: person woman sergeant camera camera 0.91\n",
      "query: person woman mana camera camera 0.93\n",
      "query: person woman servant camera camera 0.93\n",
      "query: person woman himself camera camera 0.94\n",
      "query: person woman janitor camera camera 0.9\n",
      "query: person woman priest camera camera 0.91\n",
      "query: person woman somebody camera camera 0.94\n",
      "query: person woman trooper camera camera 0.9\n",
      "query: person woman therapist camera camera 0.92\n",
      "query: person woman man projector camera 0.93\n",
      "query: person woman man microphone camera 0.95\n",
      "query: person woman man lens camera 0.94\n",
      "query: person woman man cam camera 0.94\n",
      "query: person woman man polaroid camera 0.92\n",
      "query: person woman man photographers camera 0.94\n",
      "query: person woman man cams camera 0.94\n",
      "query: person woman man microphones camera 0.93\n",
      "query: person woman man video camera 0.96\n",
      "query: person woman man pictures camera 0.94\n",
      "query: person woman man footage camera 0.95\n",
      "query: person woman man lenses camera 0.93\n",
      "query: person woman man photographic camera 0.94\n",
      "query: person woman man photography camera 0.94\n",
      "query: person woman man cameramen camera 0.95\n",
      "query: person woman man eyepiece camera 0.92\n",
      "query: person woman man jewels camera 0.92\n",
      "query: person woman man camera projector 0.93\n",
      "query: person woman man camera microphone 0.95\n",
      "query: person woman man camera lens 0.94\n",
      "query: person woman man camera cam 0.96\n",
      "query: person woman man camera polaroid 0.93\n",
      "query: person woman man camera photographers 0.93\n",
      "query: person woman man camera video 0.96\n",
      "query: person woman man camera pictures 0.94\n",
      "query: person woman man camera footage 0.93\n",
      "query: person woman man camera lenses 0.93\n",
      "query: person woman man camera photographic 0.94\n",
      "query: person woman man camera photography 0.94\n",
      "query: person woman man camera cameramen 0.95\n",
      "query: person woman man camera eyepiece 0.92\n",
      "query: person woman man camera sepia 0.92\n",
      "Step 1, Word 5: No Better Word Found but let us update combination is '['person', 'woman', 'man', 'camera', 'cameras']' with score 0.96\n",
      "query: individual woman man camera cameras 0.94\n",
      "query: employee woman man camera cameras 0.92\n",
      "query: spouse woman man camera cameras 0.92\n",
      "query: elector woman man camera cameras 0.9\n",
      "query: giver woman man camera cameras 0.91\n",
      "query: oneself woman man camera cameras 0.92\n",
      "query: caller woman man camera cameras 0.91\n",
      "query: lady woman man camera cameras 0.93\n",
      "query: whoever woman man camera cameras 0.89\n",
      "query: defendant woman man camera cameras 0.88\n",
      "query: motorist woman man camera cameras 0.9\n",
      "query: offender woman man camera cameras 0.9\n",
      "query: juror woman man camera cameras 0.89\n",
      "query: occupant woman man camera cameras 0.91\n",
      "query: patient woman man camera cameras 0.93\n",
      "query: politician woman man camera cameras 0.91\n",
      "query: claimant woman man camera cameras 0.89\n",
      "query: soldier woman man camera cameras 0.9\n",
      "query: nobody woman man camera cameras 0.92\n",
      "query: interviewee woman man camera cameras 0.92\n",
      "query: litigant woman man camera cameras 0.91\n",
      "query: borrower woman man camera cameras 0.91\n",
      "query: servant woman man camera cameras 0.91\n",
      "query: passerby woman man camera cameras 0.9\n",
      "query: wrongdoer woman man camera cameras 0.89\n",
      "query: everyone woman man camera cameras 0.91\n",
      "query: creature woman man camera cameras 0.92\n",
      "query: sufferer woman man camera cameras 0.91\n",
      "query: thing woman man camera cameras 0.94\n",
      "query: sinner woman man camera cameras 0.89\n",
      "query: kid woman man camera cameras 0.92\n",
      "query: else woman man camera cameras 0.9\n",
      "query: worker woman man camera cameras 0.93\n",
      "query: householder woman man camera cameras 0.91\n",
      "query: athlete woman man camera cameras 0.91\n",
      "query: him woman man camera cameras 0.94\n",
      "query: listener woman man camera cameras 0.91\n",
      "query: traveler woman man camera cameras 0.92\n",
      "query: one woman man camera cameras 0.93\n",
      "query: bystander woman man camera cameras 0.91\n",
      "query: user woman man camera cameras 0.94\n",
      "query: patron woman man camera cameras 0.9\n",
      "query: bloke woman man camera cameras 0.92\n",
      "query: candidate woman man camera cameras 0.91\n",
      "query: respondent woman man camera cameras 0.89\n",
      "query: guardian woman man camera cameras 0.89\n",
      "query: registrant woman man camera cameras 0.89\n",
      "query: walker woman man camera cameras 0.91\n",
      "query: entity woman man camera cameras 0.93\n",
      "query: viewer woman man camera cameras 0.93\n",
      "query: theatergoer woman man camera cameras 0.91\n",
      "query: you woman man camera cameras 0.93\n",
      "query: drinker woman man camera cameras 0.91\n",
      "query: whosoever woman man camera cameras 0.89\n",
      "query: doctor woman man camera cameras 0.91\n",
      "query: adults woman man camera cameras 0.92\n",
      "query: hearer woman man camera cameras 0.91\n",
      "query: friend woman man camera cameras 0.92\n",
      "query: westerner woman man camera cameras 0.9\n",
      "query: buyer woman man camera cameras 0.9\n",
      "query: personage woman man camera cameras 0.93\n",
      "query: inmate woman man camera cameras 0.88\n",
      "query: student woman man camera cameras 0.92\n",
      "query: enquirer woman man camera cameras 0.9\n",
      "query: she woman man camera cameras 0.94\n",
      "query: camper woman man camera cameras 0.9\n",
      "query: shooter woman man camera cameras 0.9\n",
      "query: citizens woman man camera cameras 0.9\n",
      "query: complainant woman man camera cameras 0.89\n",
      "query: or woman man camera cameras 0.93\n",
      "query: player woman man camera cameras 0.94\n",
      "query: driver woman man camera cameras 0.9\n",
      "query: type woman man camera cameras 0.91\n",
      "query: everybody woman man camera cameras 0.91\n",
      "query: hunter woman man camera cameras 0.88\n",
      "query: shopper woman man camera cameras 0.91\n",
      "query: truthfully woman man camera cameras 0.9\n",
      "query: caring woman man camera cameras 0.91\n",
      "query: seeker woman man camera cameras 0.89\n",
      "query: day woman man camera cameras 0.91\n",
      "query: how woman man camera cameras 0.9\n",
      "query: villager woman man camera cameras 0.9\n",
      "query: member woman man camera cameras 0.93\n",
      "query: beggar woman man camera cameras 0.89\n",
      "query: survivor woman man camera cameras 0.91\n",
      "query: he woman man camera cameras 0.94\n",
      "query: whosever woman man camera cameras 0.88\n",
      "query: durable woman man camera cameras 0.88\n",
      "query: person divorcee man camera cameras 0.9\n",
      "query: person nun man camera cameras 0.9\n",
      "query: person waitress man camera cameras 0.89\n",
      "query: person maid man camera cameras 0.88\n",
      "query: person passerby man camera cameras 0.89\n",
      "query: person female man camera cameras 0.94\n",
      "query: person husband man camera cameras 0.91\n",
      "query: person aunt man camera cameras 0.89\n",
      "query: person herself man camera cameras 0.92\n",
      "query: person someone man camera cameras 0.93\n",
      "query: person assailant man camera cameras 0.87\n",
      "query: person housekeeper man camera cameras 0.89\n",
      "query: person men man camera cameras 0.92\n",
      "query: person child man camera cameras 0.92\n",
      "query: person newlywed man camera cameras 0.89\n",
      "query: person blonde man camera cameras 0.89\n",
      "query: person granddaughter man camera cameras 0.89\n",
      "query: person policeman man camera cameras 0.9\n",
      "query: person male man camera cameras 0.92\n",
      "query: person teen man camera cameras 0.89\n",
      "query: person kidnapper man camera cameras 0.89\n",
      "query: person lass man camera cameras 0.91\n",
      "query: person soldier man camera cameras 0.91\n",
      "query: person actress man camera cameras 0.92\n",
      "query: person robber man camera cameras 0.86\n",
      "query: person mana man camera cameras 0.9\n",
      "query: person bride man camera cameras 0.9\n",
      "query: person wife man camera cameras 0.92\n",
      "query: person widow man camera cameras 0.9\n",
      "query: person nurse man camera cameras 0.91\n",
      "query: person caller man camera cameras 0.9\n",
      "query: person resident man camera cameras 0.89\n",
      "query: person elderly man camera cameras 0.89\n",
      "query: person suspect man camera cameras 0.87\n",
      "query: person pensioner man camera cameras 0.89\n",
      "query: person raped man camera cameras 0.86\n",
      "query: person clergyman man camera cameras 0.89\n",
      "query: person chick man camera cameras 0.92\n",
      "query: person kitten man camera cameras 0.89\n",
      "query: person matron man camera cameras 0.9\n",
      "query: person baby man camera cameras 0.89\n",
      "query: person acquaintance man camera cameras 0.88\n",
      "query: person thief man camera cameras 0.89\n",
      "query: person gentleman man camera cameras 0.92\n",
      "query: person redhead man camera cameras 0.88\n",
      "query: person exboyfriend man camera cameras 0.88\n",
      "query: person hers man camera cameras 0.91\n",
      "query: person ladies man camera cameras 0.92\n",
      "query: person pregnant man camera cameras 0.9\n",
      "query: person cabdriver man camera cameras 0.87\n",
      "query: person fiance man camera cameras 0.9\n",
      "query: person aeon man camera cameras 0.87\n",
      "query: person woman someone camera cameras 0.94\n",
      "query: person woman assailant camera cameras 0.89\n",
      "query: person woman chap camera cameras 0.93\n",
      "query: person woman fella camera cameras 0.93\n",
      "query: person woman motorist camera cameras 0.9\n",
      "query: person woman burglar camera cameras 0.89\n",
      "query: person woman passerby camera cameras 0.9\n",
      "query: person woman lad camera cameras 0.92\n",
      "query: person woman soldier camera cameras 0.91\n",
      "query: person woman youngster camera cameras 0.92\n",
      "query: person woman bandit camera cameras 0.9\n",
      "query: person woman bloke camera cameras 0.93\n",
      "query: person woman kidnapper camera cameras 0.9\n",
      "query: person woman him camera cameras 0.94\n",
      "query: person woman businessman camera cameras 0.93\n",
      "query: person woman schoolboy camera cameras 0.91\n",
      "query: person woman grandfather camera cameras 0.91\n",
      "query: person woman feller camera cameras 0.93\n",
      "query: person woman lady camera cameras 0.93\n",
      "query: person woman cabdriver camera cameras 0.88\n",
      "query: person woman gunman camera cameras 0.92\n",
      "query: person woman woman camera cameras 0.94\n",
      "query: person woman acquaintance camera cameras 0.91\n",
      "query: person woman clergyman camera cameras 0.91\n",
      "query: person woman handyman camera cameras 0.91\n",
      "query: person woman robbed camera cameras 0.89\n",
      "query: person woman kid camera cameras 0.92\n",
      "query: person woman cop camera cameras 0.91\n",
      "query: person woman workman camera cameras 0.93\n",
      "query: person woman son camera cameras 0.93\n",
      "query: person woman grandmother camera cameras 0.91\n",
      "query: person woman vagrant camera cameras 0.9\n",
      "query: person woman old camera cameras 0.92\n",
      "query: person woman thug camera cameras 0.91\n",
      "query: person woman schoolgirl camera cameras 0.9\n",
      "query: person woman he camera cameras 0.93\n",
      "query: person woman bystander camera cameras 0.91\n",
      "query: person woman father camera cameras 0.93\n",
      "query: person woman accomplice camera cameras 0.91\n",
      "query: person woman attacker camera cameras 0.89\n",
      "query: person woman uncle camera cameras 0.92\n",
      "query: person woman stepson camera cameras 0.91\n",
      "query: person woman sailor camera cameras 0.9\n",
      "query: person woman nephew camera cameras 0.9\n",
      "query: person woman preacher camera cameras 0.92\n",
      "query: person woman postman camera cameras 0.9\n",
      "query: person woman caller camera cameras 0.91\n",
      "query: person woman robbery camera cameras 0.89\n",
      "query: person woman fireman camera cameras 0.91\n",
      "query: person woman intruder camera cameras 0.9\n",
      "query: person woman mother camera cameras 0.93\n",
      "query: person woman sergeant camera cameras 0.89\n",
      "query: person woman mana camera cameras 0.92\n",
      "query: person woman servant camera cameras 0.92\n",
      "query: person woman himself camera cameras 0.93\n",
      "query: person woman janitor camera cameras 0.89\n",
      "query: person woman priest camera cameras 0.91\n",
      "query: person woman somebody camera cameras 0.93\n",
      "query: person woman trooper camera cameras 0.89\n",
      "query: person woman goitrogen camera cameras 0.9\n",
      "query: person woman man projector cameras 0.92\n",
      "query: person woman man microphone cameras 0.94\n",
      "query: person woman man lens cameras 0.93\n",
      "query: person woman man cam cameras 0.92\n",
      "query: person woman man polaroid cameras 0.91\n",
      "query: person woman man photographers cameras 0.93\n",
      "query: person woman man cams cameras 0.93\n",
      "query: person woman man microphones cameras 0.92\n",
      "query: person woman man video cameras 0.95\n",
      "query: person woman man pictures cameras 0.94\n",
      "query: person woman man footage cameras 0.94\n",
      "query: person woman man lenses cameras 0.93\n",
      "query: person woman man photographic cameras 0.93\n",
      "query: person woman man photography cameras 0.93\n",
      "query: person woman man cameramen cameras 0.94\n",
      "query: person woman man eyepiece cameras 0.92\n",
      "query: person woman man konitz cameras 0.88\n",
      "query: person woman man camera surveillance 0.93\n",
      "query: person woman man camera screens 0.95\n",
      "query: person woman man camera scanners 0.91\n",
      "query: person woman man camera detectors 0.91\n",
      "query: person woman man camera tripods 0.92\n",
      "query: person woman man camera images 0.94\n",
      "query: person woman man camera lights 0.94\n",
      "query: person woman man camera binoculars 0.93\n",
      "query: person woman man camera flashing 0.91\n",
      "query: person woman man camera sensors 0.92\n",
      "query: person woman man camera photos 0.94\n",
      "query: person woman man camera longed-for 0.92\n",
      "Step 2, Word 5: No Better Word Found but let us update combination is '['person', 'woman', 'man', 'camera', 'cameras']' with score 0.96\n",
      "query: forgetting woman man camera cameras 0.88\n",
      "query: person chicks man camera cameras 0.91\n",
      "query: person woman prevalent camera cameras 0.92\n",
      "query: person woman man cane cameras 0.92\n",
      "query: person woman man camera desmond 0.9\n",
      "Step 3, Word 5: No Better Word Found but let us update combination is '['person', 'woman', 'man', 'camera', 'cameras']' with score 0.96\n",
      "Optimized combination: '['person', 'woman', 'man', 'camera', 'cameras']'\n",
      "The combination with the maximum score is 'man woman man camera camera' with a score of 0.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "def optimize_word_closest(word2vec_model, start_words, n_steps=5, n_words=20, n_words_random=10):\n",
    "    if not isinstance(start_words, list) or len(start_words) != 5:\n",
    "        raise ValueError(\"start_words must be a list of exactly five words.\")\n",
    "\n",
    "    current_words = start_words[:]\n",
    "    best_score = manager.query(start_words)\n",
    "    print(f\"starting score: {best_score}\")\n",
    "    best_combination = start_words[:]\n",
    "\n",
    "    all_words = common_words\n",
    "    set_common_words = set(common_words)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        for i, word in enumerate(current_words):\n",
    "            # Get n_words most similar words to the current one\n",
    "            all_similar_words = word2vec_model.most_similar(word, topn=n_words * 2)\n",
    "            similar_words = [w for w, _ in all_similar_words if w not in current_words]\n",
    "            similar_words = [w.lower() for w in similar_words if w.lower() in set_common_words]\n",
    "\n",
    "            # Get n_words_random random words from the corpus\n",
    "            random_words = random.sample(all_words, n_words_random)\n",
    "            random_words = [w for w in random_words if w not in current_words]\n",
    "\n",
    "            # Combine similar and random words\n",
    "            candidate_words = similar_words + random_words\n",
    "\n",
    "            # Gather scores for candidate words\n",
    "            word_scores = []\n",
    "            for candidate_word in candidate_words:\n",
    "                combined_words = current_words[:]\n",
    "                combined_words[i] = candidate_word\n",
    "                # combined_words_str = ' '.join(combined_words)\n",
    "                try:\n",
    "                    score = manager.query(combined_words)\n",
    "                    word_scores.append(score)\n",
    "                except JSONDecodeError as e:\n",
    "                    print(\"An error occurred while trying to decode the JSON response.\")\n",
    "                    print(combined_words)\n",
    "                    raise e\n",
    "\n",
    "            # Find the word with the highest score among candidates\n",
    "            if word_scores:\n",
    "                max_index = word_scores.index(max(word_scores))\n",
    "                if word_scores[max_index] > best_score:\n",
    "                    print(\"!!!!!!! IMPROVED SCORE !!!!!!!!!!!!!\")\n",
    "                    best_score = word_scores[max_index]\n",
    "                    best_combination = current_words[:]\n",
    "                    best_combination[i] = candidate_words[max_index]\n",
    "                    print(f\"Step {step+1}, Word {i+1}: New combination is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "\n",
    "        if current_words == best_combination:\n",
    "          max_index = word_scores.index(max(word_scores))\n",
    "          best_score = word_scores[max_index]\n",
    "          best_combination = current_words[:]\n",
    "          best_combination[i] = candidate_words[max_index]\n",
    "          print(f\"Step {step+1}, Word {i+1}: No Better Word Found but let us update combination is '{best_combination}' with score {best_score}\")\n",
    "\n",
    "        current_words = best_combination\n",
    "\n",
    "\n",
    "    return current_words\n",
    "\n",
    "# Assuming the existence of the required objects and methods:\n",
    "# - word2vec_model: A word2vec model object with a most_similar method and a vocab attribute\n",
    "# - manager: An instance of QueryManager with an appropriate query method\n",
    "\n",
    "# sw = 'person woman telemarker mankind room'.split()\n",
    "# sw = 'person woman human man people'.split()\n",
    "sw = ['woman', 'camera', 'human', 'man', 'people']\n",
    "sw = ['woman', 'woman', 'man', 'camera', 'camera']\n",
    "\n",
    "sw = ['person', 'woman', 'man', 'camera', 'camera']\n",
    "\n",
    "optimized_combination = optimize_word_closest(\n",
    "    word2vec_model, start_words=sw, n_steps=3, n_words=500, n_words_random=1\n",
    ")\n",
    "print(f\"Optimized combination: '{optimized_combination}'\")\n",
    "\n",
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT1S9h9oI9kD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8uutveyI9Xr",
    "outputId": "92f546a7-b95d-4912-921c-079b3843dcd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('someone', 0.6657356023788452),\n",
       " ('persons', 0.5559711456298828),\n",
       " ('woman', 0.5470173358917236),\n",
       " ('somebody', 0.5459041595458984),\n",
       " ('peson', 0.5421414375305176),\n",
       " ('man', 0.5342026352882385),\n",
       " ('people', 0.5083409547805786),\n",
       " ('anyone', 0.5061744451522827),\n",
       " ('guy', 0.48752474784851074),\n",
       " ('Someone', 0.473005473613739),\n",
       " ('individuals', 0.47029492259025574),\n",
       " ('foreigner', 0.46837544441223145),\n",
       " ('aperson', 0.4661904275417328),\n",
       " ('citizen', 0.4597834348678589),\n",
       " ('child', 0.4595995843410492),\n",
       " ('perpetrator', 0.45697149634361267),\n",
       " ('somone', 0.4560295641422272),\n",
       " ('anybody', 0.4523891806602478),\n",
       " ('degrating', 0.448263555765152),\n",
       " ('businessperson', 0.4365586042404175),\n",
       " ('applicant', 0.43481793999671936),\n",
       " ('perosn', 0.4329134523868561),\n",
       " ('conservatee', 0.43027427792549133),\n",
       " ('Anybody', 0.42944034934043884),\n",
       " ('sweetest_kindest', 0.42599958181381226),\n",
       " ('Lt._Duhamell', 0.42578843235969543),\n",
       " ('gentleman', 0.42577871680259705),\n",
       " ('DEAR_PERPLEXED', 0.42446368932724),\n",
       " ('referers_include', 0.4241100549697876),\n",
       " ('servicemember', 0.4219302237033844),\n",
       " ('self_injurer', 0.4210955500602722),\n",
       " ('Person', 0.420937716960907),\n",
       " ('victim', 0.4196123480796814),\n",
       " (\"Why_wasn'tI\", 0.4185786843299866),\n",
       " ('Dave_Chapelle_skit', 0.4185531735420227),\n",
       " ('Normal_indiviual', 0.41658851504325867),\n",
       " ('participant', 0.41511619091033936),\n",
       " ('normal_Salim_Ozonder', 0.4145938456058502),\n",
       " ('somebody_else', 0.41417208313941956),\n",
       " ('Genuine_callers', 0.41244417428970337),\n",
       " ('individual', 0.41209033131599426),\n",
       " ('thoughtful_considerate', 0.411201536655426),\n",
       " ('vicitm', 0.4097362458705902),\n",
       " ('employee', 0.40973594784736633),\n",
       " ('spouse', 0.4093315005302429),\n",
       " ('elector', 0.4092799425125122),\n",
       " (\"Sorry_ma'am\", 0.4087944030761719),\n",
       " ('Foulch', 0.4085026979446411),\n",
       " ('giver', 0.4067527651786804),\n",
       " ('clergyperson', 0.4065447151660919),\n",
       " ('oneself', 0.40529611706733704),\n",
       " ('implicated_Jessie_Dotson', 0.40413886308670044),\n",
       " ('caller', 0.40290337800979614),\n",
       " (\"Shouldn'ta\", 0.4022523760795593),\n",
       " ('taxpaying_citizen', 0.40146103501319885),\n",
       " ('law_abider', 0.40092846751213074),\n",
       " ('Autism_impairs', 0.40077918767929077),\n",
       " ('lady', 0.40049153566360474),\n",
       " ('caregiver', 0.3996128737926483),\n",
       " ('cura_personalis', 0.3993908762931824),\n",
       " ('whoever', 0.3984144330024719),\n",
       " ('defendant', 0.3983464539051056),\n",
       " ('Insisting_Polanski', 0.3975626826286316),\n",
       " ('http://sports.forsythnews.com/_encourages_readers', 0.39739179611206055),\n",
       " ('Reasonable_suspicion', 0.39717572927474976),\n",
       " (\"shouldn'tI\", 0.39694464206695557),\n",
       " ('New_Jerseyan', 0.3967835307121277),\n",
       " ('met_Kunis_gushes', 0.39629095792770386),\n",
       " ('hawking_Toyotas', 0.39602309465408325),\n",
       " ('kindest_sweetest', 0.3956812918186188),\n",
       " ('motorist', 0.3955065608024597),\n",
       " ('diagnosable_depression', 0.3951825797557831),\n",
       " ('Dear_Wondering', 0.39482760429382324),\n",
       " ('grandparent', 0.394328236579895),\n",
       " ('TheSouthern.com_encourages_readers', 0.3942968249320984),\n",
       " ('partygoer', 0.39387696981430054),\n",
       " ('arrestee', 0.3936104476451874),\n",
       " ('Webster_dictionary_defines', 0.3935740292072296),\n",
       " ('compassionate_empathetic', 0.3929189145565033),\n",
       " ('schmoe', 0.3911154866218567),\n",
       " ('litterer', 0.3897489011287689),\n",
       " ('girl', 0.3894531726837158),\n",
       " ('offender', 0.38929587602615356),\n",
       " ('limitations_Rivenburgh', 0.389177531003952),\n",
       " ('court_creates_conservatorships', 0.3890039920806885),\n",
       " ('Spokeswoman_Whitney_Jodry', 0.38889363408088684),\n",
       " ('ResourceMFG_delivers', 0.38796576857566833),\n",
       " ('founder_Lance_Loesberg', 0.3879650831222534),\n",
       " ('coworker', 0.3877306580543518),\n",
       " ('Barney_Fife_THAT', 0.3875638544559479),\n",
       " ('Anyone', 0.38689762353897095),\n",
       " ('Teri_Barbera_spokeswoman', 0.38672617077827454),\n",
       " ('sneezing_coughing_infected', 0.3865434527397156),\n",
       " ('human_beings', 0.386135995388031),\n",
       " ('child_ren', 0.38601773977279663),\n",
       " ('searcher', 0.38527506589889526),\n",
       " ('conversationalist', 0.38498905301094055),\n",
       " ('compos_mentis', 0.38480398058891296),\n",
       " ('parent_grandparent', 0.38408443331718445),\n",
       " ('CHARLIZE', 0.3837868273258209),\n",
       " ('complaintant', 0.3831250071525574),\n",
       " ('Jared_Laugher', 0.38253819942474365),\n",
       " ('recklessly_disregards', 0.38196155428886414),\n",
       " ('Anytime_anyplace', 0.38175633549690247),\n",
       " (\"AI'm\", 0.38082194328308105),\n",
       " ('commented_Debby_Herbenick', 0.38061752915382385),\n",
       " ('movie_fanatic_Carvey', 0.3805796205997467),\n",
       " ('fingerprinted_Hirst', 0.38047918677330017),\n",
       " ('Lucid_dreaming', 0.37975820899009705),\n",
       " ('entit_ies_each', 0.3791765868663788),\n",
       " ('juror', 0.3790614902973175),\n",
       " ('occupant', 0.37902215123176575),\n",
       " ('trespasser', 0.37888678908348083),\n",
       " ('Eveyone', 0.3786230981349945),\n",
       " ('aids_abets_counsels', 0.37860944867134094),\n",
       " ('patient', 0.3780183494091034),\n",
       " ('attendee', 0.37794917821884155),\n",
       " ('stands_Choucair', 0.37791907787323),\n",
       " ('griever', 0.37780964374542236),\n",
       " ('sane_rational', 0.37760722637176514),\n",
       " ('Erin_Hamley', 0.3773871660232544),\n",
       " ('BigLook###_CEO', 0.3773617148399353),\n",
       " ('politician', 0.3773418962955475),\n",
       " ('ChicagoSports.com_staff', 0.3772663176059723),\n",
       " ('signature_gatherer', 0.37725552916526794),\n",
       " ('salesperson', 0.37678831815719604),\n",
       " ('Debra_Graziano', 0.37627628445625305),\n",
       " ('wouild', 0.37624695897102356),\n",
       " ('Dear_Unsure', 0.3760950565338135),\n",
       " ('Gottfried_Hirnschall_director', 0.3758619725704193),\n",
       " ('Entrepreneur_Tadashi_Yanai', 0.3757999539375305),\n",
       " ('Massood_Akhtar', 0.375697523355484),\n",
       " ('Lawlessness_dictatorship', 0.3749177157878876),\n",
       " ('Johannas_Pope', 0.3747546672821045),\n",
       " ('BOUCHER_Yeah', 0.3745340406894684),\n",
       " ('claimant', 0.3745172321796417),\n",
       " ('Kleibecker', 0.3745075762271881),\n",
       " ('soldier', 0.37445908784866333),\n",
       " ('violator', 0.3744325637817383),\n",
       " ('nobody', 0.3741037845611572),\n",
       " ('Maxine_Sis_Cluse', 0.37406477332115173),\n",
       " ('interviewee', 0.3740382194519043),\n",
       " ('Bonny_Bakley', 0.3739168643951416),\n",
       " ('Dispatcher_Okay', 0.37378302216529846),\n",
       " ('Accomplishing_teamwork', 0.3732660412788391),\n",
       " ('Every_cooperator', 0.37321731448173523),\n",
       " ('litigant', 0.37285491824150085),\n",
       " ('nicest_nicest', 0.37217506766319275),\n",
       " ('moviegoer', 0.37206578254699707),\n",
       " ('personâ_€_™', 0.3719848096370697),\n",
       " ('Gabe_Soumakian', 0.3718074560165405),\n",
       " ('socialiser', 0.37151867151260376),\n",
       " ('aggravated_assault_recklessly_endangering', 0.371369332075119),\n",
       " ('sweetest_gentlest', 0.37117207050323486),\n",
       " ('gifter', 0.37101224064826965),\n",
       " ('borrower', 0.3708181381225586),\n",
       " ('preborn_baby', 0.37075990438461304),\n",
       " ('depo_MORE', 0.37072134017944336),\n",
       " ('atheist_agnostic_humanist', 0.37018465995788574),\n",
       " ('Michalyshen', 0.3699725866317749),\n",
       " ('inhabitant', 0.3695671260356903),\n",
       " ('homeowner', 0.36880967020988464),\n",
       " ('DEAR_CAT', 0.3684656322002411),\n",
       " ('underage_drinker', 0.36839476227760315),\n",
       " ('Relevant_Person', 0.3683629333972931),\n",
       " ('Still_LaCock_considered', 0.3679800033569336),\n",
       " ('Maro_Turcinovic', 0.3679494857788086),\n",
       " ('servant', 0.3678978979587555),\n",
       " ('opportunity_visit_http://www.earnparttimejobs.com/index.php?id=#######',\n",
       "  0.36788201332092285),\n",
       " ('grandchild', 0.3676014840602875),\n",
       " ('alienage', 0.36755290627479553),\n",
       " ('chief_Zafaruddin', 0.3668820559978485),\n",
       " ('whomever', 0.36674827337265015),\n",
       " ('Alleged_arsonist', 0.366630882024765),\n",
       " ('thing_Fustini', 0.36625438928604126),\n",
       " (\"Why_shouldn'ta\", 0.366151362657547),\n",
       " ('shooter_FPS_genre', 0.3661341667175293),\n",
       " ('toknow', 0.36605140566825867),\n",
       " ('practicer', 0.36600205302238464),\n",
       " ('accountholder', 0.36576420068740845),\n",
       " ('locate_Furuya', 0.36552003026008606),\n",
       " ('driver_Ganassi_snarked', 0.36551904678344727),\n",
       " ('dies_intestate', 0.3655036985874176),\n",
       " ('Company_Dealmaking_Profile', 0.36549460887908936),\n",
       " ('suspect', 0.36530163884162903),\n",
       " ('transmission_Widdowson', 0.36473897099494934),\n",
       " ('peron', 0.3646446168422699),\n",
       " ('center_Emma_Margraf', 0.3646124005317688),\n",
       " ('passerby', 0.3645484149456024),\n",
       " ('family_Tohkanen', 0.36441588401794434),\n",
       " ('Commander_Thomas_Stangrecki', 0.3643217384815216),\n",
       " ('Emergency_official_Pubucairen', 0.3641728162765503),\n",
       " ('guesser', 0.36398664116859436),\n",
       " ('moocher', 0.36394137144088745),\n",
       " ('requester', 0.36379602551460266),\n",
       " ('wrongdoer', 0.3636775612831116),\n",
       " ('Dear_Lonely', 0.3636474609375),\n",
       " ('Rodeffer_Theisen', 0.36348605155944824),\n",
       " ('HOW_TO_VISIT', 0.36341655254364014),\n",
       " ('Whoever', 0.36335262656211853)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word2vec_model.most_similar(\"person\", topn=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UpV80JlJIjm",
    "outputId": "0e342d17-178a-47c7-939c-a7c8249c835d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"whomever\" in common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs7woa6CI80J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1Mj10lmJvmU",
    "outputId": "abbcddb1-6da0-40d4-8bc4-87a117c8ab28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination with the maximum score is 'man woman man camera camera' with a score of 0.96\n"
     ]
    }
   ],
   "source": [
    "max_combination, max_score = manager.get_max_score_word()\n",
    "print(f\"The combination with the maximum score is '{max_combination}' with a score of {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdlyC1oCccOd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uo1yKkPccEZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1EazS3xcbsK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzLd79m0JvYP",
    "outputId": "7df51b9e-9dd7-4340-dd16-747265b728b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: m a n   w o m a n   m a n   c a m e r a   c a m e r a 0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = ['woman', 'woman', 'woman', 'man', 'camera']\n",
    "sw = ['woman', 'woman', 'man', 'camera', 'camera']\n",
    "# sw = ['man', 'woman', 'man', 'camera', 'camera']\n",
    "# sw = ['man', 'woman', 'man', 'camera', 'camera']\n",
    "\n",
    "sw = ['person', 'woman', 'man', 'camera', 'cameras']\n",
    "sw = \" \".join(['man', 'woman', 'man', 'camera', 'camera'])\n",
    "\n",
    "manager.query(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1HnaRv3JtqI",
    "outputId": "a3b96fa0-84b8-4996-9eed-e3630846a609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\": 0.96}\n",
      "{'message': 0.96}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_list = ['man', 'woman', 'man', 'camera', 'camera']\n",
    "words_key = ' '.join(words_list)  # Convert list to string to use as a key\n",
    "\n",
    "response = requests.post('http://semantle2.advml.com/score', json={'data': words_key})\n",
    "print(response.text)\n",
    "response_data = response.json()\n",
    "\n",
    "print(response_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uC0yTijdJtdL",
    "outputId": "71da3035-a932-4f9f-c639-bb460e14a4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: person woman man tape_recorder Cameras 0.93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = ['person', 'woman', 'man', 'tape_recorder', 'Cameras']\n",
    "\n",
    "manager.query(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wGUiQl2dJsS9",
    "outputId": "88580b96-b7e5-4e8b-81a7-38a624fac1fe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Failed to process input'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = [\"deutsch\", \"woman\", \"man\", \"camera\", \"camera\"]\n",
    "\n",
    "manager.query(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQ4coqVtDKUL",
    "outputId": "2fd7d9ff-7634-47ff-9c1a-6ed0632ed639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\": 0.9}\n",
      "{'message': 0.9}\n"
     ]
    }
   ],
   "source": [
    "input_data = [\"deutsch\", \"woman\", \"man\", \"camera\", \"camera\"]\n",
    "input_data = \" \".join(input_data)\n",
    "\n",
    "response = requests.post('http://semantle2.advml.com/score', json={'data': input_data})\n",
    "print(response.text)\n",
    "response_data = response.json()\n",
    "print(response_data)\n",
    "\n",
    "# if 'flag' in response_data:\n",
    "#     print(response.text)\n",
    "\n",
    "# score = response_data[\"message\"]\n",
    "# print(f\"query: {words_key} {score}\")\n",
    "# self.queried_words[words_key] = score\n",
    "# return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "89FkGwfAmXbk",
    "outputId": "7410b280-bbcd-469b-8191-a7904d1fd836"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'deutsch woman man camera camera'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2XZEEV1DKYB",
    "outputId": "96963eba-e125-4894-d552-53b5499d82fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog forest hat space wind': 0.74,\n",
       " 'dog jogging table earth wind': 0.76,\n",
       " 'dogs snowboarding_mountain_biking dining_room mankind wind': 0.83,\n",
       " 'dogs snowboarding_mountain_biking dining_room mankind gale': 0.83,\n",
       " 'person woman telemarker mankind room': 0.9,\n",
       " 'person woman telemarker mankind all': 0.89,\n",
       " 'someone woman telemarker mankind room': 0.85,\n",
       " 'persons woman telemarker mankind room': 0.88,\n",
       " 'somebody woman telemarker mankind room': 0.87,\n",
       " 'peson woman telemarker mankind room': 0.88,\n",
       " 'man woman telemarker mankind room': 0.89,\n",
       " 'people woman telemarker mankind room': 0.88,\n",
       " 'anyone woman telemarker mankind room': 0.82,\n",
       " 'guy woman telemarker mankind room': 0.88,\n",
       " 'Someone woman telemarker mankind room': 0.85,\n",
       " 'individuals woman telemarker mankind room': 0.87,\n",
       " 'foreigner woman telemarker mankind room': 0.84,\n",
       " 'aperson woman telemarker mankind room': 0.87,\n",
       " 'citizen woman telemarker mankind room': 0.86,\n",
       " 'child woman telemarker mankind room': 0.88,\n",
       " 'perpetrator woman telemarker mankind room': 0.86,\n",
       " 'somone woman telemarker mankind room': 0.85,\n",
       " 'anybody woman telemarker mankind room': 0.84,\n",
       " 'degrating woman telemarker mankind room': 0.83,\n",
       " 'businessperson woman telemarker mankind room': 0.87,\n",
       " 'applicant woman telemarker mankind room': 0.87,\n",
       " 'perosn woman telemarker mankind room': 0.86,\n",
       " 'conservatee woman telemarker mankind room': 0.85,\n",
       " 'Anybody woman telemarker mankind room': 0.84,\n",
       " 'sweetest_kindest woman telemarker mankind room': 0.83,\n",
       " 'Lt._Duhamell woman telemarker mankind room': 0.84,\n",
       " 'gentleman woman telemarker mankind room': 0.86,\n",
       " 'DEAR_PERPLEXED woman telemarker mankind room': 0.84,\n",
       " 'referers_include woman telemarker mankind room': 0.84,\n",
       " 'servicemember woman telemarker mankind room': 0.84,\n",
       " 'self_injurer woman telemarker mankind room': 0.85,\n",
       " 'Person woman telemarker mankind room': 0.88,\n",
       " 'victim woman telemarker mankind room': 0.87,\n",
       " \"Why_wasn'tI woman telemarker mankind room\": 0.81,\n",
       " 'Dave_Chapelle_skit woman telemarker mankind room': 0.84,\n",
       " 'Normal_indiviual woman telemarker mankind room': 0.86,\n",
       " 'participant woman telemarker mankind room': 0.87,\n",
       " 'normal_Salim_Ozonder woman telemarker mankind room': 0.84,\n",
       " 'somebody_else woman telemarker mankind room': 0.85,\n",
       " 'Genuine_callers woman telemarker mankind room': 0.82,\n",
       " 'individual woman telemarker mankind room': 0.86,\n",
       " 'thoughtful_considerate woman telemarker mankind room': 0.85,\n",
       " 'vicitm woman telemarker mankind room': 0.86,\n",
       " 'employee woman telemarker mankind room': 0.86,\n",
       " 'spouse woman telemarker mankind room': 0.87,\n",
       " 'elector woman telemarker mankind room': 0.86,\n",
       " \"Sorry_ma'am woman telemarker mankind room\": 0.84,\n",
       " 'Foulch woman telemarker mankind room': 0.84,\n",
       " 'giver woman telemarker mankind room': 0.85,\n",
       " 'clergyperson woman telemarker mankind room': 0.85,\n",
       " 'oneself woman telemarker mankind room': 0.86,\n",
       " 'implicated_Jessie_Dotson woman telemarker mankind room': 0.83,\n",
       " 'caller woman telemarker mankind room': 0.83,\n",
       " \"Shouldn'ta woman telemarker mankind room\": 0.81,\n",
       " 'taxpaying_citizen woman telemarker mankind room': 0.84,\n",
       " 'law_abider woman telemarker mankind room': 0.84,\n",
       " 'Autism_impairs woman telemarker mankind room': 0.83,\n",
       " 'lady woman telemarker mankind room': 0.87,\n",
       " 'caregiver woman telemarker mankind room': 0.85,\n",
       " 'cura_personalis woman telemarker mankind room': 0.86,\n",
       " 'whoever woman telemarker mankind room': 0.83,\n",
       " 'defendant woman telemarker mankind room': 0.81,\n",
       " 'Insisting_Polanski woman telemarker mankind room': 0.84,\n",
       " 'http://sports.forsythnews.com/_encourages_readers woman telemarker mankind room': 0.82,\n",
       " 'Reasonable_suspicion woman telemarker mankind room': 0.83,\n",
       " \"shouldn'tI woman telemarker mankind room\": 0.81,\n",
       " 'New_Jerseyan woman telemarker mankind room': 0.84,\n",
       " 'met_Kunis_gushes woman telemarker mankind room': 0.84,\n",
       " 'hawking_Toyotas woman telemarker mankind room': 0.86,\n",
       " 'kindest_sweetest woman telemarker mankind room': 0.82,\n",
       " 'motorist woman telemarker mankind room': 0.87,\n",
       " 'diagnosable_depression woman telemarker mankind room': 0.83,\n",
       " 'Dear_Wondering woman telemarker mankind room': 0.84,\n",
       " 'grandparent woman telemarker mankind room': 0.84,\n",
       " 'TheSouthern.com_encourages_readers woman telemarker mankind room': 0.81,\n",
       " 'partygoer woman telemarker mankind room': 0.87,\n",
       " 'arrestee woman telemarker mankind room': 0.86,\n",
       " 'Webster_dictionary_defines woman telemarker mankind room': 0.84,\n",
       " 'compassionate_empathetic woman telemarker mankind room': 0.83,\n",
       " 'schmoe woman telemarker mankind room': 0.86,\n",
       " 'litterer woman telemarker mankind room': 0.85,\n",
       " 'girl woman telemarker mankind room': 0.88,\n",
       " 'offender woman telemarker mankind room': 0.84,\n",
       " 'limitations_Rivenburgh woman telemarker mankind room': 0.83,\n",
       " 'court_creates_conservatorships woman telemarker mankind room': 0.82,\n",
       " 'Spokeswoman_Whitney_Jodry woman telemarker mankind room': 0.84,\n",
       " 'ResourceMFG_delivers woman telemarker mankind room': 0.83,\n",
       " 'founder_Lance_Loesberg woman telemarker mankind room': 0.83,\n",
       " 'coworker woman telemarker mankind room': 0.85,\n",
       " 'Barney_Fife_THAT woman telemarker mankind room': 0.82,\n",
       " 'Anyone woman telemarker mankind room': 0.84,\n",
       " 'Teri_Barbera_spokeswoman woman telemarker mankind room': 0.83,\n",
       " 'sneezing_coughing_infected woman telemarker mankind room': 0.84,\n",
       " 'human_beings woman telemarker mankind room': 0.87,\n",
       " 'child_ren woman telemarker mankind room': 0.87,\n",
       " 'searcher woman telemarker mankind room': 0.86,\n",
       " 'conversationalist woman telemarker mankind room': 0.85,\n",
       " 'compos_mentis woman telemarker mankind room': 0.87,\n",
       " 'parent_grandparent woman telemarker mankind room': 0.86,\n",
       " 'CHARLIZE woman telemarker mankind room': 0.83,\n",
       " 'complaintant woman telemarker mankind room': 0.84,\n",
       " 'Jared_Laugher woman telemarker mankind room': 0.84,\n",
       " 'recklessly_disregards woman telemarker mankind room': 0.82,\n",
       " 'Anytime_anyplace woman telemarker mankind room': 0.85,\n",
       " \"AI'm woman telemarker mankind room\": 0.85,\n",
       " 'commented_Debby_Herbenick woman telemarker mankind room': 0.84,\n",
       " 'movie_fanatic_Carvey woman telemarker mankind room': 0.85,\n",
       " 'fingerprinted_Hirst woman telemarker mankind room': 0.84,\n",
       " 'Lucid_dreaming woman telemarker mankind room': 0.84,\n",
       " 'entit_ies_each woman telemarker mankind room': 0.85,\n",
       " 'juror woman telemarker mankind room': 0.85,\n",
       " 'occupant woman telemarker mankind room': 0.88,\n",
       " 'trespasser woman telemarker mankind room': 0.86,\n",
       " 'Eveyone woman telemarker mankind room': 0.83,\n",
       " 'aids_abets_counsels woman telemarker mankind room': 0.82,\n",
       " 'patient woman telemarker mankind room': 0.86,\n",
       " 'attendee woman telemarker mankind room': 0.85,\n",
       " 'stands_Choucair woman telemarker mankind room': 0.86,\n",
       " 'griever woman telemarker mankind room': 0.86,\n",
       " 'sane_rational woman telemarker mankind room': 0.84,\n",
       " 'Erin_Hamley woman telemarker mankind room': 0.84,\n",
       " 'BigLook###_CEO woman telemarker mankind room': 0.82,\n",
       " 'politician woman telemarker mankind room': 0.85,\n",
       " 'ChicagoSports.com_staff woman telemarker mankind room': 0.83,\n",
       " 'signature_gatherer woman telemarker mankind room': 0.85,\n",
       " 'salesperson woman telemarker mankind room': 0.86,\n",
       " 'Debra_Graziano woman telemarker mankind room': 0.85,\n",
       " 'wouild woman telemarker mankind room': 0.86,\n",
       " 'Dear_Unsure woman telemarker mankind room': 0.85,\n",
       " 'Gottfried_Hirnschall_director woman telemarker mankind room': 0.86,\n",
       " 'Entrepreneur_Tadashi_Yanai woman telemarker mankind room': 0.83,\n",
       " 'Massood_Akhtar woman telemarker mankind room': 0.84,\n",
       " 'Lawlessness_dictatorship woman telemarker mankind room': 0.83,\n",
       " 'Johannas_Pope woman telemarker mankind room': 0.84,\n",
       " 'BOUCHER_Yeah woman telemarker mankind room': 0.85,\n",
       " 'claimant woman telemarker mankind room': 0.83,\n",
       " 'Kleibecker woman telemarker mankind room': 0.85,\n",
       " 'soldier woman telemarker mankind room': 0.86,\n",
       " 'violator woman telemarker mankind room': 0.85,\n",
       " 'nobody woman telemarker mankind room': 0.85,\n",
       " 'Maxine_Sis_Cluse woman telemarker mankind room': 0.84,\n",
       " 'interviewee woman telemarker mankind room': 0.86,\n",
       " 'Bonny_Bakley woman telemarker mankind room': 0.85,\n",
       " 'Dispatcher_Okay woman telemarker mankind room': 0.83,\n",
       " 'Accomplishing_teamwork woman telemarker mankind room': 0.84,\n",
       " 'Every_cooperator woman telemarker mankind room': 0.84,\n",
       " 'litigant woman telemarker mankind room': 0.85,\n",
       " 'nicest_nicest woman telemarker mankind room': 0.83,\n",
       " 'moviegoer woman telemarker mankind room': 0.87,\n",
       " 'personâ_€_™ woman telemarker mankind room': 0.87,\n",
       " 'Gabe_Soumakian woman telemarker mankind room': 0.85,\n",
       " 'socialiser woman telemarker mankind room': 0.85,\n",
       " 'aggravated_assault_recklessly_endangering woman telemarker mankind room': 0.81,\n",
       " 'sweetest_gentlest woman telemarker mankind room': 0.82,\n",
       " 'gifter woman telemarker mankind room': 0.84,\n",
       " 'borrower woman telemarker mankind room': 0.86,\n",
       " 'preborn_baby woman telemarker mankind room': 0.85,\n",
       " 'depo_MORE woman telemarker mankind room': 0.83,\n",
       " 'atheist_agnostic_humanist woman telemarker mankind room': 0.84,\n",
       " 'Michalyshen woman telemarker mankind room': 0.84,\n",
       " 'inhabitant woman telemarker mankind room': 0.87,\n",
       " 'homeowner woman telemarker mankind room': 0.85,\n",
       " 'DEAR_CAT woman telemarker mankind room': 0.84,\n",
       " 'underage_drinker woman telemarker mankind room': 0.84,\n",
       " 'Relevant_Person woman telemarker mankind room': 0.87,\n",
       " 'Still_LaCock_considered woman telemarker mankind room': 0.84,\n",
       " 'Maro_Turcinovic woman telemarker mankind room': 0.86,\n",
       " 'servant woman telemarker mankind room': 0.84,\n",
       " 'opportunity_visit_http://www.earnparttimejobs.com/index.php?id=####### woman telemarker mankind room': 0.8,\n",
       " 'grandchild woman telemarker mankind room': 0.85,\n",
       " 'alienage woman telemarker mankind room': 0.84,\n",
       " 'chief_Zafaruddin woman telemarker mankind room': 0.82,\n",
       " 'whomever woman telemarker mankind room': 0.86,\n",
       " 'Alleged_arsonist woman telemarker mankind room': 0.82,\n",
       " 'thing_Fustini woman telemarker mankind room': 0.85,\n",
       " \"Why_shouldn'ta woman telemarker mankind room\": 0.8,\n",
       " 'shooter_FPS_genre woman telemarker mankind room': 0.87,\n",
       " 'toknow woman telemarker mankind room': 0.85,\n",
       " 'practicer woman telemarker mankind room': 0.85,\n",
       " 'accountholder woman telemarker mankind room': 0.86,\n",
       " 'locate_Furuya woman telemarker mankind room': 0.8,\n",
       " 'driver_Ganassi_snarked woman telemarker mankind room': 0.82,\n",
       " 'dies_intestate woman telemarker mankind room': 0.84,\n",
       " 'Company_Dealmaking_Profile woman telemarker mankind room': 0.85,\n",
       " 'suspect woman telemarker mankind room': 0.84,\n",
       " 'transmission_Widdowson woman telemarker mankind room': 0.85,\n",
       " 'peron woman telemarker mankind room': 0.87,\n",
       " 'center_Emma_Margraf woman telemarker mankind room': 0.84,\n",
       " 'passerby woman telemarker mankind room': 0.86,\n",
       " 'family_Tohkanen woman telemarker mankind room': 0.85,\n",
       " 'Commander_Thomas_Stangrecki woman telemarker mankind room': 0.83,\n",
       " 'Emergency_official_Pubucairen woman telemarker mankind room': 0.83,\n",
       " 'guesser woman telemarker mankind room': 0.86,\n",
       " 'moocher woman telemarker mankind room': 0.84,\n",
       " 'requester woman telemarker mankind room': 0.86,\n",
       " 'wrongdoer woman telemarker mankind room': 0.84,\n",
       " 'Dear_Lonely woman telemarker mankind room': 0.84,\n",
       " 'Rodeffer_Theisen woman telemarker mankind room': 0.84,\n",
       " 'HOW_TO_VISIT woman telemarker mankind room': 0.82,\n",
       " 'Whoever woman telemarker mankind room': 0.84,\n",
       " \"21/64'' woman telemarker mankind room\": 0.84,\n",
       " 'baked woman telemarker mankind room': 0.86,\n",
       " 'ire woman telemarker mankind room': 0.87,\n",
       " 'glued woman telemarker mankind room': 0.85,\n",
       " 'vested woman telemarker mankind room': 0.85,\n",
       " 'avoiding woman telemarker mankind room': 0.82,\n",
       " 'gladius woman telemarker mankind room': 0.84,\n",
       " 'cochran woman telemarker mankind room': 0.83,\n",
       " 'stethoscope woman telemarker mankind room': 0.85,\n",
       " 'exceedingly woman telemarker mankind room': 0.85,\n",
       " '1,500 woman telemarker mankind room': 0.83,\n",
       " 'compound woman telemarker mankind room': 0.84,\n",
       " 'mg. woman telemarker mankind room': 0.85,\n",
       " 'labor-based woman telemarker mankind room': 0.84,\n",
       " 'optimistic woman telemarker mankind room': 0.84,\n",
       " 'couples woman telemarker mankind room': 0.89,\n",
       " 'big-large woman telemarker mankind room': 0.85,\n",
       " '114 woman telemarker mankind room': 0.83,\n",
       " 'tomes woman telemarker mankind room': 0.85,\n",
       " 'mistress woman telemarker mankind room': 0.84,\n",
       " 'mouthful woman telemarker mankind room': 0.86,\n",
       " 'code woman telemarker mankind room': 0.84,\n",
       " 'couched woman telemarker mankind room': 0.86,\n",
       " 'van woman telemarker mankind room': 0.87,\n",
       " 'dust woman telemarker mankind room': 0.84,\n",
       " 'sante woman telemarker mankind room': 0.84,\n",
       " 'di-iodotyrosine woman telemarker mankind room': 0.83,\n",
       " 'spinrad woman telemarker mankind room': 0.84,\n",
       " 'dynamical woman telemarker mankind room': 0.85,\n",
       " 'bulge woman telemarker mankind room': 0.86,\n",
       " 'festivals woman telemarker mankind room': 0.86,\n",
       " 'circulate woman telemarker mankind room': 0.84,\n",
       " 'labrador woman telemarker mankind room': 0.85,\n",
       " 'telegram woman telemarker mankind room': 0.83,\n",
       " '$4.9 woman telemarker mankind room': 0.84,\n",
       " 'feelings woman telemarker mankind room': 0.85,\n",
       " 'utilities woman telemarker mankind room': 0.84,\n",
       " 'egotism woman telemarker mankind room': 0.84,\n",
       " 'feminist woman telemarker mankind room': 0.86,\n",
       " 'd-c woman telemarker mankind room': 0.86,\n",
       " 'misunderstanding woman telemarker mankind room': 0.84,\n",
       " \"labor's woman telemarker mankind room\": 0.85,\n",
       " 'mournful woman telemarker mankind room': 0.84,\n",
       " 'crows woman telemarker mankind room': 0.83,\n",
       " 'emasculation woman telemarker mankind room': 0.85,\n",
       " \"katie's woman telemarker mankind room\": 0.85,\n",
       " 'windowpanes woman telemarker mankind room': 0.86,\n",
       " 'seven-concert woman telemarker mankind room': 0.84,\n",
       " 'beowulf woman telemarker mankind room': 0.82,\n",
       " 'notre-dame woman telemarker mankind room': 0.84,\n",
       " \"wagner's woman telemarker mankind room\": 0.84,\n",
       " 'speculation woman telemarker mankind room': 0.86,\n",
       " 'veined woman telemarker mankind room': 0.85,\n",
       " 'reiterate woman telemarker mankind room': 0.85,\n",
       " '50,000,000 woman telemarker mankind room': 0.83,\n",
       " 'mandatory woman telemarker mankind room': 0.84,\n",
       " 'incumbents woman telemarker mankind room': 0.85,\n",
       " 'influence woman telemarker mankind room': 0.84,\n",
       " 'cycly woman telemarker mankind room': 0.86,\n",
       " 'smacks woman telemarker mankind room': 0.84,\n",
       " 'nooks woman telemarker mankind room': 0.85,\n",
       " \"rca-victor's woman telemarker mankind room\": 0.84,\n",
       " 'flats woman telemarker mankind room': 0.86,\n",
       " 'witty woman telemarker mankind room': 0.85,\n",
       " '376 woman telemarker mankind room': 0.81,\n",
       " 'sowered woman telemarker mankind room': 0.85,\n",
       " 'idyll woman telemarker mankind room': 0.86,\n",
       " 'demanding woman telemarker mankind room': 0.84,\n",
       " 'repel woman telemarker mankind room': 0.82,\n",
       " 'bonhoffer woman telemarker mankind room': 0.84,\n",
       " 'pre-anglo-saxon woman telemarker mankind room': 0.81,\n",
       " 'micrometeorite woman telemarker mankind room': 0.85,\n",
       " 'nogay woman telemarker mankind room': 0.85,\n",
       " 'proportional woman telemarker mankind room': 0.85,\n",
       " 'winners woman telemarker mankind room': 0.83,\n",
       " '$22.50 woman telemarker mankind room': 0.83,\n",
       " 'playtime woman telemarker mankind room': 0.86,\n",
       " \"men's woman telemarker mankind room\": 0.87,\n",
       " 'splashy woman telemarker mankind room': 0.86,\n",
       " 'cosmology woman telemarker mankind room': 0.83,\n",
       " \"educator's woman telemarker mankind room\": 0.85,\n",
       " 'movie-goer woman telemarker mankind room': 0.87,\n",
       " 'coloring woman telemarker mankind room': 0.84,\n",
       " 'sovereigns woman telemarker mankind room': 0.85,\n",
       " 'formative woman telemarker mankind room': 0.85,\n",
       " '1665.32 woman telemarker mankind room': 0.82,\n",
       " 'pettit woman telemarker mankind room': 0.86,\n",
       " 'hearn woman telemarker mankind room': 0.85,\n",
       " 'signor woman telemarker mankind room': 0.86,\n",
       " 'ankle-deep woman telemarker mankind room': 0.84,\n",
       " 'non-propagandistic woman telemarker mankind room': 0.85,\n",
       " 'aquisition woman telemarker mankind room': 0.85,\n",
       " 'semi-minor woman telemarker mankind room': 0.86,\n",
       " '$2,700 woman telemarker mankind room': 0.82,\n",
       " 'verisimilitude woman telemarker mankind room': 0.86,\n",
       " 'automobiles woman telemarker mankind room': 0.86,\n",
       " 'dow woman telemarker mankind room': 0.84,\n",
       " 'leave woman telemarker mankind room': 0.83,\n",
       " 'sacrifices woman telemarker mankind room': 0.84,\n",
       " 'drove woman telemarker mankind room': 0.85,\n",
       " 'person man telemarker mankind room': 0.88,\n",
       " 'person girl telemarker mankind room': 0.88,\n",
       " 'person teenage_girl telemarker mankind room': 0.86,\n",
       " 'person teenager telemarker mankind room': 0.86,\n",
       " 'person lady telemarker mankind room': 0.88,\n",
       " 'person teenaged_girl telemarker mankind room': 0.85,\n",
       " 'person mother telemarker mankind room': 0.86,\n",
       " 'person policewoman telemarker mankind room': 0.87,\n",
       " 'person boy telemarker mankind room': 0.87,\n",
       " 'person Woman telemarker mankind room': 0.88,\n",
       " 'person sexually_assualted telemarker mankind room': 0.82,\n",
       " 'person she telemarker mankind room': 0.85,\n",
       " 'person Leah_Questin telemarker mankind room': 0.83,\n",
       " 'person WOMAN telemarker mankind room': 0.88,\n",
       " 'person housewife telemarker mankind room': 0.87,\n",
       " 'person victim telemarker mankind room': 0.87,\n",
       " 'person daughter telemarker mankind room': 0.85,\n",
       " 'person grandmother telemarker mankind room': 0.86,\n",
       " 'person schoolgirl telemarker mankind room': 0.87,\n",
       " 'person teen_ager telemarker mankind room': 0.85,\n",
       " 'person her telemarker mankind room': 0.86,\n",
       " 'person TEENAGE_girl telemarker mankind room': 0.85,\n",
       " 'person businesswoman telemarker mankind room': 0.88,\n",
       " 'person Latoyia_Figueroa telemarker mankind room': 0.84,\n",
       " 'person women telemarker mankind room': 0.88,\n",
       " 'person Gunshot_victim telemarker mankind room': 0.84,\n",
       " 'person newborn_baby telemarker mankind room': 0.85,\n",
       " 'person awoman telemarker mankind room': 0.88,\n",
       " 'person Masego_Kgomo telemarker mankind room': 0.82,\n",
       " 'person Yannick_Brea telemarker mankind room': 0.83,\n",
       " 'person Rachel_Wattenbarger telemarker mankind room': 0.83,\n",
       " 'person prostitute telemarker mankind room': 0.85,\n",
       " 'person Carole_Nordella telemarker mankind room': 0.84,\n",
       " 'person toddler telemarker mankind room': 0.85,\n",
       " 'person stepdaughter telemarker mankind room': 0.85,\n",
       " 'person niece telemarker mankind room': 0.84,\n",
       " 'person womans telemarker mankind room': 0.87,\n",
       " 'person Mbarek_Lafrem telemarker mankind room': 0.83,\n",
       " 'person motorist telemarker mankind room': 0.86,\n",
       " 'person boyfriend telemarker mankind room': 0.86,\n",
       " 'person Manious telemarker mankind room': 0.85,\n",
       " 'person divorcee telemarker mankind room': 0.86,\n",
       " 'person Stabbing_victim telemarker mankind room': 0.84,\n",
       " 'person Suspected_burglar telemarker mankind room': 0.83,\n",
       " 'person nun telemarker mankind room': 0.86,\n",
       " 'person waitress telemarker mankind room': 0.85,\n",
       " 'person Matthew_Tassio telemarker mankind room': 0.83,\n",
       " 'person hitchhiker telemarker mankind room': 0.85,\n",
       " 'person vicitm telemarker mankind room': 0.85,\n",
       " 'person HEAVILY_pregnant_woman telemarker mankind room': 0.84,\n",
       " 'person Stab_victim telemarker mankind room': 0.84,\n",
       " 'person girlfriend telemarker mankind room': 0.87,\n",
       " 'person maid telemarker mankind room': 0.85,\n",
       " 'person bicyclist telemarker mankind room': 0.85,\n",
       " 'person Candice_Moncayo telemarker mankind room': 0.84,\n",
       " 'person dementia_sufferer telemarker mankind room': 0.84,\n",
       " 'person sexually_assaulting_developmentally_disabled telemarker mankind room': 0.81,\n",
       " 'person passerby telemarker mankind room': 0.86,\n",
       " 'person wielding_screwdriver telemarker mankind room': 0.84,\n",
       " 'person FRAIL_pensioner telemarker mankind room': 0.83,\n",
       " 'person Attempted_carjacking telemarker mankind room': 0.82,\n",
       " 'person female telemarker mankind room': 0.88,\n",
       " 'person Donnisha_Hill telemarker mankind room': 0.83,\n",
       " 'person motorcyclist telemarker mankind room': 0.86,\n",
       " 'person vicitim telemarker mankind room': 0.85,\n",
       " 'person jogger telemarker mankind room': 0.86,\n",
       " \"person QI'ma telemarker mankind room\": 0.83,\n",
       " 'person Yanisa_Fonteece telemarker mankind room': 0.84,\n",
       " 'person transwoman telemarker mankind room': 0.86,\n",
       " 'person Purse_snatched telemarker mankind room': 0.84,\n",
       " 'person wielding_butcher_knife telemarker mankind room': 0.83,\n",
       " 'person Miguel_Carrasquillo telemarker mankind room': 0.82,\n",
       " 'person husband telemarker mankind room': 0.87,\n",
       " 'person AN_##-YEAR-OLD telemarker mankind room': 0.84,\n",
       " 'person Ahmed_Nahl telemarker mankind room': 0.83,\n",
       " 'person WHEELCHAIR_bound telemarker mankind room': 0.85,\n",
       " 'person aunt telemarker mankind room': 0.84,\n",
       " 'person Tonya_Evette_Johnson telemarker mankind room': 0.83,\n",
       " 'person Chihuahua_puppy telemarker mankind room': 0.85,\n",
       " 'person Galperina telemarker mankind room': 0.84,\n",
       " 'person Baby_Jane_Doe telemarker mankind room': 0.84,\n",
       " 'person Marice_McGregor telemarker mankind room': 0.85,\n",
       " 'person Kristyn_Haino telemarker mankind room': 0.83,\n",
       " 'person Raechel_Betts telemarker mankind room': 0.83,\n",
       " 'person window_peeper telemarker mankind room': 0.85,\n",
       " 'person knife_wielding_thief telemarker mankind room': 0.84,\n",
       " 'person herself telemarker mankind room': 0.85,\n",
       " 'person Walmart_greeter telemarker mankind room': 0.83,\n",
       " 'person Nimisha_Tiwari telemarker mankind room': 0.82,\n",
       " 'person Stabbing_suspect telemarker mankind room': 0.83,\n",
       " 'person Sascha_Schmidt telemarker mankind room': 0.83,\n",
       " 'person someone telemarker mankind room': 0.85,\n",
       " 'person Decomposed_body telemarker mankind room': 0.84,\n",
       " 'person Alleged_rapist telemarker mankind room': 0.81,\n",
       " 'person Sabbar_Kashur telemarker mankind room': 0.82,\n",
       " 'person Robbery_suspect telemarker mankind room': 0.84,\n",
       " 'person Tiferet_Tratner telemarker mankind room': 0.83,\n",
       " 'person taxi_cab_driver telemarker mankind room': 0.85,\n",
       " 'person Good_samaritan telemarker mankind room': 0.83,\n",
       " \"person DEAR_ELLIE_I'ma telemarker mankind room\": 0.82,\n",
       " 'person moped_rider telemarker mankind room': 0.85,\n",
       " 'person Magdeline_Makola telemarker mankind room': 0.84,\n",
       " 'person Jeffrey_Dolloff telemarker mankind room': 0.83,\n",
       " 'person AN_ELDERLY_woman telemarker mankind room': 0.85,\n",
       " 'person divorcée telemarker mankind room': 0.85,\n",
       " 'person NEWBORN_baby telemarker mankind room': 0.84,\n",
       " 'person Attempted_abduction telemarker mankind room': 0.82,\n",
       " 'person Woman_recants telemarker mankind room': 0.86,\n",
       " 'person Lisa_Keem telemarker mankind room': 0.83,\n",
       " 'person assailant telemarker mankind room': 0.83,\n",
       " 'person Herre_Bagwell telemarker mankind room': 0.84,\n",
       " 'person Irene_Prusik telemarker mankind room': 0.85,\n",
       " 'person Saul_Dos_Reis telemarker mankind room': 0.83,\n",
       " 'person tow_truck_driver telemarker mankind room': 0.85,\n",
       " 'person Michelle_Nyce telemarker mankind room': 0.84,\n",
       " 'person masked_intruder telemarker mankind room': 0.84,\n",
       " 'person Nakisha_Allen telemarker mankind room': 0.82,\n",
       " 'person good_samaritan telemarker mankind room': 0.83,\n",
       " 'person sexually_propositioning telemarker mankind room': 0.85,\n",
       " 'person nightclubber telemarker mankind room': 0.85,\n",
       " 'person housekeeper telemarker mankind room': 0.85,\n",
       " 'person men telemarker mankind room': 0.87,\n",
       " 'person Missing_canoeist telemarker mankind room': 0.83,\n",
       " 'person GREAT_GRANDMOTHER telemarker mankind room': 0.84,\n",
       " 'person Sandra_Lisset_Castro telemarker mankind room': 0.84,\n",
       " 'person Attempted_robbery telemarker mankind room': 0.82,\n",
       " 'person Teenaged_girl telemarker mankind room': 0.85,\n",
       " 'person pharaoh_Queen_Hatshepsut telemarker mankind room': 0.8,\n",
       " 'person suspected_purse_snatcher telemarker mankind room': 0.82,\n",
       " 'person Janet_Abaroa telemarker mankind room': 0.84,\n",
       " 'person child telemarker mankind room': 0.87,\n",
       " 'person Suspected_kidnapper telemarker mankind room': 0.83,\n",
       " 'person Sayed_Wakhan_sunburned telemarker mankind room': 0.83,\n",
       " 'person collapse_Betsy_Sathers telemarker mankind room': 0.83,\n",
       " 'person burqa_clad_woman telemarker mankind room': 0.86,\n",
       " 'person Purse_snatcher telemarker mankind room': 0.84,\n",
       " 'person Ms_Naggs telemarker mankind room': 0.82,\n",
       " 'person housemaid telemarker mankind room': 0.85,\n",
       " 'person Elizabeth_Lafantaisie telemarker mankind room': 0.83,\n",
       " 'person accomplice_Hudgens telemarker mankind room': 0.84,\n",
       " 'person motorcylist telemarker mankind room': 0.85,\n",
       " 'person Ina_Caterina_Remhof telemarker mankind room': 0.83,\n",
       " 'person YOUNG_mum telemarker mankind room': 0.85,\n",
       " 'person spokeswoman_Eren_Stephens telemarker mankind room': 0.83,\n",
       " 'person Lucille_Runyan telemarker mankind room': 0.83,\n",
       " 'person partially_disrobed telemarker mankind room': 0.86,\n",
       " 'person purse_snatcher telemarker mankind room': 0.85,\n",
       " 'person cab_driver telemarker mankind room': 0.84,\n",
       " 'person preoperative_transsexual telemarker mankind room': 0.83,\n",
       " 'person sunbather telemarker mankind room': 0.86,\n",
       " 'person contracted_venereal_disease telemarker mankind room': 0.81,\n",
       " 'person Drawbough telemarker mankind room': 0.83,\n",
       " 'person hitch_hiker telemarker mankind room': 0.84,\n",
       " 'person maroon_minivan telemarker mankind room': 0.86,\n",
       " 'person Ms_Knowles_Samarraie telemarker mankind room': 0.84,\n",
       " 'person SEX_fiend telemarker mankind room': 0.84,\n",
       " 'person vivacious_blonde telemarker mankind room': 0.85,\n",
       " 'person botched_purse_snatching telemarker mankind room': 0.83,\n",
       " 'person newlywed telemarker mankind room': 0.86,\n",
       " 'person suburban_housewife telemarker mankind room': 0.87,\n",
       " 'person pizza_deliverer telemarker mankind room': 0.84,\n",
       " 'person teenage_girls telemarker mankind room': 0.85,\n",
       " 'person Bridget_Sanetti telemarker mankind room': 0.84,\n",
       " 'person dismemberment_slaying telemarker mankind room': 0.84,\n",
       " 'person knife_wielding_attacker telemarker mankind room': 0.84,\n",
       " 'person Pellus telemarker mankind room': 0.82,\n",
       " 'person Alleged_kidnapper telemarker mankind room': 0.82,\n",
       " 'person Gunman_flees telemarker mankind room': 0.83,\n",
       " 'person Ranjeeta_Sharma telemarker mankind room': 0.82,\n",
       " 'person Fatal_stabbing telemarker mankind room': 0.82,\n",
       " 'person blonde telemarker mankind room': 0.86,\n",
       " 'person serial_flasher telemarker mankind room': 0.84,\n",
       " 'person Geraldine_Beardy telemarker mankind room': 0.83,\n",
       " 'person ex_boyfriend telemarker mankind room': 0.85,\n",
       " 'person Johannas_Pope telemarker mankind room': 0.82,\n",
       " 'person captive_Faye_Turney telemarker mankind room': 0.83,\n",
       " 'person Paroled_killer telemarker mankind room': 0.83,\n",
       " 'person Dorothy_Wight telemarker mankind room': 0.84,\n",
       " 'person car_jacker telemarker mankind room': 0.85,\n",
       " 'person Caira_Ferguson telemarker mankind room': 0.83,\n",
       " 'person Dog_mauls telemarker mankind room': 0.84,\n",
       " 'person carjacker telemarker mankind room': 0.83,\n",
       " 'person Alleged_robber telemarker mankind room': 0.82,\n",
       " 'person knife_wielding_assailant telemarker mankind room': 0.84,\n",
       " 'person passenger_Demaris_Meyer telemarker mankind room': 0.83,\n",
       " 'person Fort_Bliss_soldier telemarker mankind room': 0.82,\n",
       " 'person Sandra_Teichow telemarker mankind room': 0.85,\n",
       " 'person heartless_thug telemarker mankind room': 0.82,\n",
       " 'person female_jogger telemarker mankind room': 0.87,\n",
       " 'person POLICEWOMAN telemarker mankind room': 0.86,\n",
       " 'person initials_MFR_died telemarker mankind room': 0.81,\n",
       " 'person Giuseppina_Pasqualino_di_Marineo telemarker mankind room': 0.83,\n",
       " 'person Tina_Adovasio telemarker mankind room': 0.85,\n",
       " 'person Patricia_Pokriots telemarker mankind room': 0.83,\n",
       " 'person Arsema_Dawit telemarker mankind room': 0.84,\n",
       " 'person Bledaite telemarker mankind room': 0.83,\n",
       " 'person hyperventilating_breathlessly_squealing telemarker mankind room': 0.83,\n",
       " 'person GOOD_Samaritan telemarker mankind room': 0.83,\n",
       " 'person Deshira_Selimaj telemarker mankind room': 0.83,\n",
       " 'person mournfully telemarker mankind room': 0.83,\n",
       " 'person tubs telemarker mankind room': 0.86,\n",
       " 'person naked telemarker mankind room': 0.86,\n",
       " 'person lauchli telemarker mankind room': 0.84,\n",
       " 'person felons telemarker mankind room': 0.83,\n",
       " 'person musing telemarker mankind room': 0.85,\n",
       " 'person delivered telemarker mankind room': 0.84,\n",
       " 'person wisely telemarker mankind room': 0.85,\n",
       " 'person redhead telemarker mankind room': 0.85,\n",
       " 'person composes telemarker mankind room': 0.85,\n",
       " 'person comprising telemarker mankind room': 0.85,\n",
       " 'person deller telemarker mankind room': 0.85,\n",
       " 'person trig telemarker mankind room': 0.84,\n",
       " 'person fling telemarker mankind room': 0.85,\n",
       " 'person contacted telemarker mankind room': 0.83,\n",
       " 'person unimposing telemarker mankind room': 0.87,\n",
       " 'person remove telemarker mankind room': 0.84,\n",
       " 'person bloops telemarker mankind room': 0.84,\n",
       " 'person fires telemarker mankind room': 0.84,\n",
       " 'person were telemarker mankind room': 0.86,\n",
       " 'person $0.9 telemarker mankind room': 0.83,\n",
       " 'person stained telemarker mankind room': 0.83,\n",
       " \"person harbor's telemarker mankind room\": 0.83,\n",
       " 'person loveliest telemarker mankind room': 0.85,\n",
       " 'person two-part telemarker mankind room': 0.86,\n",
       " 'person oriental telemarker mankind room': 0.85,\n",
       " 'person spilling telemarker mankind room': 0.84,\n",
       " 'person rolls-royces telemarker mankind room': 0.84,\n",
       " \"person euclid's telemarker mankind room\": 0.82,\n",
       " 'person springing telemarker mankind room': 0.84,\n",
       " 'person home-run telemarker mankind room': 0.84,\n",
       " 'person rustled telemarker mankind room': 0.83,\n",
       " 'person lonelier telemarker mankind room': 0.85,\n",
       " 'person leyden telemarker mankind room': 0.83,\n",
       " 'person 100% telemarker mankind room': 0.85,\n",
       " 'person esteem telemarker mankind room': 0.86,\n",
       " 'person rearranging telemarker mankind room': 0.85,\n",
       " \"person celie's telemarker mankind room\": 0.83,\n",
       " 'person donovan telemarker mankind room': 0.84,\n",
       " 'person clench telemarker mankind room': 0.85,\n",
       " 'person pettibone telemarker mankind room': 0.83,\n",
       " 'person airplane telemarker mankind room': 0.84,\n",
       " 'person slowing telemarker mankind room': 0.84,\n",
       " 'person doorman telemarker mankind room': 0.85,\n",
       " 'person clot telemarker mankind room': 0.84,\n",
       " 'person groot telemarker mankind room': 0.83,\n",
       " 'person sigma telemarker mankind room': 0.83,\n",
       " 'person cultivation telemarker mankind room': 0.84,\n",
       " 'person reverdy telemarker mankind room': 0.86,\n",
       " 'person small-boat telemarker mankind room': 0.85,\n",
       " 'person full-fledged telemarker mankind room': 0.86,\n",
       " 'person unpack telemarker mankind room': 0.84,\n",
       " 'person pre-determined telemarker mankind room': 0.84,\n",
       " 'person memorialized telemarker mankind room': 0.84,\n",
       " \"person singers' telemarker mankind room\": 0.84,\n",
       " 'person pensioner telemarker mankind room': 0.86,\n",
       " 'person der telemarker mankind room': 0.85,\n",
       " 'person downpayment telemarker mankind room': 0.84,\n",
       " 'person confirmation telemarker mankind room': 0.83,\n",
       " 'person spycket telemarker mankind room': 0.84,\n",
       " \"person 500's telemarker mankind room\": 0.83,\n",
       " 'person subdivision telemarker mankind room': 0.85,\n",
       " 'person saturdays telemarker mankind room': 0.85,\n",
       " 'person rallies telemarker mankind room': 0.85,\n",
       " 'person parisology telemarker mankind room': 0.84,\n",
       " 'person warsaw telemarker mankind room': 0.83,\n",
       " \"person fair's telemarker mankind room\": 0.84,\n",
       " 'person spacesuit telemarker mankind room': 0.84,\n",
       " 'person skipped telemarker mankind room': 0.83,\n",
       " 'person threaded telemarker mankind room': 0.83,\n",
       " 'person speckled telemarker mankind room': 0.85,\n",
       " 'person ter. telemarker mankind room': 0.87,\n",
       " 'person dissensions telemarker mankind room': 0.84,\n",
       " 'person slice telemarker mankind room': 0.85,\n",
       " 'person weapons telemarker mankind room': 0.86,\n",
       " 'person savoyards telemarker mankind room': 0.84,\n",
       " 'person grayer telemarker mankind room': 0.86,\n",
       " 'person cogs telemarker mankind room': 0.84,\n",
       " 'person 61.2% telemarker mankind room': 0.83,\n",
       " 'person sophocles telemarker mankind room': 0.81,\n",
       " 'person apocalypse telemarker mankind room': 0.84,\n",
       " 'person schultz telemarker mankind room': 0.84,\n",
       " 'person mauldin telemarker mankind room': 0.84,\n",
       " 'person universalistic telemarker mankind room': 0.84,\n",
       " 'person yankee-hatred telemarker mankind room': 0.82,\n",
       " 'person floats telemarker mankind room': 0.85,\n",
       " 'person stinkpotters telemarker mankind room': 0.83,\n",
       " 'person neesen telemarker mankind room': 0.83,\n",
       " 'person inventive telemarker mankind room': 0.85,\n",
       " 'person succumbed telemarker mankind room': 0.85,\n",
       " 'person 1858 telemarker mankind room': 0.82,\n",
       " 'person vivier telemarker mankind room': 0.85,\n",
       " \"person steel's telemarker mankind room\": 0.83,\n",
       " 'person sours telemarker mankind room': 0.85,\n",
       " 'person 145-pound telemarker mankind room': 0.84,\n",
       " 'person puppets telemarker mankind room': 0.86,\n",
       " 'person 1159 telemarker mankind room': 0.83,\n",
       " 'person karns telemarker mankind room': 0.83,\n",
       " 'person strafe telemarker mankind room': 0.84,\n",
       " 'person indelicate telemarker mankind room': 0.84,\n",
       " 'person woman human man people': 0.9,\n",
       " 'someone woman human man people': 0.89,\n",
       " 'persons woman human man people': 0.9,\n",
       " 'somebody woman human man people': 0.89,\n",
       " 'peson woman human man people': 0.87,\n",
       " 'anyone woman human man people': 0.86,\n",
       " 'guy woman human man people': 0.88,\n",
       " 'Someone woman human man people': 0.88,\n",
       " 'individuals woman human man people': 0.88,\n",
       " 'foreigner woman human man people': 0.86,\n",
       " 'aperson woman human man people': 0.87,\n",
       " 'citizen woman human man people': 0.87,\n",
       " 'child woman human man people': 0.88,\n",
       " 'perpetrator woman human man people': 0.84,\n",
       " 'somone woman human man people': 0.88,\n",
       " 'anybody woman human man people': 0.86,\n",
       " 'degrating woman human man people': 0.83,\n",
       " 'businessperson woman human man people': 0.86,\n",
       " 'applicant woman human man people': 0.87,\n",
       " 'perosn woman human man people': 0.87,\n",
       " 'conservatee woman human man people': 0.84,\n",
       " 'Anybody woman human man people': 0.85,\n",
       " 'sweetest_kindest woman human man people': 0.82,\n",
       " 'Lt._Duhamell woman human man people': 0.82,\n",
       " 'gentleman woman human man people': 0.87,\n",
       " 'DEAR_PERPLEXED woman human man people': 0.83,\n",
       " 'referers_include woman human man people': 0.82,\n",
       " 'servicemember woman human man people': 0.85,\n",
       " 'self_injurer woman human man people': 0.84,\n",
       " 'Person woman human man people': 0.89,\n",
       " 'victim woman human man people': 0.87,\n",
       " \"Why_wasn'tI woman human man people\": 0.81,\n",
       " 'Dave_Chapelle_skit woman human man people': 0.83,\n",
       " 'Normal_indiviual woman human man people': 0.85,\n",
       " 'participant woman human man people': 0.87,\n",
       " 'normal_Salim_Ozonder woman human man people': 0.83,\n",
       " 'somebody_else woman human man people': 0.86,\n",
       " 'Genuine_callers woman human man people': 0.84,\n",
       " 'individual woman human man people': 0.88,\n",
       " 'thoughtful_considerate woman human man people': 0.83,\n",
       " 'vicitm woman human man people': 0.85,\n",
       " 'employee woman human man people': 0.88,\n",
       " 'spouse woman human man people': 0.87,\n",
       " 'elector woman human man people': 0.85,\n",
       " \"Sorry_ma'am woman human man people\": 0.84,\n",
       " 'Foulch woman human man people': 0.84,\n",
       " 'giver woman human man people': 0.85,\n",
       " 'clergyperson woman human man people': 0.85,\n",
       " 'oneself woman human man people': 0.87,\n",
       " 'implicated_Jessie_Dotson woman human man people': 0.83,\n",
       " 'caller woman human man people': 0.87,\n",
       " \"Shouldn'ta woman human man people\": 0.83,\n",
       " 'taxpaying_citizen woman human man people': 0.84,\n",
       " 'law_abider woman human man people': 0.83,\n",
       " 'Autism_impairs woman human man people': 0.81,\n",
       " 'lady woman human man people': 0.88,\n",
       " 'caregiver woman human man people': 0.86,\n",
       " 'cura_personalis woman human man people': 0.86,\n",
       " 'whoever woman human man people': 0.85,\n",
       " 'defendant woman human man people': 0.82,\n",
       " 'Insisting_Polanski woman human man people': 0.83,\n",
       " 'http://sports.forsythnews.com/_encourages_readers woman human man people': 0.82,\n",
       " 'Reasonable_suspicion woman human man people': 0.84,\n",
       " \"shouldn'tI woman human man people\": 0.84,\n",
       " 'New_Jerseyan woman human man people': 0.84,\n",
       " 'met_Kunis_gushes woman human man people': 0.84,\n",
       " 'hawking_Toyotas woman human man people': 0.85,\n",
       " 'kindest_sweetest woman human man people': 0.82,\n",
       " 'motorist woman human man people': 0.86,\n",
       " 'diagnosable_depression woman human man people': 0.84,\n",
       " 'Dear_Wondering woman human man people': 0.84,\n",
       " 'grandparent woman human man people': 0.85,\n",
       " 'TheSouthern.com_encourages_readers woman human man people': 0.81,\n",
       " 'partygoer woman human man people': 0.86,\n",
       " 'arrestee woman human man people': 0.85,\n",
       " 'Webster_dictionary_defines woman human man people': 0.82,\n",
       " 'compassionate_empathetic woman human man people': 0.83,\n",
       " 'schmoe woman human man people': 0.85,\n",
       " 'litterer woman human man people': 0.85,\n",
       " 'girl woman human man people': 0.9,\n",
       " 'offender woman human man people': 0.85,\n",
       " 'limitations_Rivenburgh woman human man people': 0.82,\n",
       " 'court_creates_conservatorships woman human man people': 0.81,\n",
       " 'Spokeswoman_Whitney_Jodry woman human man people': 0.84,\n",
       " 'ResourceMFG_delivers woman human man people': 0.8,\n",
       " 'founder_Lance_Loesberg woman human man people': 0.82,\n",
       " 'coworker woman human man people': 0.85,\n",
       " 'Barney_Fife_THAT woman human man people': 0.82,\n",
       " 'Anyone woman human man people': 0.86,\n",
       " 'Teri_Barbera_spokeswoman woman human man people': 0.83,\n",
       " 'sneezing_coughing_infected woman human man people': 0.84,\n",
       " 'human_beings woman human man people': 0.86,\n",
       " 'child_ren woman human man people': 0.86,\n",
       " 'searcher woman human man people': 0.85,\n",
       " 'conversationalist woman human man people': 0.86,\n",
       " 'compos_mentis woman human man people': 0.86,\n",
       " 'parent_grandparent woman human man people': 0.86,\n",
       " 'CHARLIZE woman human man people': 0.82,\n",
       " 'complaintant woman human man people': 0.84,\n",
       " 'Jared_Laugher woman human man people': 0.84,\n",
       " 'recklessly_disregards woman human man people': 0.81,\n",
       " 'Anytime_anyplace woman human man people': 0.86,\n",
       " \"AI'm woman human man people\": 0.84,\n",
       " 'commented_Debby_Herbenick woman human man people': 0.83,\n",
       " 'movie_fanatic_Carvey woman human man people': 0.85,\n",
       " 'fingerprinted_Hirst woman human man people': 0.84,\n",
       " 'Lucid_dreaming woman human man people': 0.84,\n",
       " 'entit_ies_each woman human man people': 0.84,\n",
       " 'juror woman human man people': 0.84,\n",
       " 'occupant woman human man people': 0.86,\n",
       " 'trespasser woman human man people': 0.85,\n",
       " 'Eveyone woman human man people': 0.84,\n",
       " 'aids_abets_counsels woman human man people': 0.82,\n",
       " 'patient woman human man people': 0.88,\n",
       " 'attendee woman human man people': 0.86,\n",
       " 'stands_Choucair woman human man people': 0.84,\n",
       " 'griever woman human man people': 0.85,\n",
       " 'sane_rational woman human man people': 0.85,\n",
       " 'Erin_Hamley woman human man people': 0.83,\n",
       " 'BigLook###_CEO woman human man people': 0.81,\n",
       " 'politician woman human man people': 0.86,\n",
       " 'ChicagoSports.com_staff woman human man people': 0.83,\n",
       " 'signature_gatherer woman human man people': 0.83,\n",
       " 'salesperson woman human man people': 0.86,\n",
       " 'Debra_Graziano woman human man people': 0.84,\n",
       " 'wouild woman human man people': 0.86,\n",
       " 'Dear_Unsure woman human man people': 0.84,\n",
       " 'Gottfried_Hirnschall_director woman human man people': 0.85,\n",
       " 'Entrepreneur_Tadashi_Yanai woman human man people': 0.82,\n",
       " 'Massood_Akhtar woman human man people': 0.84,\n",
       " 'Lawlessness_dictatorship woman human man people': 0.81,\n",
       " 'Johannas_Pope woman human man people': 0.83,\n",
       " 'BOUCHER_Yeah woman human man people': 0.84,\n",
       " 'claimant woman human man people': 0.83,\n",
       " 'Kleibecker woman human man people': 0.85,\n",
       " 'soldier woman human man people': 0.86,\n",
       " 'violator woman human man people': 0.84,\n",
       " 'nobody woman human man people': 0.87,\n",
       " 'Maxine_Sis_Cluse woman human man people': 0.83,\n",
       " 'interviewee woman human man people': 0.87,\n",
       " 'Bonny_Bakley woman human man people': 0.84,\n",
       " 'Dispatcher_Okay woman human man people': 0.83,\n",
       " 'Accomplishing_teamwork woman human man people': 0.84,\n",
       " 'Every_cooperator woman human man people': 0.83,\n",
       " 'litigant woman human man people': 0.84,\n",
       " 'nicest_nicest woman human man people': 0.83,\n",
       " 'moviegoer woman human man people': 0.86,\n",
       " 'personâ_€_™ woman human man people': 0.87,\n",
       " 'Gabe_Soumakian woman human man people': 0.84,\n",
       " 'socialiser woman human man people': 0.86,\n",
       " 'aggravated_assault_recklessly_endangering woman human man people': 0.81,\n",
       " 'sweetest_gentlest woman human man people': 0.82,\n",
       " 'gifter woman human man people': 0.85,\n",
       " 'borrower woman human man people': 0.86,\n",
       " 'preborn_baby woman human man people': 0.85,\n",
       " 'depo_MORE woman human man people': 0.82,\n",
       " 'atheist_agnostic_humanist woman human man people': 0.84,\n",
       " 'Michalyshen woman human man people': 0.84,\n",
       " 'inhabitant woman human man people': 0.86,\n",
       " 'homeowner woman human man people': 0.87,\n",
       " 'DEAR_CAT woman human man people': 0.84,\n",
       " 'underage_drinker woman human man people': 0.84,\n",
       " 'Relevant_Person woman human man people': 0.86,\n",
       " 'Still_LaCock_considered woman human man people': 0.81,\n",
       " 'Maro_Turcinovic woman human man people': 0.85,\n",
       " 'servant woman human man people': 0.85,\n",
       " 'opportunity_visit_http://www.earnparttimejobs.com/index.php?id=####### woman human man people': 0.82,\n",
       " 'grandchild woman human man people': 0.86,\n",
       " 'alienage woman human man people': 0.85,\n",
       " 'chief_Zafaruddin woman human man people': 0.82,\n",
       " 'whomever woman human man people': 0.86,\n",
       " 'Alleged_arsonist woman human man people': 0.83,\n",
       " 'thing_Fustini woman human man people': 0.84,\n",
       " \"Why_shouldn'ta woman human man people\": 0.81,\n",
       " 'shooter_FPS_genre woman human man people': 0.85,\n",
       " 'toknow woman human man people': 0.85,\n",
       " 'practicer woman human man people': 0.85,\n",
       " 'accountholder woman human man people': 0.85,\n",
       " 'locate_Furuya woman human man people': 0.79,\n",
       " 'driver_Ganassi_snarked woman human man people': 0.83,\n",
       " 'dies_intestate woman human man people': 0.83,\n",
       " 'Company_Dealmaking_Profile woman human man people': 0.84,\n",
       " 'suspect woman human man people': 0.84,\n",
       " 'transmission_Widdowson woman human man people': 0.85,\n",
       " 'peron woman human man people': 0.86,\n",
       " 'center_Emma_Margraf woman human man people': 0.83,\n",
       " 'passerby woman human man people': 0.87,\n",
       " 'family_Tohkanen woman human man people': 0.83,\n",
       " 'Commander_Thomas_Stangrecki woman human man people': 0.82,\n",
       " 'Emergency_official_Pubucairen woman human man people': 0.82,\n",
       " 'guesser woman human man people': 0.86,\n",
       " 'moocher woman human man people': 0.84,\n",
       " 'requester woman human man people': 0.86,\n",
       " 'wrongdoer woman human man people': 0.84,\n",
       " 'Dear_Lonely woman human man people': 0.85,\n",
       " 'Rodeffer_Theisen woman human man people': 0.84,\n",
       " 'HOW_TO_VISIT woman human man people': 0.83,\n",
       " 'Whoever woman human man people': 0.84,\n",
       " 'outdoor woman human man people': 0.87,\n",
       " 'reduce woman human man people': 0.83,\n",
       " \"contractor's woman human man people\": 0.84,\n",
       " 'ventilated woman human man people': 0.84,\n",
       " 'recompence woman human man people': 0.84,\n",
       " 'reactionaries woman human man people': 0.82,\n",
       " 'mon woman human man people': 0.86,\n",
       " 'ambush woman human man people': 0.85,\n",
       " 'oily woman human man people': 0.85,\n",
       " 'atrophy woman human man people': 0.84,\n",
       " 'q. woman human man people': 0.86,\n",
       " 'szold woman human man people': 0.86,\n",
       " 'n.a. woman human man people': 0.86,\n",
       " 'haverfield woman human man people': 0.84,\n",
       " 'neurenschatz woman human man people': 0.84,\n",
       " 'two-hour woman human man people': 0.86,\n",
       " 'waver woman human man people': 0.85,\n",
       " 'farming woman human man people': 0.85,\n",
       " 'hel woman human man people': 0.88,\n",
       " 'intensities woman human man people': 0.85,\n",
       " 'sags woman human man people': 0.84,\n",
       " 'slicker woman human man people': 0.85,\n",
       " 'homesick woman human man people': 0.85,\n",
       " 'alight woman human man people': 0.84,\n",
       " 'moosilauke woman human man people': 0.82,\n",
       " 'treasured woman human man people': 0.84,\n",
       " 'almost woman human man people': 0.84,\n",
       " 'none woman human man people': 0.86,\n",
       " 'offensives woman human man people': 0.85,\n",
       " 'tri-motor woman human man people': 0.85,\n",
       " \"who's woman human man people\": 0.85,\n",
       " 'shibboleths woman human man people': 0.83,\n",
       " 'olympic woman human man people': 0.85,\n",
       " 'pauson woman human man people': 0.85,\n",
       " 'soprano woman human man people': 0.85,\n",
       " 'ghastly woman human man people': 0.84,\n",
       " \"klemperer's woman human man people\": 0.84,\n",
       " 'detonating woman human man people': 0.84,\n",
       " 'pl. woman human man people': 0.86,\n",
       " 'undergo woman human man people': 0.84,\n",
       " 'dauntless woman human man people': 0.84,\n",
       " 'mahone woman human man people': 0.85,\n",
       " 'ainus woman human man people': 0.83,\n",
       " 'contest woman human man people': 0.85,\n",
       " '- woman human man people': 0.88,\n",
       " 'orate woman human man people': 0.86,\n",
       " 'impose woman human man people': 0.84,\n",
       " 'forthrightly woman human man people': 0.84,\n",
       " 'floundering woman human man people': 0.84,\n",
       " 'biz woman human man people': 0.86,\n",
       " 'hardboiled woman human man people': 0.84,\n",
       " 'fullness woman human man people': 0.85,\n",
       " 'septum woman human man people': 0.85,\n",
       " 'considering woman human man people': 0.84,\n",
       " 'unusual woman human man people': 0.87,\n",
       " 'boating woman human man people': 0.85,\n",
       " \"lighter'n woman human man people\": 0.84,\n",
       " 'candidacy woman human man people': 0.85,\n",
       " 'penn woman human man people': 0.83,\n",
       " 'diana woman human man people': 0.84,\n",
       " 'bungalow woman human man people': 0.87,\n",
       " 'escape woman human man people': 0.84,\n",
       " 'flirtation woman human man people': 0.85,\n",
       " '1781 woman human man people': 0.84,\n",
       " \"writer's woman human man people\": 0.86,\n",
       " 'surface-active woman human man people': 0.86,\n",
       " 'gelatin-like woman human man people': 0.84,\n",
       " 'burlesques woman human man people': 0.85,\n",
       " 'pistons woman human man people': 0.84,\n",
       " 'ymca woman human man people': 0.84,\n",
       " 'proclaims woman human man people': 0.82,\n",
       " \"hallowell's woman human man people\": 0.83,\n",
       " 'ross woman human man people': 0.85,\n",
       " 'portly woman human man people': 0.84,\n",
       " 'music woman human man people': 0.87,\n",
       " 'gaming-card woman human man people': 0.86,\n",
       " \"ginsberg's woman human man people\": 0.82,\n",
       " 'scientifique woman human man people': 0.85,\n",
       " 'ariadne woman human man people': 0.83,\n",
       " 'laxness woman human man people': 0.85,\n",
       " 'wingback woman human man people': 0.85,\n",
       " 'loathed woman human man people': 0.83,\n",
       " \"defendant's woman human man people\": 0.82,\n",
       " 'periodicity woman human man people': 0.84,\n",
       " 'non-publishers woman human man people': 0.85,\n",
       " '$23,000,000 woman human man people': 0.83,\n",
       " 'bucket-shop woman human man people': 0.85,\n",
       " \"whip's woman human man people\": 0.83,\n",
       " 'adulterated woman human man people': 0.84,\n",
       " 'eluded woman human man people': 0.85,\n",
       " 'try woman human man people': 0.86,\n",
       " 'wiry woman human man people': 0.85,\n",
       " 'blackman woman human man people': 0.86,\n",
       " 'consultative woman human man people': 0.84,\n",
       " 'propulsion woman human man people': 0.84,\n",
       " 'invade woman human man people': 0.83,\n",
       " 'craven woman human man people': 0.84,\n",
       " 'tualatin woman human man people': 0.83,\n",
       " 'shifters woman human man people': 0.85,\n",
       " 'vesole woman human man people': 0.86,\n",
       " 'discusses woman human man people': 0.85,\n",
       " 'klux woman human man people': 0.84,\n",
       " 'salted woman human man people': 0.84,\n",
       " 'earliest woman human man people': 0.81,\n",
       " 'mineralogies woman human man people': 0.83,\n",
       " 'launderings woman human man people': 0.84,\n",
       " '$12.00 woman human man people': 0.84,\n",
       " 'trabb woman human man people': 0.86,\n",
       " 'happened woman human man people': 0.86,\n",
       " 'high-protein woman human man people': 0.84,\n",
       " 'half-understood woman human man people': 0.85,\n",
       " 'viciousness woman human man people': 0.84,\n",
       " 'wedlock woman human man people': 0.85,\n",
       " 'yardage woman human man people': 0.84,\n",
       " 'deplorably woman human man people': 0.84,\n",
       " 'staged woman human man people': 0.86,\n",
       " \"foreman's woman human man people\": 0.84,\n",
       " 'shielded woman human man people': 0.81,\n",
       " 'fastidious woman human man people': 0.84,\n",
       " 'bulbs woman human man people': 0.86,\n",
       " 'person girl human man people': 0.88,\n",
       " 'person teenage_girl human man people': 0.86,\n",
       " 'person teenager human man people': 0.86,\n",
       " 'person lady human man people': 0.89,\n",
       " 'person teenaged_girl human man people': 0.86,\n",
       " 'person mother human man people': 0.87,\n",
       " 'person policewoman human man people': 0.87,\n",
       " 'person boy human man people': 0.87,\n",
       " 'person Woman human man people': 0.89,\n",
       " 'person sexually_assualted human man people': 0.82,\n",
       " 'person she human man people': 0.87,\n",
       " 'person Leah_Questin human man people': 0.82,\n",
       " 'person WOMAN human man people': 0.89,\n",
       " 'person housewife human man people': 0.88,\n",
       " 'person victim human man people': 0.85,\n",
       " 'person daughter human man people': 0.86,\n",
       " 'person grandmother human man people': 0.87,\n",
       " 'person schoolgirl human man people': 0.85,\n",
       " 'person teen_ager human man people': 0.85,\n",
       " 'person her human man people': 0.86,\n",
       " 'person TEENAGE_girl human man people': 0.85,\n",
       " 'person businesswoman human man people': 0.87,\n",
       " 'person Latoyia_Figueroa human man people': 0.84,\n",
       " 'person women human man people': 0.89,\n",
       " 'person Gunshot_victim human man people': 0.83,\n",
       " 'person newborn_baby human man people': 0.85,\n",
       " 'person awoman human man people': 0.88,\n",
       " 'person Masego_Kgomo human man people': 0.81,\n",
       " 'person Yannick_Brea human man people': 0.82,\n",
       " 'person Rachel_Wattenbarger human man people': 0.83,\n",
       " 'person prostitute human man people': 0.85,\n",
       " 'person Carole_Nordella human man people': 0.83,\n",
       " 'person toddler human man people': 0.85,\n",
       " 'person stepdaughter human man people': 0.85,\n",
       " 'person niece human man people': 0.84,\n",
       " 'person womans human man people': 0.89,\n",
       " 'person Mbarek_Lafrem human man people': 0.82,\n",
       " 'person motorist human man people': 0.85,\n",
       " 'person boyfriend human man people': 0.85,\n",
       " 'person Manious human man people': 0.83,\n",
       " 'person divorcee human man people': 0.85,\n",
       " 'person Stabbing_victim human man people': 0.83,\n",
       " 'person Suspected_burglar human man people': 0.83,\n",
       " 'person nun human man people': 0.85,\n",
       " 'person waitress human man people': 0.84,\n",
       " 'person Matthew_Tassio human man people': 0.82,\n",
       " 'person hitchhiker human man people': 0.84,\n",
       " 'person vicitm human man people': 0.84,\n",
       " 'person HEAVILY_pregnant_woman human man people': 0.83,\n",
       " 'person Stab_victim human man people': 0.82,\n",
       " 'person girlfriend human man people': 0.87,\n",
       " 'person maid human man people': 0.84,\n",
       " 'person bicyclist human man people': 0.84,\n",
       " 'person Candice_Moncayo human man people': 0.84,\n",
       " 'person dementia_sufferer human man people': 0.83,\n",
       " 'person sexually_assaulting_developmentally_disabled human man people': 0.8,\n",
       " 'person passerby human man people': 0.86,\n",
       " 'person wielding_screwdriver human man people': 0.84,\n",
       " 'person FRAIL_pensioner human man people': 0.83,\n",
       " 'person Attempted_carjacking human man people': 0.81,\n",
       " 'person female human man people': 0.88,\n",
       " 'person Donnisha_Hill human man people': 0.82,\n",
       " 'person motorcyclist human man people': 0.85,\n",
       " 'person vicitim human man people': 0.84,\n",
       " 'person jogger human man people': 0.84,\n",
       " \"person QI'ma human man people\": 0.84,\n",
       " 'person Yanisa_Fonteece human man people': 0.82,\n",
       " 'person transwoman human man people': 0.86,\n",
       " 'person Purse_snatched human man people': 0.83,\n",
       " 'person wielding_butcher_knife human man people': 0.82,\n",
       " 'person Miguel_Carrasquillo human man people': 0.82,\n",
       " 'person husband human man people': 0.86,\n",
       " 'person AN_##-YEAR-OLD human man people': 0.83,\n",
       " 'person Ahmed_Nahl human man people': 0.81,\n",
       " 'person WHEELCHAIR_bound human man people': 0.83,\n",
       " 'person aunt human man people': 0.85,\n",
       " 'person Tonya_Evette_Johnson human man people': 0.83,\n",
       " 'person Chihuahua_puppy human man people': 0.84,\n",
       " ...}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.queried_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIDNwO38DKbo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MDeC8lDDKhs",
    "outputId": "c15a1d15-46e3-428c-a99a-9c8e5c03dfff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "freq_dist = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "common_words = [word for word, count in freq_dist.most_common(10_000)]  # get the top 5000 common words\n",
    "\n",
    "print(len(common_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaxUAbquPl_Z",
    "outputId": "dcc301b9-fba4-4997-a2e8-c396f82913db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "common_words = [word for word, count in freq_dist.most_common(10_000)]  # get the top 5000 common words\n",
    "\n",
    "print(len(common_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2YccMgINM7Q",
    "outputId": "efa874a8-acb4-4b71-eb3b-7a51e97b5189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.query(['person', 'woman', 'man', 'camera'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8TW5MwdyDKk5",
    "outputId": "a40ddb5a-2e10-41ee-bdfd-1e11952fc007"
   },
   "outputs": [],
   "source": [
    "curr = ['person', 'woman', 'man', 'camera']\n",
    "best_score = manager.query(curr)\n",
    "\n",
    "new_best_score = []\n",
    "new_best_phrase = []\n",
    "\n",
    "for common_word in common_words:\n",
    "  target = ['person', 'woman', 'man', 'camera', common_word]\n",
    "  score = manager.query(target)\n",
    "  if score >= best_score:\n",
    "    print(f\"{target} {score}\")\n",
    "    print()\n",
    "\n",
    "    new_best_score.append(score)\n",
    "    new_best_phrase.append(target.copy())\n",
    "\n",
    "print(new_best_score)\n",
    "print(new_best_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs2KFCm9DKnj",
    "outputId": "c459e2f9-e6f6-4a62-a5dd-0138e9d7318e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'woman', 'man', 'camera', 'television']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QM8mA_MDKtE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9m_o2tMUDK3L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpvQy90-DK9C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmxUCOc0Cjxm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyAAS5L-Ci7y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
